S.No,Title,Abstract,Authors,Journal,Year
1,Fuzzy adaptive decision-making for boundedly rational traders in speculative stock markets,"The development of new models that would enhance predictability for time series with dynamic time-varying, nonlinear features is a major challenge for speculators. Boundedly rational investors called ""chartists"" use advanced heuristics and rules-of-thumb to make profit by trading, or even hedge against potential market risks. This paper introduces a hybrid neurofuzzy system for decision-making and trading under uncertainty. The efficiency of a technical trading strategy based on the neurofuzzy model is investigated, in order to predict the direction of the market for 10 of the most prominent stock indices of U.S.A, Europe and Southeast Asia. It is demonstrated via an extensive empirical analysis that the neurofuzzy model allows technical analysts to earn significantly higher returns by providing valid information for a potential turning point on the next trading day. The total profit of the proposed neurofuzzy model, including transaction costs, is consistently superior to a recurrent neural network and a Buy & Hold strategy for all indices, particularly for the highly speculative, emerging Southeast Asian markets. Optimal prediction is based on the dynamic update and adaptive calibration of the heuristic fuzzy learning rules, which reflect the psychological and behavioral patterns of the traders.",Stelios D. Bekiros,European Journal of Operational Research,2010
2,Online Portfolio Selection Strategy Based on Combining Experts' Advice,"The weak aggregating algorithm (WAA) developed from learning and prediction with expert advice makes decisions by considering all the experts' advice, and each expert's weight is updated according to his performance in previous periods. In this paper, we apply the WAA to the online portfolio selection problem. We first consider a simple case in which the expert advice is the strategy for investing in one stock; for this case, we obtain a portfolio selection strategy WAAS and prove that the WAAS can identify the best stock. We also discuss a more complicated case in which constant rebalanced portfolios are considered as expert advice, and obtain a corresponding portfolio selection strategy WAAC. The theoretical result shows that the cumulative gain that WAAC achieves is as large as that of the best constant rebalanced portfolio. Numerical analysis shows that the cumulative gains of our proposed strategies are as large as those of the best expert advice.","Yong Zhang,Xingyu Yang",Computing in Economics and Finance,2017
3,Reinforcement Learning: An Introduction,"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.","Richard S. Sutton,Richard S. Sutton,Andrew G. Barto",,1988
4,Policy Gradient Methods for Reinforcement Learning with Function Approximation,"Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.","Richard S. Sutton,Richard S. Sutton,David McAllester,Satinder Singh,Yishay Mansour",,1999
5,Learning to trade via direct reinforcement,"We present methods for optimizing portfolios, asset allocations, and trading systems based on direct reinforcement (DR). In this approach, investment decision-making is viewed as a stochastic control problem, and strategies are discovered directly. We present an adaptive algorithm called recurrent reinforcement learning (RRL) for discovering investment policies. The need to build forecasting models is eliminated, and better trading performance is obtained. The direct reinforcement approach differs from dynamic programming and reinforcement algorithms such as TD-learning and Q-learning, which attempt to estimate a value function for the control problem. We find that the RRL direct reinforcement framework enables a simpler problem representation, avoids Bellman's curse of dimensionality and offers compelling advantages in efficiency. We demonstrate how direct reinforcement can be used to optimize risk-adjusted investment returns (including the differential Sharpe ratio), while accounting for the effects of transaction costs. In extensive simulation work using real financial data, we find that our approach based on RRL produces better trading strategies than systems utilizing Q-learning (a value function method). Real-world applications include an intra-daily currency trader and a monthly asset allocation system for the S&P 500 Stock Index and T-Bills.","John Moody,Matthew Saffell",IEEE Transactions on Neural Networks,2001
6,Continuous control with deep reinforcement learning,"We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.","Timothy Lillicrap,Jonathan J. Hunt,Alexander Pritzel,Nicolas Heess,Tom Erez,Yuval Tassa,David Silver,Daan Wierstra",arXiv: Learning,2015
7,Deep Reinforcement Learning for Trading,"We adopt Deep Reinforcement Learning algorithms to design trading strategies for continuous futures contracts. Both discrete and continuous action spaces are considered and volatility scaling is incorporated to create reward functions which scale trade positions based on market volatility. We test our algorithms on the 50 most liquid futures contracts from 2011 to 2019, and investigate how performance varies across different asset classes including commodities, equity indices, fixed income and FX markets. We compare our algorithms against classical time series momentum strategies, and show that our method outperforms such baseline models, delivering positive profits despite heavy transaction costs. The experiments show that the proposed algorithms can follow large market trends without changing positions and can also scale down, or hold, through consolidation periods.","Zihao Zhang,Zihao Zhang,Zihao Zhang,Stefan Zohren,Stephen J. Roberts,Stephen Roberts",arXiv: Computational Finance,2019
8,The Sharpe Ratio,". Over 25 years ago, in Sharpe [1966], I introduced a measure for the performance of mutual funds and proposed the term reward-to-variability ratio to describe it (the measure is also described in Sharpe [1975] ). While the measure has gained considerable popularity, the name has not. Other authors have termed the original version the Sharpe Index (Radcliff [1990, p. 286] and Haugen [1993, p. 315]), the Sharpe Measure (Bodie, Kane and Marcus [1993, p. 804], Elton and Gruber [1991, p. 652], and Reilly [1989, p.803]), or the Sharpe Ratio (Morningstar [1993, p. 24]). Generalized versions have also appeared under various names (see. for example, BARRA [1992, p. 21] and Capaul, Rowley and Sharpe [1993, p. 33]).",William F. Sharpe,The Journal of Portfolio Management,1994
9,Optimal Asset Allocation using Adaptive Dynamic Programming,"In recent years, the interest of investors has shifted to computerized asset allocation (portfolio management) to exploit the growing dynamics of the capital markets. In this paper, asset allocation is formalized as a Markovian Decision Problem which can be optimized by applying dynamic programming or reinforcement learning based algorithms. Using an artificial exchange rate, the asset allocation strategy optimized with reinforcement learning (Q-Learning) is shown to be equivalent to a policy computed by dynamic programming. The approach is then tested on the task to invest liquid capital in the German stock market. Here, neural networks are used as value function approximators. The resulting asset allocation strategy is superior to a heuristic benchmark policy. This is a further example which demonstrates the applicability of neural network based reinforcement learning to a problem setting with a high dimensional state space.",Ralph Neuneier,,1995
10,Enhancing Q-Learning for Optimal Asset Allocation,"This paper enhances the Q-learning algorithm for optimal asset allocation proposed in (Neuneier, 1996 [6]). The new formulation simplifies the approach by using only one value-function for many assets and allows model-free policy-iteration. After testing the new algorithm on real data, the possibility of risk management within the framework of Markov decision problems is analyzed. The proposed methods allows the construction of a multi-period portfolio management system which takes into account transaction costs, the risk preferences of the investor, and several constraints on the allocation.",Ralph Neuneier,,1997
11,Asynchronous methods for deep reinforcement learning,"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.","Volodymyr Mnih,Adrià Puigdomènech Badia,Mehdi Mirza,Alex Graves,Tim Harley,Timothy Lillicrap,David Silver,Koray Kavukcuoglu",,2016
12,Cryptocurrency Portfolio Management with Deep Reinforcement Learning,"Portfolio management is the decision-making process of allocating an amount of fund into different financial investment products. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. This paper presents a model-less convolutional neural network with historic prices of a set of financial assets as its input, outputting portfolio weights of the set. The network is trained with 0.7 years' price data from a cryptocurrency exchange. The training is done in a reinforcement manner, maximizing the accumulative return, which is regarded as the reward function of the network. Backtest trading experiments with trading period of 30 minutes is conducted in the same market, achieving 10-fold returns in 1.8 months' periods. Some recently published portfolio selection strategies are also used to perform the same back-tests, whose results are compared with the neural network. The network is not limited to cryptocurrency, but can be applied to any other financial markets.","Zhengyao Jiang,Jinjun Liang",arXiv: Learning,2016
13,An intelligent hybrid trading system for discovering trading rules for the futures market using rough sets and genetic algorithms,"Display Omitted This study proposes an intelligent hybrid trading system for discovering technicaltrading rules.This study deals with the optimization problem of data discretization and reducts.Rough set analysis is adopted to represent trading rules.A genetic algorithm is used to discover optimal and sub-optimal trading rules.To evaluate the proposed system, a sliding window method is applied. Discovering intelligent technical trading rules from nonlinear and complex stock market data, and then developing decision support trading systems, is an important challenge. The objective of this study is to develop an intelligent hybrid trading system for discovering technical trading rules using rough set analysis and a genetic algorithm (GA). In order to obtain better trading decisions, a novel rule discovery mechanism using a GA approach is proposed for solving optimization problems (i.e., data discretization and reducts) of rough set analysis when discovering technical trading rules for the futures market. Experiments are designed to test the proposed model against comparable approaches (i.e., random, correlation, and GA approaches). In addition, these comprehensive experiments cover most of the current trading system topics, including the use of a sliding window method (with or without validation dataset), the number of trading rules, and the size of training period. To evaluate an intelligent hybrid trading system, experiments were carried out on the historical data of the Korea Composite Stock Price Index 200 (KOSPI 200) futures market. In particular, trading performance is analyzed according to the number of sets of decision rules and the size of the training period for discovering trading rules for the testing period. The results show that the proposed model significantly outperforms the benchmark model in terms of the average return and as a risk-adjusted measure.","Youngmin Kim,Young Min Kim,Wonbin Ahn,Kyong Joo Oh,David Enke",Applied Soft Computing,2017
14,Deep Direct Reinforcement Learning for Financial Signal Representation and Trading,"Can we train the computer to beat experienced traders for financial assert trading? In this paper, we try to address this challenge by introducing a recurrent deep neural network (NN) for real-time financial signal representation and trading. Our model is inspired by two biological-related learning concepts of deep learning (DL) and reinforcement learning (RL). In the framework, the DL part automatically senses the dynamic market condition for informative feature learning. Then, the RL module interacts with deep representations and makes trading decisions to accumulate the ultimate rewards in an unknown environment. The learning system is implemented in a complex NN that exhibits both the deep and recurrent structures. Hence, we propose a task-aware backpropagation through time method to cope with the gradient vanishing issue in deep training. The robustness of the neural system is verified on both the stock and the commodity future markets under broad testing conditions.","Yue Deng,Feng Bao,Youyong Kong,Youyong Kong,Zhiquan Ren,Qionghai Dai",IEEE Transactions on Neural Networks,2017
15,Proximal Policy Optimization Algorithms,"We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ""surrogate"" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.","John Schulman,Filip Wolski,Filip Wolski,Prafulla Dhariwal,Alec Radford,Oleg Klimov",arXiv: Learning,2017
16,Application of Deep Reinforcement Learning on Automated Stock Trading,"How to make right decisions in stock trading is a vital and challenging task for investors. Since deep reinforcement learning (DRL) has outperformed human beings in many fields such as playing Atari Games, can a DRL agent automatically make trading decisions and achieve long-term stable profits? In this paper, we try to solve this challenge by applying Deep Q-network (DQN) and Deep Recurrent Q-network (DRQN) in stock trading and try to build an end-to-end daily stock trading system which can decide to buy or to sell automatically at each trading day. The S…P500 ETF is selected as our trading asset and its daily trading data are used as the state of the trading environment. The agent’s performance is evaluated by comparing with benchmarks of Buy and Hold (BH) and Random action-selected DQN trader. Experiment results show that our DQN trader outperforms the two benchmarks and DRQN trader is even better than DQN trader mainly because the recurrence framework can discover and exploit profitable patterns hidden in time-related sequence.","Lin Chen,Qiang Gao",,2019
17,Actor-critic algorithms,"Many complex decision making problems like scheduling in manufacturing systems, portfolio management in finance, admission control in communication networks etc., with clear and precise objectives, can be formulated as stochastic dynamic programming problems in which the objective of decision making is to maximize a single “overall” reward. In these formulations, finding an optimal decision policy involves computing a certain “value function” which assigns to each state the optimal reward one would obtain if the system was started from that state. This function then naturally prescribes the optimal policy, which is to take decisions that drive the system to states with maximum value. 
For many practical problems, the computation of the exact value function is intractable, analytically and numerically, due to the enormous size of the state space. Therefore one has to resort to one of the following approximation methods to find a good sub-optimal policy: (1)Approximate the value function. (2)Restrict the search for a good policy to a smaller family of policies. 
In this thesis, we propose and study actor-critic algorithms which combine the above two approaches with simulation to find the best policy among a parameterized class of policies. Actor-critic algorithms have two learning units: an actor and a critic. An actor is a decision maker with a tunable parameter. A critic is a function approximator. The critic tries to approximate the value function of the policy used by the actor, and the actor in turn tries to improve its policy based on the current approximation provided by the critic. Furthermore, the critic evolves on a faster time-scale than the actor. 
We propose several variants of actor-critic algorithms. In all the variants, the critic uses Temporal Difference (TD) learning with linear function approximation. Some of the variants are inspired by a new geometric interpretation of the formula for the gradient of the overall reward with respect to the actor parameters. This interpretation suggests a natural set of basis functions for the critic, determined by the family of policies parameterized by the actor's parameters. We concentrate on the average expected reward criterion but we also show how the algorithms can be modified for other objective criteria. We prove convergence of the algorithms for problems with general (finite, countable, or continuous) state and decision spaces. 
To compute the rate of convergence (ROC) of our algorithms, we develop a general theory of the ROC of two-time-scale algorithms and we apply it to study our algorithms. In the process, we study the ROC of TD learning and compare it with related methods such as Least Squares TD (LSTD). We study the effect of the basis functions used for linear function approximation on the ROC of TD. We also show that the ROC of actor-critic algorithms does not depend on the actual basis functions used in the critic but depends only on the subspace spanned by them and study this dependence. 
Finally, we compare the performance of our algorithms with other algorithms that optimize over a parameterized family of policies. We show that when only the “natural” basis functions are used for the critic, the rate of convergence of the actor critic algorithms is the same as that of certain stochastic gradient descent algorithms. However, with appropriate additional basis functions for the critic, we show that our algorithms outperform the existing ones in terms of ROC. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)","Vijay R. Konda,John N. Tsitsiklis",,2002
18,Passive Investment Strategies and Efficient Markets,"This paper presents the case for and the evidence in favour of passive investment strategies and examines the major criticisms of the technique. I conclude that the evidence strongly supports passive investment management in all markets— smallcapitalisation stocks as well as large-capitalisation equities, US markets as well as international markets, and bonds as well as stocks. Recent attacks on the efficient market hypothesis do not weaken the case for indexing.",Burton G. Malkiel,European Financial Management,2003
19,Heterogeneous trading strategies with adaptive fuzzy Actor–Critic reinforcement learning: A behavioral approach,"The present study addresses the learning mechanism of boundedly rational agents in the dynamic and noisy environment of financial markets. The main objective is the development of a system that ""decodes"" the knowledge-acquisition strategy and the decision-making process of technical analysts called ""chartists"". It advances the literature on heterogeneous learning in speculative markets by introducing a trading system wherein market environment and agent beliefs are represented by fuzzy inference rules. The resulting functionality leads to the derivation of the parameters of the fuzzy rules by means of adaptive training. In technical terms, it expands the literature that has utilized Actor-Critic reinforcement learning and fuzzy systems in agent-based applications, by presenting an adaptive fuzzy reinforcement learning approach that provides with accurate and prompt identification of market turning points and thus higher predictability. The purpose of this paper is to illustrate this concretely through a comparative investigation against other well-established models. The results indicate that with the inclusion of transaction costs, the profitability of the novel system in case of NASDAQ Composite, FTSE100 and NIKKEI255 indices is consistently superior to that of a Recurrent Neural Network, a Markov-switching model and a Buy and Hold strategy. Overall, the proposed system via the reinforcement learning mechanism, the fuzzy rule-based state space modeling and the adaptive action selection policy, leads to superior predictions upon the direction-of-change of the market.",Stelios D. Bekiros,Journal of Economic Dynamics and Control,2010
20,"Skulls, Financial Turbulence, and Risk Management","Based on a methodology introduced in 1927 to analyze human skulls and later applied to turbulence in financial markets, this study shows how to use a statistically derived measure of financial turbulence to measure and manage risk and to improve investment performance. View a webinar based on this article.","Mark Kritzman,Yuanzhen Li,Yuanzhen Li",Financial Analysts Journal,2010
21,Trading Strategies to Exploit Blog and News Sentiment,"We use quantitative media (blogs, and news as a comparison) data generated by a large-scale natural language processing (NLP) text analysis system to perform a comprehensive and comparative study on how company related news variables anticipates or reflects the company's stock trading volumes and financial returns. Building on our findings, we give a sentiment-based market-neutral trading strategy which gives consistently favorable returns with low volatility over a long period. Our results are significant in confirming the performance of general blog and news sentiment analysis methods over broad domains and sources. Moreover, several remarkable differences between news and blogs are also identified.","Wenbin Zhang,Steven Skiena",,2010
22,Bitcoin Is Volatile! Isn’t that Right?,"In this study, we substantiate with financial data collection and analysis the hypothesis regarding the volatility of Bitcoin exchange rate against common currencies. Financial data were collected from July 2010 until April 2014. The raw annualised volatility of Bitcoin is compared to conventional and major exchange rates. The first set of results indicate a high value of annualised volatility for the Bitcoin exchange rate. When the volume of Bitcoin transactions is considered, the volatility of the Bitcoin exchange rate stabilizes significantly.","Svetlana Sapuric,Angelika I. Kokkinaki",,2014
23,Human-level control through deep reinforcement learning,"An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.","Volodymyr Mnih,Koray Kavukcuoglu,David Silver,Andrei A. Rusu,Joel Veness,Marc G. Bellemare,Alex Graves,Martin Riedmiller,Andreas K. Fidjeland,Georg Ostrovski,Stig Petersen,Charles Beattie,Amir Sadik,Ioannis Antonoglou,Ioannis Antonoglou,Helen King,Dharshan Kumaran,Daan Wierstra,Shane Legg,Demis Hassabis",Nature,2015
24,Trust Region Policy Optimization,"We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.","John Schulman,Sergey Levine,Philipp Moritz,Michael I. Jordan,Pieter Abbeel",arXiv: Learning,2015
25,Mastering the game of Go with deep neural networks and tree search,"The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of stateof-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.","David Silver,Aja Huang,Chris J. Maddison,Arthur Guez,Laurent Sifre,George van den Driessche,Julian Schrittwieser,Ioannis Antonoglou,Ioannis Antonoglou,Veda Panneershelvam,Marc Lanctot,Sander Dieleman,Dominik Grewe,John Nham,Nal Kalchbrenner,Ilya Sutskever,Timothy Lillicrap,Madeleine Leach,Koray Kavukcuoglu,Thore Graepel,Demis Hassabis",Nature,2016
26,A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem,"Financial portfolio management is the process of constant redistribution of a fund into different financial products. This paper presents a financial-model-free Reinforcement Learning framework to provide a deep machine learning solution to the portfolio management problem. The framework consists of the Ensemble of Identical Independent Evaluators (EIIE) topology, a Portfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL) scheme, and a fully exploiting and explicit reward function. This framework is realized in three instants in this work with a Convolutional Neural Network (CNN), a basic Recurrent Neural Network (RNN), and a Long Short-Term Memory (LSTM). They are, along with a number of recently reviewed or published portfolio-selection strategies, examined in three back-test experiments with a trading period of 30 minutes in a cryptocurrency market. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. All three instances of the framework monopolize the top three positions in all experiments, outdistancing other compared trading algorithms. Although with a high commission rate of 0.25% in the backtests, the framework is able to achieve at least 4-fold returns in 50 days.","Zhengyao Jiang,Zhengyao Jiang,Dixing Xu,Jinjun Liang,Jinjun Liang",arXiv: Computational Finance,2017
27,TensorLayer: A Versatile Library for Efficient Deep Learning Development,"Recently we have observed emerging uses of deep learning techniques in multimedia systems. Developing a practical deep learning system is arduous and complex. It involves labor-intensive tasks for constructing sophisticated neural networks, coordinating multiple network models, and managing a large amount of training-related data. To facilitate such a development process, we propose TensorLayer which is a Python-based versatile deep learning library. TensorLayer provides high-level modules that abstract sophisticated operations towards neuron layers, network models, training data and dependent training jobs. In spite of offering simplicity, it has transparent module interfaces that allows developers to flexibly embed low-level controls within a backend engine, with the aim of supporting fine-grain tuning towards training. Real-world cluster experiment results show that TensorLayeris able to achieve competitive performance and scalability in critical deep learning tasks. TensorLayer was released in September 2016 on GitHub. Since after, it soon become one of the most popular open-sourced deep learning library used by researchers and practitioners.","Hao Dong,Akara Supratak,Luo Mai,Fangde Liu,Axel Oehmichen,Simiao Yu,Yike Guo",,2017
28,Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning,"In this paper, we propose a novel framework for training vision-based agent for First-Person Shooter (FPS) Game, in particular Doom.
Our framework combines the state-of-the-art reinforcement learning approach (Asynchronous Advantage Actor-Critic (A3C) model) with curriculum learning. Our model is simple in design and only uses game states from the AI side, rather than using opponents' information. On a known map, our agent won 10 out of the 11 attended games and the champion of Track1 in ViZDoom AI Competition 2016 by a large margin, 35\% higher score than the second place.","Yuxin Wu,Yuandong Tian",,2017
29,A Practical Machine Learning Approach for Dynamic Stock Recommendation,"Stock recommendation is vital to investment companies and investors. However, no single stock selection strategy will always win while analysts may not have enough time to check all S&P 500 stocks (the Standard & Poor's 500). In this paper, we propose a practical scheme that recommends stocks from S&P 500 using machine learning. Our basic idea is to buy and hold the top 20% stocks dynamically. First, we select representative stock indicators with good explanatory power. Secondly, we take five frequently used machine learning methods, including linear regression, ridge regression, stepwise regression, random forest and generalized boosted regression, to model stock indicators and quarterly log-return in a rolling window. Thirdly, we choose the model with the lowest Mean Square Error in each period to rank stocks. Finally, we test the selected stocks by conducting portfolio allocation methods such as equally weighted, mean-variance, and minimum-variance. Our empirical results show that the proposed scheme outperforms the long-only strategy on the S&P 500 index in terms of Sharpe ratio and cumulative returns.","Hongyang Yang,Hongyang Yang,Xiao-Yang Liu,Xiao-Yang Liu,Xiao-Yang Liu,Xiao-Yang Liu,Qingwei Wu",,2018
30,Large-Scale Study of Curiosity-Driven Learning,"Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the hand-designed extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at this https URL","Yuri Burda,Harri Edwards,Harrison Edwards,Deepak Pathak,Amos Storkey,Trevor Darrell,Alexei A. Efros",arXiv: Learning,2018
31,Learning to Trade with Deep Actor Critic Methods,"In this paper, we propose a new trading framework to apply deep actor critic methods to financial trading problems. Different from traditional actor critic methods, our model use not only actor but also critic to make the final decision. And for generalization purpose, a siamese structure in which the actor and the critic share the same LSTM features extraction part is adopted. The extracted features are then passed to different dense connected networks to compute q values and policy logits. The experiment results on different periods of CSI 300 prove that DACT has significantly better performances than B&H, DQT and DDRT. Furthermore, the idea of exploring based on the ensemble of actor and critic is valuable for other reinforcement learning problems.","Jinke Li,Ruonan Rao,Jun Shi",,2018
32,Performance of the Average Directional Index as a market timing tool for the most actively traded USD based currency pairs,"The aim of this study is to test a trading system based on the average directional index, which is complemented with the parabolic stop and reverse indicator. The trend-based system is tested onto the most actively traded USD based foreign currency pairs, using both monthly and weekly data set over 2000–2018. Sharpe and Sortino measures are used to track the performance of the currency pairs, based on total risk and downside risk assumptions. Results are robust tested by decomposing the data into pre and post 2008 financial crisis. Using an investment horizon over 18 years, the reliance upon the monthly model produced lower maximum drawdowns and lesser trades than the weekly model. While Swiss Franc had the best (worse) performance in the monthly (weekly) based model, the Chinese Renminbi witnessed the worse (best) performance in the monthly (weekly) based model. Pre and post financial crisis decompositions suggest the weekly-based system is more reliable than the monthly one with relatively more trades and positive performance, where the Chinese Renminbi and Japanese Yen posted the highest Sharpe and Sortino values of 0.996 and 4.452 respectively in the post crisis period. Proportionately high level of negative returns coupled with relatively low positive Sharpe and Sortino values, however, suggest that a trading system relying on the average directional index and parabolic stop and reverse indicator to be further tested and analyzed at higher frequencies.",Ikhlaas Gurrib,Banks and Bank Systems,2018
33,"Reinforcement learning for control: Performance, stability, and deep approximators","Abstract   Reinforcement learning (RL) offers powerful algorithms to search for optimal controllers of systems with nonlinear, possibly stochastic dynamics that are unknown or highly uncertain. This review mainly covers artificial-intelligence approaches to RL, from the viewpoint of the control engineer. We explain how approximate representations of the solution make RL feasible for problems with continuous states and control actions. Stability is a central concern in control, and we argue that while the control-theoretic RL subfield called adaptive dynamic programming is dedicated to it, stability of RL largely remains an open question. We also cover in detail the case where deep neural networks are used for approximation, leading to the field of deep RL, which has shown great success in recent years. With the control practitioner in mind, we outline opportunities and pitfalls of deep RL; and we close the survey with an outlook that – among other things – points out some avenues for bridging the gap between control and artificial-intelligence RL techniques.","Lucian Busoniu,Tim de Bruin,Domagoj Tolic,Domagoj Tolic,Jens Kober,Ivana Palunko",Annual Reviews in Control,2018
34,Reinforcement learning in financial markets - a survey,"The advent of reinforcement learning (RL) in financial markets is driven by several advantages inherent to this field of artificial intelligence. In particular, RL allows to combine the ""prediction"" and the ""portfolio construction"" task in one integrated step, thereby closely aligning the machine learning problem with the objectives of the investor. At the same time, important constraints, such as transaction costs, market liquidity, and the investor's degree of risk-aversion, can be conveniently taken into account. Over the past two decades, and albeit most attention still being devoted to supervised learning methods, the RL research community has made considerable advances in the finance domain. The present paper draws insights from almost 50 publications, and categorizes them into three main approaches, i.e., critic-only approach, actor-only approach, and actor-critic approach. Within each of these categories, the respective contributions are summarized and reviewed along the representation of the state, the applied reward function, and the action space of the agent. This cross-sectional perspective allows us to identify recurring design decisions as well as potential levers to improve the agent's performance. Finally, the individual strengths and weaknesses of each approach are discussed, and directions for future research are pointed out.",Thomas G. Fischer,,2018
35,Supervised Reinforcement Learning with Recurrent Neural Network for Dynamic Treatment Recommendation,"Dynamic treatment recommendation systems based on large-scale electronic health records (EHRs) become a key to successfully improve practical clinical outcomes. Prior relevant studies recommend treatments either use supervised learning (e.g. matching the indicator signal which denotes doctor prescriptions), or reinforcement learning (e.g. maximizing evaluation signal which indicates cumulative reward from survival rates). However, none of these studies have considered to combine the benefits of supervised learning and reinforcement learning. In this paper, we propose Supervised Reinforcement Learning with Recurrent Neural Network (SRL-RNN), which fuses them into a synergistic learning framework. Specifically, SRL-RNN applies an off-policy actor-critic framework to handle complex relations among multiple medications, diseases and individual characteristics. The ""actor"" in the framework is adjusted by both the indicator signal and evaluation signal to ensure effective prescription and low mortality. RNN is further utilized to solve the Partially-Observed Markov Decision Process (POMDP) problem due to the lack of fully observed states in real world applications. Experiments on the publicly real-world dataset, i.e., MIMIC-3, illustrate that our model can reduce the estimated mortality, while providing promising accuracy in matching doctors' prescriptions.","Lu Wang,Wei Zhang,Wei Zhang,Wei Zhang,Wei Zhang,Wei Zhang,Wei Zhang,Xiaofeng He,Hongyuan Zha",arXiv: Learning,2018
36,Get real: realism metrics for robust limit order book market simulations,"Machine learning (especially reinforcement learning) methods for trading are increasingly reliant on simulation for agent training and testing. Furthermore, simulation is important for validation of hand-coded trading strategies and for testing hypotheses about market structure. A challenge, however, concerns the robustness of policies validated in simulation because the simulations lack fidelity. In fact, researchers have shown that many market simulation approaches fail to reproduce statistics and stylized facts seen in real markets. As a step towards addressing this we surveyed the literature to collect a set of reference metrics and applied them to real market data and simulation output. Our paper provides a comprehensive catalog of these metrics including mathematical formulations where appropriate. Our results show that there are still significant discrepancies between simulated markets and real ones. However, this work serves as a benchmark against which we can measure future improvement.","Svitlana Vyetrenko,David Byrd,Nick Petosa,Mahmoud Mahfouz,Danial Dervovic,Danial Dervovic,Danial Dervovic,Manuela Veloso,Tucker Hybinette Balch,Tucker Balch,Tucker Hybinette Balch",,2019
37,Multi-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis,"Liquidation is the process of selling a large number of shares of one stock sequentially within a given time frame, taking into consideration the costs arising from market impact and a trader's risk aversion. The main challenge in optimizing liquidation is to find an appropriate modeling system that can incorporate the complexities of the stock market and generate practical trading strategies. In this paper, we propose to use multi-agent deep reinforcement learning model, which better captures high-level complexities comparing to various machine learning methods, such that agents can learn how to make the best selling decisions. First, we theoretically analyze the Almgren and Chriss model and extend its fundamental mechanism so it can be used as the multi-agent trading environment. Our work builds the foundation for future multi-agent environment trading analysis. Secondly, we analyze the cooperative and competitive behaviours between agents by adjusting the reward functions for each agent, which overcomes the limitation of single-agent reinforcement learning algorithms. Finally, we simulate trading and develop an optimal trading strategy with practical constraints by using a reinforcement learning method, which shows the capabilities of reinforcement learning methods in solving realistic liquidation problems.","Wenhang Bao,Xiao-yang Liu,Xiao-Yang Liu",Research Papers in Economics,2019
38,Practical Machine Learning Approach to Capture the Scholar Data Driven Alpha in AI Industry,"AI technologies are helping more and more companies leverage their resources to expand business, reach higher financial performance and become more valuable for investors. However, it is difficult to capture and predict the impacts of AI technologies on companies’ stock prices through traditional financial factors. Moreover, common information sources such as company’s earnings calls and news are not enough to quantity and predict the actual AI premium for a certain company. In this paper, we utilize scholar data as alternative data for trading strategy development and propose a practical machine learning approach to quantity the AI premium of a company and capture the scholar data driven alpha in the AI industry. First, we collect the scholar data from the Microsoft Academic Graph database, and conduct feature engineering based on AI publication and patent data, such as conference/journal publication counts, patent counts, fields of studies and paper citations. Second, we apply machine learning algorithms to weight and re-balance stocks using the scholar data and traditional financial factors every month, and construct portfolios using the “buy-and-hold-long only” strategy. Finally, we evaluate our factor and portfolio in terms of factor performance and portfolio’s cumulative return. The proposed scholar data driven approach achieves a cumulative return of 1029.1% during our backtesting period, which significantly outperforms the Nasdaq 100 index’s 529.5% and S&P 500’s 222.6%. The traditional financial factors approach only leads to 776.7%, which indicates that our scholar data driven approach is better at capturing investment alpha in AI industry than traditional financial factors.","Yunzhe Fang,Xiao-Yang Liu,Xiao-Yang Liu,Xiao-Yang Liu,Hongyang Yang",,2019
39,Reinforcement Learning in Stock Trading.,"Using machine learning techniques in financial markets, particularly in stock trading, attracts a lot of attention from both academia and practitioners in recent years. Researchers have studied different supervised and unsupervised learning techniques to either predict stock price movement or make decisions in the market.","Quang-Vinh Dang,Quang-Vinh Dang",,2019
40,An empirical investigation of the challenges of real-world reinforcement learning.,"Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called realworldrl-suite which we propose an as an open-source benchmark.","Gabriel Dulac-Arnold,Nir Levine,Daniel J. Mankowitz,Jerry Li,Cosmin Paduraru,Cosmin Paduraru,Sven Gowal,Todd Hester",arXiv: Learning,2020
41,Quantifying ESG alpha using scholar big data: an automated machine learning approach,"ESG (Environmental, social and governance) alpha strategy that makes sustainable investment has gained popularity among investors. The ESG fields of study in scholar big data is a valuable alternative data that reflects a company's long-term ESG commitment. However, it is considered a difficulty to quantitatively measure a company's ESG premium and its impact to the company's stock price using scholar big data. In this paper, we utilize ESG scholar data as alternative data to develop an automatic trading strategy and propose a practical machine learning approach to quantify the ESG premium of a company and capture the ESG alpha. First, we construct our ESG investment universe and apply feature engineering on the companies' ESG scholar data from the Microsoft Academic Graph database. Then, we train six complementary machine learning models using a combination of financial indicators and ESG scholar data features and employ an ensemble method to predict stock prices and automatically set up portfolio allocation. Finally, we manage our portfolio, trade and rebalance the portfolio allocation monthly using predicted stock prices. We backtest our ESG alpha strategy and compare its performance with benchmarks. The proposed ESG alpha strategy achieves a cumulative return of 2,154.4% during the backtesting period of ten years, which significantly outperforms the NASDAQ-100 index's 397.4% and S&P 500's 226.9%. The traditional financial indicators results in only 1,443.7%, thus our scholar data-based ESG alpha strategy is better at capturing ESG premium than traditional financial indicators.","Qian Chen,Xiao-Yang Liu",,2020
42,Recommending cryptocurrency trading points with deep reinforcement learning approach,"The net profit of investors can rapidly increase if they correctly decide to take one of these three actions: buying, selling, or holding the stocks. The right action is related to massive stock market measurements. Therefore, defining the right action requires specific knowledge from investors. The economy scientists, following their research, have suggested several strategies and indicating factors that serve to find the best option for trading in a stock market. However, several investors’ capital decreased when they tried to trade the basis of the recommendation of these strategies. That means the stock market needs more satisfactory research, which can give more guarantee of success for investors. To address this challenge, we tried to apply one of the machine learning algorithms, which is called deep reinforcement learning (DRL) on the stock market. As a result, we developed an application that observes historical price movements and takes action on real-time prices. We tested our proposal algorithm with three—Bitcoin (BTC), Litecoin (LTC), and Ethereum (ETH)—crypto coins’ historical data. The experiment on Bitcoin via DRL application shows that the investor got 14.4% net profits within one month. Similarly, tests on Litecoin and Ethereum also finished with 74% and 41% profit, respectively.","Otabek Sattarov,Azamjon Muminov,Cheol Won Lee,Cheol Won Lee,Hyun Kyu Kang,Hyun Kyu Kang,Hyun Kang,Ryum-Duck Oh,Junho Ahn,Hyung Jun Oh,Heung Seok Jeon",Applied Sciences,2020
43,A Deep Reinforcement Learning Framework for Optimal Trade Execution,"In this article, we propose a deep reinforcement learning based framework to learn to minimize trade execution costs by splitting a sell order into child orders and execute them sequentially over a fixed period. The framework is based on a variant of the Deep Q-Network (DQN) algorithm that integrates the Double DQN, Dueling Network, and Noisy Nets. In contrast to previous research work, which uses implementation shortfall as the immediate rewards, we use a shaped reward structure, and we also incorporate the zero-ending inventory constraint into the DQN algorithm by slightly modifying the Q-function updates relative to standard Q-learning at the final step.","Siyu Lin,Peter A. Beling,Peter A. Beling",,2021
44,Deep hedging,,"H. Buehler,L. Gonon,J. Teichmann,B. Wood",Quantitative Finance,
