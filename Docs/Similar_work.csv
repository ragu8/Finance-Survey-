S.no,Title,Abstract,Authors,Journal,Year,,,
1,Deep Direct Reinforcement Learning for Financial Signal Representation and Trading,"Can we train the computer to beat experienced traders for financial assert trading? In this paper, we try to address this challenge by introducing a recurrent deep neural network (NN) for real-time financial signal representation and trading. Our model is inspired by two biological-related learning concepts of deep learning (DL) and reinforcement learning (RL). In the framework, the DL part automatically senses the dynamic market condition for informative feature learning. Then, the RL module interacts with deep representations and makes trading decisions to accumulate the ultimate rewards in an unknown environment. The learning system is implemented in a complex NN that exhibits both the deep and recurrent structures. Hence, we propose a task-aware backpropagation through time method to cope with the gradient vanishing issue in deep training. The robustness of the neural system is verified on both the stock and the commodity future markets under broad testing conditions.","Yue Deng,Feng Bao,Youyong Kong,Youyong Kong,Zhiquan Ren,Qionghai Dai",IEEE Transactions on Neural Networks,2017,,,
2,Reinforcement Learning: An Introduction,"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.","Richard S. Sutton,Richard S. Sutton,Andrew G. Barto",,1988,,,
3,Learning to trade via direct reinforcement,"We present methods for optimizing portfolios, asset allocations, and trading systems based on direct reinforcement (DR). In this approach, investment decision-making is viewed as a stochastic control problem, and strategies are discovered directly. We present an adaptive algorithm called recurrent reinforcement learning (RRL) for discovering investment policies. The need to build forecasting models is eliminated, and better trading performance is obtained. The direct reinforcement approach differs from dynamic programming and reinforcement algorithms such as TD-learning and Q-learning, which attempt to estimate a value function for the control problem. We find that the RRL direct reinforcement framework enables a simpler problem representation, avoids Bellman's curse of dimensionality and offers compelling advantages in efficiency. We demonstrate how direct reinforcement can be used to optimize risk-adjusted investment returns (including the differential Sharpe ratio), while accounting for the effects of transaction costs. In extensive simulation work using real financial data, we find that our approach based on RRL produces better trading strategies than systems utilizing Q-learning (a value function method). Real-world applications include an intra-daily currency trader and a monthly asset allocation system for the S&P 500 Stock Index and T-Bills.","John Moody,Matthew Saffell",IEEE Transactions on Neural Networks,2001,,,
4,Deep Reinforcement Learning for Trading,"In this article, the authors adopt deep reinforcement learning algorithms to design trading strategies for continuous futures contracts. Both discrete and continuous action spaces are considered, and volatility scaling is incorporated to create reward functions that scale trade positions based on market volatility. They test their algorithms on 50 very liquid futures contracts from 2011 to 2019 and investigate how performance varies across different asset classes, including commodities, equity indexes, fixed income, and foreign exchange markets. They compare their algorithms against classical time-series momentum strategies and show that their method outperforms such baseline models, delivering positive profits despite heavy transaction costs. The experiments show that the proposed algorithms can follow large market trends without changing positions and can also scale down, or hold, through consolidation periods.  TOPICS:Futures and forward contracts, exchanges/markets/clearinghouses, statistical methods, simulations  Key Findings  • In this article, the authors introduce reinforcement learning algorithms to design trading strategies for futures contracts. They investigate both discrete and continuous action spaces and improve reward functions by using volatility scaling to scale trade positions based on market volatility.  • The authors discuss the connection between modern portfolio theory and the reinforcement learning reward hypothesis and show that they are equivalent if a linear utility function is used.  • The authors back test their methods on 50 very liquid futures contracts from 2011 to 2019, and their algorithms deliver positive profits despite heavy transaction costs.","Zihao Zhang,Zihao Zhang,Zihao Zhang,Stefan Zohren,Stephen J. Roberts,Roberts Stephen",,2020,,,
5,Proximal Policy Optimization Algorithms,"We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ""surrogate"" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.","John Schulman,Filip Wolski,Filip Wolski,Prafulla Dhariwal,Alec Radford,Oleg Klimov",arXiv: Learning,2017,,,
6,A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem,"Financial portfolio management is the process of constant redistribution of a fund into different financial products. This paper presents a financial-model-free Reinforcement Learning framework to provide a deep machine learning solution to the portfolio management problem. The framework consists of the Ensemble of Identical Independent Evaluators (EIIE) topology, a Portfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL) scheme, and a fully exploiting and explicit reward function. This framework is realized in three instants in this work with a Convolutional Neural Network (CNN), a basic Recurrent Neural Network (RNN), and a Long Short-Term Memory (LSTM). They are, along with a number of recently reviewed or published portfolio-selection strategies, examined in three back-test experiments with a trading period of 30 minutes in a cryptocurrency market. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. All three instances of the framework monopolize the top three positions in all experiments, outdistancing other compared trading algorithms. Although with a high commission rate of 0.25% in the backtests, the framework is able to achieve at least 4-fold returns in 50 days.","Zhengyao Jiang,Zhengyao Jiang,Dixing Xu,Jinjun Liang,Jinjun Liang",arXiv: Computational Finance,2017,,,
7,Human-level control through deep reinforcement learning,"An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.","Volodymyr Mnih,Koray Kavukcuoglu,David Silver,Andrei A. Rusu,Joel Veness,Marc G. Bellemare,Alex Graves,Martin Riedmiller,Andreas K. Fidjeland,Georg Ostrovski,Stig Petersen,Charles Beattie,Amir Sadik,Ioannis Antonoglou,Ioannis Antonoglou,Helen King,Dharshan Kumaran,Daan Wierstra,Shane Legg,Demis Hassabis",Nature,2015,,,
8,Mastering the game of Go with deep neural networks and tree search,"The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of stateof-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.","David Silver,Aja Huang,Chris J. Maddison,Arthur Guez,Laurent Sifre,George van den Driessche,Julian Schrittwieser,Ioannis Antonoglou,Ioannis Antonoglou,Veda Panneershelvam,Marc Lanctot,Sander Dieleman,Dominik Grewe,John Nham,Nal Kalchbrenner,Ilya Sutskever,Timothy Lillicrap,Madeleine Leach,Koray Kavukcuoglu,Thore Graepel,Demis Hassabis",Nature,2016,,,
9,Asynchronous methods for deep reinforcement learning,"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.","Volodymyr Mnih,Adrià Puigdomènech Badia,Mehdi Mirza,Alex Graves,Tim Harley,Timothy Lillicrap,David Silver,Koray Kavukcuoglu",,2016,,,
10,Application of Deep Reinforcement Learning on Automated Stock Trading,"How to make right decisions in stock trading is a vital and challenging task for investors. Since deep reinforcement learning (DRL) has outperformed human beings in many fields such as playing Atari Games, can a DRL agent automatically make trading decisions and achieve long-term stable profits? In this paper, we try to solve this challenge by applying Deep Q-network (DQN) and Deep Recurrent Q-network (DRQN) in stock trading and try to build an end-to-end daily stock trading system which can decide to buy or to sell automatically at each trading day. The S…P500 ETF is selected as our trading asset and its daily trading data are used as the state of the trading environment. The agent’s performance is evaluated by comparing with benchmarks of Buy and Hold (BH) and Random action-selected DQN trader. Experiment results show that our DQN trader outperforms the two benchmarks and DRQN trader is even better than DQN trader mainly because the recurrence framework can discover and exploit profitable patterns hidden in time-related sequence.","Lin Chen,Qiang Gao",,2019,,,
11,Testing different Reinforcement Learning configurations for financial trading: Introduction and applications,"Abstract   The construction of automatic Financial Trading Systems (FTSs) is a subject of research of high interest for both academic environment and financial one due to the potential promises by self-learning methodologies and by the increasing power of actual computers. In this paper we consider Reinforcement Learning (RL) type algorithms, that is algorithms that optimize their behavior in relation to the responses they get from the environment in which they operate, without the need for a supervisor. In particular, first we introduce the essential aspects of RL which are of interest for our purposes, then we present some original automatic FTSs based on differently configured RL algorithms and apply such FTSs to artificial and real time series of daily financial asset prices.","Francesco Bertoluzzo,Francesco Bertoluzzo,Marco Corazza",Procedia. Economics and finance,2012,,,
12,Continuous control with deep reinforcement learning,"We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.","Timothy Lillicrap,Jonathan J. Hunt,Alexander Pritzel,Nicolas Heess,Tom Erez,Yuval Tassa,David Silver,Daan Wierstra",arXiv: Learning,2015,,,
13,Continuous control with deep reinforcement learning,"Abstract: We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.","Timothy Lillicrap,Jonathan J. Hunt,Alexander Pritzel,Nicolas Heess,Tom Erez,Yuval Tassa,David Silver,Daan Wierstra",,2016,,,
14,Playing Atari with Deep Reinforcement Learning,"We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.","Volodymyr Mnih,Koray Kavukcuoglu,David Silver,Alex Graves,Ioannis Antonoglou,Daan Wierstra,Martin Riedmiller",arXiv: Learning,2013,,,
15,Reinforcement Learning for Trading,"We propose to train trading systems by optimizing financial objective functions via reinforcement learning. The performance functions that we consider are profit or wealth, the Sharpe ratio and our recently proposed differential Sharpe ratio for online learning. In Moody & Wu (1997), we presented empirical results that demonstrate the advantages of reinforcement learning relative to supervised learning. Here we extend our previous work to compare Q-Learning to our Recurrent Reinforcement Learning (RRL) algorithm. We provide new simulation results that demonstrate the presence of predictability in the monthly S&P 500 Stock Index for the 25 year period 1970 through 1994, as well as a sensitivity analysis that provides economic insight into the trader's structure.","John Moody,Matthew Saffell",,1998,,,
16,"Improving financial trading decisions using deep Q-learning: Predicting the number of shares, action strategies, and transfer learning","Abstract   We study trading systems using reinforcement learning with three newly proposed methods to maximize total profits and reflect real financial market situations while overcoming the limitations of financial data. First, we propose a trading system that can predict the number of shares to trade. Specifically, we design an automated system that predicts the number of shares by adding a deep neural network (DNN) regressor to a deep Q-network, thereby combining reinforcement learning and a DNN. Second, we study various action strategies that use Q-values to analyze which action strategies are beneficial for profits in a confused market. Finally, we propose transfer learning approaches to prevent overfitting from insufficient financial data. We use four different stock indices—the S&P500, KOSPI, HSI, and EuroStoxx50—to experimentally verify our proposed methods and then conduct extensive research. The proposed automated trading system, which enables us to predict the number of shares with the DNN regressor, increases total profits by four times in S&P500, five times in KOSPI, 12 times in HSI, and six times in EuroStoxx50 compared with the fixed-number trading system. When the market situation is confused, delaying the decision to buy or sell increases total profits by 18% in S&P500, 24% in KOSPI, and 49% in EuroStoxx50. Further, transfer learning increases total profits by twofold in S&P500, 3 times in KOSPI, twofold in HSI, and 2.5 times in EuroStoxx50. The trading system with all three proposed methods increases total profits by 13 times in S&P500, 24 times in KOSPI, 30 times in HSI, and 18 times in EuroStoxx50, outperforming the market and the reinforcement learning model.","Gyeeun Jeong,Ha Young Kim,Ha-young Kim",Expert Systems With Applications,2019,,,
17,Cryptocurrency portfolio management with deep reinforcement learning,"Portfolio management is the decision-making process of allocating an amount of fund into different financial investment products. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. This paper presents a model-less convolutional neural network with historic prices of a set of financial assets as its input, outputting portfolio weights of the set. The network is trained with 0.7 years' price data from a cryptocurrency exchange. The training is done in a reinforcement manner, maximizing the accumulative return, which is regarded as the reward function of the network. Back test trading experiments with trading period of 30 minutes is conducted in the same market, achieving 10-fold returns in 1.8 month's periods. Some recently published portfolio selection strategies are also used to perform the same back tests, whose results are compared with the neural network. The network is not limited to cryptocurrency, but can be applied to any other financial markets.","Zhengyao Jiang,Jinjun Liang",,2017,,,
18,AlphaStock: A Buying-Winners-and-Selling-Losers Investment Strategy using Interpretable Deep Reinforcement Attention Networks,"Recent years have witnessed the successful marriage of finance innovations and AI techniques in various finance applications including quantitative trading (QT). Despite great research efforts devoted to leveraging deep learning (DL) methods for building better QT strategies, existing studies still face serious challenges especially from the side of finance, such as the balance of risk and return, the resistance to extreme loss, and the interpretability of strategies, which limit the application of DL-based strategies in real-life financial markets. In this work, we propose AlphaStock, a novel reinforcement learning (RL) based investment strategy enhanced by interpretable deep attention networks, to address the above challenges. Our main contributions are summarized as follows: i) We integrate deep attention networks with a Sharpe ratio-oriented reinforcement learning framework to achieve a risk-return balanced investment strategy; ii) We suggest modeling interrelationships among assets to avoid selection bias and develop a cross-asset attention mechanism; iii) To our best knowledge, this work is among the first to offer an interpretable investment strategy using deep reinforcement learning models. The experiments on long-periodic U.S. and Chinese markets demonstrate the effectiveness and robustness of AlphaStock over diverse market states. It turns out that AlphaStock tends to select the stocks as winners with high long-term growth, low volatility, high intrinsic value, and being undervalued recently.","Jingyuan Wang,Jingyuan Wang,Yang Zhang,Ke Tang,Ke Tang,Junjie Wu,Junjie Wu,Junjie Wu,Zhang Xiong,Zhang Xiong,Zhang Xiong,Zhang Xiong",,2019,,,
19,Practical Deep Reinforcement Learning Approach for Stock Trading,"Stock trading strategy plays a crucial role in investment companies. However, it is challenging to obtain optimal strategy in the complex and dynamic stock market. We explore the potential of deep reinforcement learning to optimize stock trading strategy and thus maximize investment return. 30 stocks are selected as our trading stocks and their daily prices are used as the training and trading market environment. We train a deep reinforcement learning agent and obtain an adaptive trading strategy. The agent's performance is evaluated and compared with Dow Jones Industrial Average and the traditional min-variance portfolio allocation strategy. The proposed deep reinforcement learning approach is shown to outperform the two baselines in terms of both the Sharpe ratio and cumulative returns.","Zhuoran Xiong,Xiao-Yang Liu,Shan Zhong,Hongyang Yang,Anwar Walid",,2018,,,
20,Deep hedging,,"H. Buehler,L. Gonon,J. Teichmann,B. Wood",Quantitative Finance,,,,
21,An automated FX trading system using adaptive reinforcement learning,"This paper introduces adaptive reinforcement learning (ARL) as the basis for a fully automated trading system application. The system is designed to trade foreign exchange (FX) markets and relies on a layered structure consisting of a machine learning algorithm, a risk management overlay and a dynamic utility optimization layer. An existing machine-learning method called recurrent reinforcement learning (RRL) was chosen as the underlying algorithm for ARL. One of the strengths of our approach is that the dynamic optimization layer makes a fixed choice of model tuning parameters unnecessary. It also allows for a risk-return trade-off to be made by the user within the system. The trading system is able to make consistent gains out-of-sample while avoiding large draw-downs.","Michael A. H. Dempster,V. Leemans",Expert Systems With Applications,2006,,,
22,Fuzzy adaptive decision-making for boundedly rational traders in speculative stock markets,"The development of new models that would enhance predictability for time series with dynamic time-varying, nonlinear features is a major challenge for speculators. Boundedly rational investors called ""chartists"" use advanced heuristics and rules-of-thumb to make profit by trading, or even hedge against potential market risks. This paper introduces a hybrid neurofuzzy system for decision-making and trading under uncertainty. The efficiency of a technical trading strategy based on the neurofuzzy model is investigated, in order to predict the direction of the market for 10 of the most prominent stock indices of U.S.A, Europe and Southeast Asia. It is demonstrated via an extensive empirical analysis that the neurofuzzy model allows technical analysts to earn significantly higher returns by providing valid information for a potential turning point on the next trading day. The total profit of the proposed neurofuzzy model, including transaction costs, is consistently superior to a recurrent neural network and a Buy & Hold strategy for all indices, particularly for the highly speculative, emerging Southeast Asian markets. Optimal prediction is based on the dynamic update and adaptive calibration of the heuristic fuzzy learning rules, which reflect the psychological and behavioral patterns of the traders.",Stelios D. Bekiros,European Journal of Operational Research,2010,,,
23,Long short-term memory,"Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.","Sepp Hochreiter,Jürgen Schmidhuber",Neural Computation,1997,,,
24,Optimal Asset Allocation using Adaptive Dynamic Programming,"In recent years, the interest of investors has shifted to computerized asset allocation (portfolio management) to exploit the growing dynamics of the capital markets. In this paper, asset allocation is formalized as a Markovian Decision Problem which can be optimized by applying dynamic programming or reinforcement learning based algorithms. Using an artificial exchange rate, the asset allocation strategy optimized with reinforcement learning (Q-Learning) is shown to be equivalent to a policy computed by dynamic programming. The approach is then tested on the task to invest liquid capital in the German stock market. Here, neural networks are used as value function approximators. The resulting asset allocation strategy is superior to a heuristic benchmark policy. This is a further example which demonstrates the applicability of neural network based reinforcement learning to a problem setting with a high dimensional state space.",Ralph Neuneier,,1995,,,
25,Enhancing Q-Learning for Optimal Asset Allocation,"This paper enhances the Q-learning algorithm for optimal asset allocation proposed in (Neuneier, 1996 [6]). The new formulation simplifies the approach by using only one value-function for many assets and allows model-free policy-iteration. After testing the new algorithm on real data, the possibility of risk management within the framework of Markov decision problems is analyzed. The proposed methods allows the construction of a multi-period portfolio management system which takes into account transaction costs, the risk preferences of the investor, and several constraints on the allocation.",Ralph Neuneier,,1997,,,
26,Deep reinforcement learning with double Q-Learning,"The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.","Hado van Hasselt,Arthur Guez,David Silver",,2016,,,
27,Online Portfolio Selection Strategy Based on Combining Experts' Advice,"The weak aggregating algorithm (WAA) developed from learning and prediction with expert advice makes decisions by considering all the experts' advice, and each expert's weight is updated according to his performance in previous periods. In this paper, we apply the WAA to the online portfolio selection problem. We first consider a simple case in which the expert advice is the strategy for investing in one stock; for this case, we obtain a portfolio selection strategy WAAS and prove that the WAAS can identify the best stock. We also discuss a more complicated case in which constant rebalanced portfolios are considered as expert advice, and obtain a corresponding portfolio selection strategy WAAC. The theoretical result shows that the cumulative gain that WAAC achieves is as large as that of the best constant rebalanced portfolio. Numerical analysis shows that the cumulative gains of our proposed strategies are as large as those of the best expert advice.","Yong Zhang,Xingyu Yang",Computing in Economics and Finance,2017,,,
28,An adaptive portfolio trading system,"A reinforcement learning trading algorithm with expected drawdown risk is proposed.The expected maximum drawdown is shown to improve portfolio signal generation.The effectiveness of the method is validated using different transaction costs.An adaptive portfolio rebalancing system with automated retraining is recommended. Dynamic control theory has long been used in solving optimal asset allocation problems, and a number of trading decision systems based on reinforcement learning methods have been applied in asset allocation and portfolio rebalancing. In this paper, we extend the existing work in recurrent reinforcement learning (RRL) and build an optimal variable weight portfolio allocation under a coherent downside risk measure, the expected maximum drawdown, E(MDD). In particular, we propose a recurrent reinforcement learning method, with a coherent risk adjusted performance objective function, the Calmar ratio, to obtain both buy and sell signals and asset allocation weights. Using a portfolio consisting of the most frequently traded exchange-traded funds, we show that the expected maximum drawdown risk based objective function yields superior return performance compared to previously proposed RRL objective functions (i.e. the Sharpe ratio and the Sterling ratio), and that variable weight RRL long/short portfolios outperform equal weight RRL long/short portfolios under different transaction cost scenarios. We further propose an adaptive E(MDD) risk based RRL portfolio rebalancing decision system with a transaction cost and market condition stop-loss retraining mechanism, and we show that the proposed portfolio trading system responds to transaction cost effects better and outperforms hedge fund benchmarks consistently.","Saud Almahdi,Steve Y. Yang",Expert Systems With Applications,2017,,,
29,Financial Trading as a Game: A Deep Reinforcement Learning Approach,"An automatic program that generates constant profit from the financial market is lucrative for every market practitioner. Recent advance in deep reinforcement learning provides a framework toward end-to-end training of such trading agent. In this paper, we propose an Markov Decision Process (MDP) model suitable for the financial trading task and solve it with the state-of-the-art deep recurrent Q-network (DRQN) algorithm. We propose several modifications to the existing learning algorithm to make it more suitable under the financial trading setting, namely 1. We employ a substantially small replay memory (only a few hundreds in size) compared to ones used in modern deep reinforcement learning algorithms (often millions in size.) 2. We develop an action augmentation technique to mitigate the need for random exploration by providing extra feedback signals for all actions to the agent. This enables us to use greedy policy over the course of learning and shows strong empirical performance compared to more commonly used epsilon-greedy exploration. However, this technique is specific to financial trading under a few market assumptions. 3. We sample a longer sequence for recurrent neural network training. A side product of this mechanism is that we can now train the agent for every T steps. This greatly reduces training time since the overall computation is down by a factor of T. We combine all of the above into a complete online learning algorithm and validate our approach on the spot foreign exchange market.",Chien Yi Huang,arXiv: Trading and Market Microstructure,2018,,,
30,Adversarial Deep Reinforcement Learning in Portfolio Management,"In this paper, we implement three state-of-art continuous reinforcement learning algorithms, Deep Deterministic Policy Gradient (DDPG), Proximal Policy Optimization (PPO) and Policy Gradient (PG)in portfolio management. All of them are widely-used in game playing and robot control. What's more, PPO has appealing theoretical propeties which is hopefully potential in portfolio management. We present the performances of them under different settings, including different learning rates, objective functions, feature combinations, in order to provide insights for parameters tuning, features selection and data preparation. We also conduct intensive experiments in China Stock market and show that PG is more desirable in financial market than DDPG and PPO, although both of them are more advanced. What's more, we propose a so called Adversarial Training method and show that it can greatly improve the training efficiency and significantly promote average daily return and sharpe ratio in back test. Based on this new modification, our experiments results show that our agent based on Policy Gradient can outperform UCRP.","Zhipeng Liang,Hao Chen,Junhao Zhu,Kangkang Jiang,Yanran Li",arXiv: Portfolio Management,2018,,,
31,Reinforcement learning in financial markets - a survey,"The advent of reinforcement learning (RL) in financial markets is driven by several advantages inherent to this field of artificial intelligence. In particular, RL allows to combine the ""prediction"" and the ""portfolio construction"" task in one integrated step, thereby closely aligning the machine learning problem with the objectives of the investor. At the same time, important constraints, such as transaction costs, market liquidity, and the investor's degree of risk-aversion, can be conveniently taken into account. Over the past two decades, and albeit most attention still being devoted to supervised learning methods, the RL research community has made considerable advances in the finance domain. The present paper draws insights from almost 50 publications, and categorizes them into three main approaches, i.e., critic-only approach, actor-only approach, and actor-critic approach. Within each of these categories, the respective contributions are summarized and reviewed along the representation of the state, the applied reward function, and the action space of the agent. This cross-sectional perspective allows us to identify recurring design decisions as well as potential levers to improve the agent's performance. Finally, the individual strengths and weaknesses of each approach are discussed, and directions for future research are pointed out.",Thomas G. Fischer,,2018,,,
32,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,"We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.","Ryan Lowe,Yi Wu,Aviv Tamar,Jean Harb,OpenAI Pieter Abbeel,Pieter Abbeel,Igor Mordatch",,2017,,,
33,Enhancing Time-Series Momentum Strategies Using Deep Neural Networks,"While time series momentum is a well-studied phenomenon in finance, common strategies require the explicit definition of both a trend estimator and a position sizing rule. In this paper, we introduce Deep Momentum Networks -- a hybrid approach which injects deep learning based trading rules into the volatility scaling framework of time series momentum. The model also simultaneously learns both trend estimation and position sizing in a data-driven manner, with networks directly trained by optimising the Sharpe ratio of the signal. Backtesting on a portfolio of 88 continuous futures contracts, we demonstrate that the Sharpe-optimised LSTM improved traditional methods by more than two times in the absence of transactions costs, and continue outperforming when considering transaction costs up to 2-3 basis points. To account for more illiquid assets, we also propose a turnover regularisation term which trains the network to factor in costs at run-time.","Bryan Lim,Stefan Zohren,Stephen J. Roberts",,2019,,,
34,Grandmaster level in StarCraft II using multi-agent reinforcement learning.,"Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players. AlphaStar uses a multi-agent reinforcement learning algorithm and has reached Grandmaster level, ranking among the top 0.2% of human players for the real-time strategy game StarCraft II.","Oriol Vinyals,Igor Babuschkin,Igor Babuschkin,Wojciech Marian Czarnecki,Michael Mathieu,Andrew Dudzik,Junyoung Chung,David H. Choi,Richard E. Powell,Timo Ewalds,Petko Georgiev,Junhyuk Oh,Dan Horgan,Manuel Kroiss,Ivo Danihelka,Aja Huang,Laurent Sifre,Trevor Cai,John Agapiou,Max Jaderberg,Alexander Vezhnevets,Rémi Leblond,Rémi Leblond,Tobias Pohlen,Valentin Dalibard,David Budden,Yury Sulsky,James Molloy,Tom Le Paine,Caglar Gulcehre,Ziyu Wang,Tobias Pfaff,Yuhuai Wu,Roman Ring,Dani Yogatama,Dario Wünsch,Katrina McKinney,Oliver Smith,Tom Schaul,Timothy Lillicrap,Koray Kavukcuoglu,Demis Hassabis,Chris Apps,David Silver",Nature,2019,,,
35,Reinforcement Learning in Stock Trading.,"Using machine learning techniques in financial markets, particularly in stock trading, attracts a lot of attention from both academia and practitioners in recent years. Researchers have studied different supervised and unsupervised learning techniques to either predict stock price movement or make decisions in the market.","Quang-Vinh Dang,Quang-Vinh Dang",,2019,,,
36,Recommending cryptocurrency trading points with deep reinforcement learning approach,"The net profit of investors can rapidly increase if they correctly decide to take one of these three actions: buying, selling, or holding the stocks. The right action is related to massive stock market measurements. Therefore, defining the right action requires specific knowledge from investors. The economy scientists, following their research, have suggested several strategies and indicating factors that serve to find the best option for trading in a stock market. However, several investors’ capital decreased when they tried to trade the basis of the recommendation of these strategies. That means the stock market needs more satisfactory research, which can give more guarantee of success for investors. To address this challenge, we tried to apply one of the machine learning algorithms, which is called deep reinforcement learning (DRL) on the stock market. As a result, we developed an application that observes historical price movements and takes action on real-time prices. We tested our proposal algorithm with three—Bitcoin (BTC), Litecoin (LTC), and Ethereum (ETH)—crypto coins’ historical data. The experiment on Bitcoin via DRL application shows that the investor got 14.4% net profits within one month. Similarly, tests on Litecoin and Ethereum also finished with 74% and 41% profit, respectively.","Otabek Sattarov,Azamjon Muminov,Cheol Won Lee,Cheol Won Lee,Hyun Kyu Kang,Hyun Kyu Kang,Hyun Kang,Ryum-Duck Oh,Junho Ahn,Hyung Jun Oh,Heung Seok Jeon",Applied Sciences,2020,,,
37,Adaptive stock trading strategies with deep reinforcement learning methods,"Abstract   The increasing complexity and dynamical property in stock markets are key challenges of the financial industry, in which inflexible trading strategies designed by experienced financial practitioners fail to achieve satisfactory performance in all market conditions. To meet this challenge, adaptive stock trading strategies with deep reinforcement learning methods are proposed. For the time-series nature of stock market data, the Gated Recurrent Unit (GRU) is applied to extract informative financial features, which can represent the intrinsic characteristics of the stock market for adaptive trading decisions. Furthermore, with the tailored design of state and action spaces, two trading strategies with reinforcement learning methods are proposed as GDQN (Gated Deep Q-learning trading strategy) and GDPG (Gated Deterministic Policy Gradient trading strategy). To verify the robustness and effectiveness of GDQN and GDPG, they are tested both in the trending and in the volatile stock market from different countries. Experimental results show that the proposed GDQN and GDPG not only outperform the Turtle trading strategy but also achieve more stable returns than a state-of-the-art direct reinforcement learning method, DRL trading strategy, in the volatile stock market. As far as the GDQN and the GDPG are compared, experimental results demonstrate that the GDPG with an actor-critic framework is more stable than the GDQN with a critic-only framework in the ever-evolving stock market.","Xing Wu,Haolei Chen,Jianjia Wang,Jianjia Wang,Jianjia Wang,Luigi Troiano,Vincenzo Loia,Hamido Fujita,Hamido Fujita,Hamido Fujita,Hamido Fujita",Information Sciences,2020,,,
38,Adversarial Deep Reinforcement Learning in Portfolio Management,"In this paper, we implement three state-of-art continuous reinforcement learning algorithms, Deep Deterministic Policy Gradient (DDPG), Proximal Policy Optimization (PPO) and Policy Gradient (PG)in portfolio management. All of them are widely-used in game playing and robot control. What's more, PPO has appealing theoretical propeties which is hopefully potential in portfolio management. We present the performances of them under different settings, including different learning rates, objective functions, feature combinations, in order to provide insights for parameters tuning, features selection and data preparation. We also conduct intensive experiments in China Stock market and show that PG is more desirable in financial market than DDPG and PPO, although both of them are more advanced. What's more, we propose a so called Adversarial Training method and show that it can greatly improve the training efficiency and significantly promote average daily return and sharpe ratio in back test. Based on this new modification, our experiments results show that our agent based on Policy Gradient can outperform UCRP.","Zhipeng Liang,Hao Chen,Junhao Zhu,Kangkang Jiang,Yanran Li",,2018,,,
39,Passive Investment Strategies and Efficient Markets,"This paper presents the case for and the evidence in favour of passive investment strategies and examines the major criticisms of the technique. I conclude that the evidence strongly supports passive investment management in all markets— smallcapitalisation stocks as well as large-capitalisation equities, US markets as well as international markets, and bonds as well as stocks. Recent attacks on the efficient market hypothesis do not weaken the case for indexing.",Burton G. Malkiel,European Financial Management,2003,,,
40,Optimistic Bull or Pessimistic Bear: Adaptive Deep Reinforcement Learning for Stock Portfolio Allocation,"Portfolio allocation is crucial for investment companies. However, getting the best strategy in a complex and dynamic stock market is challenging. In this paper, we propose a novel Adaptive Deep Deterministic Reinforcement Learning scheme (Adaptive DDPG) for the portfolio allocation task, which incorporates optimistic or pessimistic deep reinforcement learning that is reflected in the influence from prediction errors. Dow Jones 30 component stocks are selected as our trading stocks and their daily prices are used as the training and testing data. We train the Adaptive DDPG agent and obtain a trading strategy. The Adaptive DDPG's performance is compared with the vanilla DDPG, Dow Jones Industrial Average index and the traditional min-variance and mean-variance portfolio allocation strategies. Adaptive DDPG outperforms the baselines in terms of the investment return and the Sharpe ratio.","Xinyi Li,Yinchuan Li,Yinchuan Li,Yuancheng Zhan,Xiao-Yang Liu,Xiao-Yang Liu,Xiao-Yang Liu",,2019,,,
41,An application of deep reinforcement learning to algorithmic trading,"Abstract   This scientific research paper presents an innovative approach based on deep reinforcement learning (DRL) to solve the algorithmic trading problem of determining the optimal trading position at any point in time during a trading activity in the stock market. It proposes a novel DRL trading policy so as to maximise the resulting Sharpe ratio performance indicator on a broad range of stock markets. Denominated the Trading Deep Q-Network algorithm (TDQN), this new DRL approach is inspired from the popular DQN algorithm and significantly adapted to the specific algorithmic trading problem at hand. The training of the resulting reinforcement learning (RL) agent is entirely based on the generation of artificial trajectories from a limited set of stock market historical data. In order to objectively assess the performance of trading strategies, the research paper also proposes a novel, more rigorous performance assessment methodology. Following this new performance assessment approach, promising results are reported for the TDQN algorithm.","Thibaut Théate,Damien Ernst",Expert Systems With Applications,2021,,,
42,FinRL: Deep Reinforcement Learning Framework to Automate Trading in Quantitative Finance,"Deep reinforcement learning (DRL) has been envisioned to have a competitive edge in quantitative finance. However, there is a steep development curve for quantitative traders to obtain an agent that automatically positions to win in the market, namely \textit{to decide where to trade, at what price} and \textit{what quantity}, due to the error-prone programming and arduous debugging. In this paper, we present the first open-source framework \textit{FinRL} as a full pipeline to help quantitative traders overcome the steep learning curve. FinRL is featured with simplicity, applicability and extensibility under the key principles, \textit{full-stack framework, customization, reproducibility} and \textit{hands-on tutoring}. Embodied as a three-layer architecture with modular structures, FinRL implements fine-tuned state-of-the-art DRL algorithms and common reward functions, while alleviating the debugging workloads. Thus, we help users pipeline the strategy design at a high turnover rate. At multiple levels of time granularity, FinRL simulates various markets as training environments using historical data and live trading APIs. Being highly extensible, FinRL reserves a set of user-import interfaces and incorporates trading constraints such as market friction, market liquidity and investor's risk-aversion. Moreover, serving as practitioners' stepping stones, typical trading tasks are provided as step-by-step tutorials, e.g., stock trading, portfolio allocation, cryptocurrency trading, etc.","Xiao-Yang Liu,Hongyang Yang,Jiechao Gao,Jiechao Gao,Christina Dan Wang",Research Papers in Economics,2021,,,
43,Bitcoin Is Volatile! Isn’t that Right?,"In this study, we substantiate with financial data collection and analysis the hypothesis regarding the volatility of Bitcoin exchange rate against common currencies. Financial data were collected from July 2010 until April 2014. The raw annualised volatility of Bitcoin is compared to conventional and major exchange rates. The first set of results indicate a high value of annualised volatility for the Bitcoin exchange rate. When the volume of Bitcoin transactions is considered, the volatility of the Bitcoin exchange rate stabilizes significantly.","Svetlana Sapuric,Angelika I. Kokkinaki",,2014,,,
44,Adam: A Method for Stochastic Optimization,"We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.","Diederik P. Kingma,Jimmy Ba",arXiv: Learning,2014,,,
45,Temporal abstraction in reinforcement learning,"Decision making usually involves choosing among different courses of action over a broad range of time scales. For instance, a person planning a trip to a distant location makes high-level decisions regarding what means of transportation to use, but also chooses low-level actions, such as the movements for getting into a car. The problem of picking an appropriate time scale for reasoning and learning has been explored in artificial intelligence, control theory and robotics. In this dissertation we develop a framework that allows novel solutions to this problem, in the context of Markov Decision Processes (MDPs) and reinforcement learning. 
In this dissertation, we present a general framework for prediction, control and learning at multiple temporal scales. In this framework, temporally extended actions are represented by a way of behaving (a policy) together with a termination condition. An action represented in this way is called an option. Options can be easily incorporated in MDPs, allowing an agent to use existing controllers, heuristics for picking actions, or learned courses of action. 
The effects of behaving according to an option can be predicted using multi-time models, learned by interacting with the environment. In this dissertation we develop multi-time models, and we illustrate the way in which they can be used to produce plans of behavior very quickly, using classical dynamic programming or reinforcement learning techniques. 
The most interesting feature of our framework is that it allows an agent to work simultaneously with high-level and low-level temporal representations. The interplay of these levels can be exploited in order to learn and plan more efficiently and more accurately. We develop new algorithms that take advantage of this structure to improve the quality of plans, and to learn in parallel about the effects of many different options.","Doina Precup,Doina Precup,Doina Precup,Richard S. Sutton,Richard S. Sutton",,2000,,,
46,Optimal execution of portfolio transactions,"We consider the execution of portfolio transactions with the aim of minimizing a combination of volatility risk and transaction costs arising from permanent and temporary market impact. For a simple linear cost model, we explicitly construct the efficient frontier in the space of time-dependent liquidation strategies, which have minimum expected cost for a given level of uncertainty. We may then select optimal strategies either by minimizing a quadratic utility function, or by minimizing Value at Risk. The latter choice leads to the concept of Liquidity-adjusted VAR, or L-VaR, that explicitly considers the best tradeoff between volatility risk and liquidation costs. ∗We thank Andrew Alford, Alix Baudin, Mark Carhart, Ray Iwanowski, and Giorgio De Santis (Goldman Sachs Asset Management), Robert Ferstenberg (ITG), Michael Weber (Merrill Lynch), Andrew Lo (Sloan School, MIT), and George Constaninides (Graduate School of Business, University of Chicago) for helpful conversations. This paper was begun while the first author was at the University of Chicago, and the second author was first at Morgan Stanley Dean Witter and then at Goldman Sachs Asset Management. †University of Toronto, Departments of Mathematics and Computer Science; almgren@math.toronto.edu ‡ICor Brokerage and Courant Institute of Mathematical Sciences; Neil.Chriss@ICorBroker.com","Robert Almgren,Neil Chriss",Journal of Risk,2001,,,
47,Portfolio Theory and Capital Markets,"William Sharpe's influential Portfolio Theory and Capital Management is as relevant today as when it was first published in 1970. McGraw-Hill is proud to reintroduce tiffs hard-to-Find classic in its original edition. Dr. Sharpe's groundbreaking approach to the Capital Asset Pricing Model (CAPM) laid tile foundation for today's most important investment tools and theories, gave the investment world the stillvital Sharpe Ratio -- and made him the co-recipient of the 1990 Nobel Prize in Economics!A new foreword helps place Dr. Sharpe's synthesis of portfolio and capital markets theories into today's financial environment, while his rules for the intelligent selection of investments tinder conditions of risk remain as fresh today as in 1970. Serious investors and students of finance will respect its history ... as they reabsorb its timeless lessons.",William F. Sharpe,,1970,,,
48,A reinforcement learning extension to the Almgren-Chriss framework for optimal trade execution,"Reinforcement learning is explored as a candidate machine learning technique to enhance existing analytical solutions for optimal trade execution with elements from the market microstructure. Given a volume-to-trade, fixed time horizon and discrete trading periods, the aim is to adapt a given volume trajectory such that it is dynamic with respect to favourable/unfavourable conditions during realtime execution, thereby improving overall cost of trading. We consider the standard Almgren-Chriss model with linear price impact as a candidate base model. This model is popular amongst sell-side institutions as a basis for arrival price benchmark execution algorithms. By training a learning agent to modify a volume trajectory based on the market's prevailing spread and volume dynamics, we are able to improve post-trade implementation shortfall by up to 10.3% on average compared to the base model, based on a sample of stocks and trade sizes in the South African equity market.","Dieter Hendricks,Diane Wilcox",,2014,,,
49,Allocative Efficiency of Markets with Zero-intelligence Traders: Market as a Partial Substitute for Individual Rationality,"We report market experiments in which human traders are replaced by ""zero-intelligence"" programs that submit random bids and offers. Imposing a budget constraint (i.e., not permitting traders to sell below their costs or buy above their values) is sufficient to raise the allocative efficiency of these auctions close to 100 percent. Allocative efficiency of a double auction derives largely from its structure, independent of traders' motivation, intelligence, or learning. Adam Smith's invisible hand may be more powerful than some may have thought; it can generate aggregate rationality not only from individual rationality but also from individual irrationality.","Dhananjay (Dan) K. Gode,Shyam Sunder",Journal of Political Economy,1993,,,
50,On Stefan Banach and some of his results,"In the paper a short biography of Stefan Banach, a few stories about Banach and the Scottish Cafe and some results that nowadays are named by Banach’s surname are presented.",Krzysztof Ciesielski,Banach Journal of Mathematical Analysis,2007,,,
51,Adaptive stock trading with dynamic asset allocation using reinforcement learning,"Stock trading is an important decision-making problem that involves both stock selection and asset management. Though many promising results have been reported for predicting prices, selecting stocks, and managing assets using machine-learning techniques, considering all of them is challenging because of their complexity. In this paper, we present a new stock trading method that incorporates dynamic asset allocation in a reinforcement-learning framework. The proposed asset allocation strategy, called meta policy (MP), is designed to utilize the temporal information from both stock recommendations and the ratio of the stock fund over the asset. Local traders are constructed with pattern-based multiple predictors, and used to decide the purchase money per recommendation. Formulating the MP in the reinforcement learning framework is achieved by a compact design of the environment and the learning agent. Experimental results using the Korean stock market show that the proposed MP method outperforms other fixed asset-allocation strategies, and reduces the risks inherent in local traders.","Jangmin O,Jongwoo Lee,Jae Won Lee,Byoung-Tak Zhang",Information Sciences,2006,,,
52,Going deeper with convolutions,"We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.","Christian Szegedy,Wei Liu,Yangqing Jia,Pierre Sermanet,Scott Reed,Dragomir Anguelov,Dumitru Erhan,Vincent Vanhoucke,Andrew Rabinovich",,2015,,,
53,"Skulls, Financial Turbulence, and Risk Management","Based on a methodology introduced in 1927 to analyze human skulls and later applied to turbulence in financial markets, this study shows how to use a statistically derived measure of financial turbulence to measure and manage risk and to improve investment performance. View a webinar based on this article.","Mark Kritzman,Yuanzhen Li,Yuanzhen Li",Financial Analysts Journal,2010,,,
54,Double Q-learning,"In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation.",Hado van Hasselt,,2010,,,
55,Hierarchical reinforcement learning with the MAXQ value function decomposition,"This paper presents a new approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The decomposition, known as the MAXQ decomposition, has both a procedural semantics--as a subroutine hierarchy--and a declarative semantics--as a representation of the value function of a hierarchical policy. MAXQ unifies and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. It is based on the assumption that the programmer can identify useful subgoals and define subtasks that achieve these subgoals. By defining such subgoals, the programmer constrains the set of policies that need to be considered during reinforcement learning. The MAXQ value function decomposition can represent the value function of any policy that is consistent with the given hierarchy. The decomposition also creates opportunities to exploit state abstractions, so that individual MDPs within the hierarchy can ignore large parts of the state space. This is important for the practical application of the method. This paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges with probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this nonhierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.",Thomas G. Dietterich,Journal of Artificial Intelligence Research,2000,,,
56,The Sharpe Ratio,". Over 25 years ago, in Sharpe [1966], I introduced a measure for the performance of mutual funds and proposed the term reward-to-variability ratio to describe it (the measure is also described in Sharpe [1975] ). While the measure has gained considerable popularity, the name has not. Other authors have termed the original version the Sharpe Index (Radcliff [1990, p. 286] and Haugen [1993, p. 315]), the Sharpe Measure (Bodie, Kane and Marcus [1993, p. 804], Elton and Gruber [1991, p. 652], and Reilly [1989, p.803]), or the Sharpe Ratio (Morningstar [1993, p. 24]). Generalized versions have also appeared under various names (see. for example, BARRA [1992, p. 21] and Capaul, Rowley and Sharpe [1993, p. 33]).",William F. Sharpe,The Journal of Portfolio Management,1994,,,
57,Mean-Variance Investing,"Mean-variance investing is all about diversification. Diversification considers assets holistically and exploits the interaction of assets with each other, rather than viewing assets in isolation. Holding a diversified portfolio allows investors to increase expected returns while reducing risks. In practice, mean-variance portfolios that constrain the mean, volatility, and correlation inputs to reduce sampling error have performed much better than unconstrained portfolios. These special cases include equal-weighted, minimum variance, and risk parity portfolios.",Andrew Ang,,2012,,,
58,Deterministic Policy Gradient Algorithms,"In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.","David Silver,Guy Lever,Nicolas Heess,Thomas Degris,Daan Wierstra,Martin Riedmiller",,2014,,,
59,A Survey on Transfer Learning,"A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.","Sinno Jialin Pan,Qiang Yang",IEEE Transactions on Knowledge and Data Engineering,2010,,,
60,An intelligent hybrid trading system for discovering trading rules for the futures market using rough sets and genetic algorithms,"Display Omitted This study proposes an intelligent hybrid trading system for discovering technicaltrading rules.This study deals with the optimization problem of data discretization and reducts.Rough set analysis is adopted to represent trading rules.A genetic algorithm is used to discover optimal and sub-optimal trading rules.To evaluate the proposed system, a sliding window method is applied. Discovering intelligent technical trading rules from nonlinear and complex stock market data, and then developing decision support trading systems, is an important challenge. The objective of this study is to develop an intelligent hybrid trading system for discovering technical trading rules using rough set analysis and a genetic algorithm (GA). In order to obtain better trading decisions, a novel rule discovery mechanism using a GA approach is proposed for solving optimization problems (i.e., data discretization and reducts) of rough set analysis when discovering technical trading rules for the futures market. Experiments are designed to test the proposed model against comparable approaches (i.e., random, correlation, and GA approaches). In addition, these comprehensive experiments cover most of the current trading system topics, including the use of a sliding window method (with or without validation dataset), the number of trading rules, and the size of training period. To evaluate an intelligent hybrid trading system, experiments were carried out on the historical data of the Korea Composite Stock Price Index 200 (KOSPI 200) futures market. In particular, trading performance is analyzed according to the number of sets of decision rules and the size of the training period for discovering trading rules for the testing period. The results show that the proposed model significantly outperforms the benchmark model in terms of the average return and as a risk-adjusted measure.","Youngmin Kim,Young Min Kim,Wonbin Ahn,Kyong Joo Oh,David Enke",Applied Soft Computing,2017,,,
61,Reinforcement learning with deep energy-based policies,"We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.","Tuomas Haarnoja,Haoran Tang,Pieter Abbeel,Sergey Levine",,2017,,,
62,Stock Price Prediction via Discovering Multi-Frequency Trading Patterns,"Stock prices are formed based on short and/or long-term commercial and trading activities that reflect different frequencies of trading patterns. However, these patterns are often elusive as they are affected by many uncertain political-economic factors in the real world, such as corporate performances, government policies, and even breaking news circulated across markets. Moreover, time series of stock prices are non-stationary and non-linear, making the prediction of future price trends much challenging. To address them, we propose a novel State Frequency Memory (SFM) recurrent network to capture the multi-frequency trading patterns from past market data to make long and short term predictions over time. Inspired by Discrete Fourier Transform (DFT), the SFM decomposes the hidden states of memory cells into multiple frequency components, each of which models a particular frequency of latent trading pattern underlying the fluctuation of stock price. Then the future stock prices are predicted as a nonlinear mapping of the combination of these components in an Inverse Fourier Transform (IFT) fashion. Modeling multi-frequency trading patterns can enable more accurate predictions for various time ranges: while a short-term prediction usually depends on high frequency trading patterns, a long-term prediction should focus more on the low frequency trading patterns targeting at long-term return. Unfortunately, no existing model explicitly distinguishes between various frequencies of trading patterns to make dynamic predictions in literature. The experiments on the real market data also demonstrate more competitive performance by the SFM as compared with the state-of-the-art methods.","Liheng Zhang,Charu C. Aggarwal,Guo-Jun Qi",,2017,,,
63,Machine Learning for Trading,"In multi-period trading with realistic market impact, determining the dynamic trading strategy that optimizes expected utility of final wealth is a hard problem. In this paper we show that, with an appropriate choice of the reward function, reinforcement learning techniques (specifically, Q-learning) can successfully handle the risk-averse case. We provide a proof of concept in the form of a simulated market which permits a statistical arbitrage even with trading costs. The Q-learning agent finds and exploits this arbitrage.",Gordon Ritter,Social Science Research Network,2017,,,
64,LightGBM: a highly efficient gradient boosting decision tree,"Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.","Guolin Ke,Guolin Ke,Qi Meng,Thomas William Finley,Taifeng Wang,Wei Chen,Wei Chen,Wei Chen,Wei Chen,Weidong Ma,Qiwei Ye,Tie-Yan Liu",,2017,,,
65,Listening to Chaotic Whispers: A Deep Learning Framework for News-oriented Stock Trend Prediction,"Stock trend prediction plays a critical role in seeking maximized profit from the stock investment. However, precise trend prediction is very difficult since the highly volatile and non-stationary nature of the stock market. Exploding information on the Internet together with the advancing development of natural language processing and text mining techniques have enabled investors to unveil market trends and volatility from online content. Unfortunately, the quality, trustworthiness, and comprehensiveness of online content related to stock market vary drastically, and a large portion consists of the low-quality news, comments, or even rumors. To address this challenge, we imitate the learning process of human beings facing such chaotic online news, driven by three principles: sequential content dependency, diverse influence, and effective and efficient learning. In this paper, to capture the first two principles, we designed a Hybrid Attention Networks(HAN) to predict the stock trend based on the sequence of recent related news. Moreover, we apply the self-paced learning mechanism to imitate the third principle. Extensive experiments on real-world stock market data demonstrate the effectiveness of our framework. A further simulation illustrates that a straightforward trading strategy based on our proposed framework can significantly increase the annualized return.","Ziniu Hu,Weiqing Liu,Jiang Bian,Xuanzhe Liu,Tie-Yan Liu,Tie-Yan Liu",,2018,,,
66,Deep Reinforcement Learning in Portfolio Management,"In this paper, we implement two state-of-art continuous reinforcement learning algorithms, Deep Deterministic Policy Gradient (DDPG) and Proximal Policy Optimization (PPO) in portfolio management. Both of them are widely-used in game playing and robot control. What's more, PPO has appealing theoretical propeties which is hopefully potential in portfolio management. We present the performances of them under different settings, including different learning rate, objective function, markets, feature combinations, in order to provide insights for parameter tuning, features selection and data preparation.","Zhipeng Liang,Kangkang Jiang,Hao Chen,Junhao Zhu,Yanran Li",,2018,,,
67,A Practical Machine Learning Approach for Dynamic Stock Recommendation,"Stock recommendation is vital to investment companies and investors. However, no single stock selection strategy will always win while analysts may not have enough time to check all S&P 500 stocks (the Standard & Poor's 500). In this paper, we propose a practical scheme that recommends stocks from S&P 500 using machine learning. Our basic idea is to buy and hold the top 20% stocks dynamically. First, we select representative stock indicators with good explanatory power. Secondly, we take five frequently used machine learning methods, including linear regression, ridge regression, stepwise regression, random forest and generalized boosted regression, to model stock indicators and quarterly log-return in a rolling window. Thirdly, we choose the model with the lowest Mean Square Error in each period to rank stocks. Finally, we test the selected stocks by conducting portfolio allocation methods such as equally weighted, mean-variance, and minimum-variance. Our empirical results show that the proposed scheme outperforms the long-only strategy on the S&P 500 index in terms of Sharpe ratio and cumulative returns.","Hongyang Yang,Hongyang Yang,Xiao-Yang Liu,Xiao-Yang Liu,Xiao-Yang Liu,Xiao-Yang Liu,Qingwei Wu",,2018,,,
68,Reinforcement Learning for High-Frequency Market Making.,"In this paper we present the first practical application of reinforcement learning to optimal market making in high-frequency trading.
States, actions, and reward formulations unique to high-frequency market making are proposed, including a novel use of the CARA utility as a
terminal reward for improving learning. We show that the optimal policy
trained using Q-learning outperforms state-of-the-art market making algorithms. Finally, we analyse the optimal reinforcement learning policies,
and the influence of the CARA utility from a trading perspective.","Ye-Sheen Lim,Denise Gorse",,2018,,,
69,Dopamine: A Research Framework for Deep Reinforcement Learning,"Deep reinforcement learning (deep RL) research has grown significantly in recent years. A number of software offerings now exist that provide stable, comprehensive implementations for benchmarking. At the same time, recent deep RL research has become more diverse in its goals. In this paper we introduce Dopamine, a new research framework for deep RL that aims to support some of that diversity. Dopamine is open-source, TensorFlow-based, and provides compact and reliable implementations of some state-of-the-art deep RL agents. We complement this offering with a taxonomy of the different research objectives in deep RL research. While by no means exhaustive, our analysis highlights the heterogeneity of research in the field, and the value of frameworks such as ours.","Pablo Samuel Castro,Subhodeep Moitra,Carles Gelada,Saurabh Kumar,Marc G. Bellemare",arXiv: Learning,2018,,,
70,Model-based Deep Reinforcement Learning for Dynamic Portfolio Optimization.,"Dynamic portfolio optimization is the process of sequentially allocating wealth to a collection of assets in some consecutive trading periods, based on investors' return-risk profile. Automating this process with machine learning remains a challenging problem. Here, we design a deep reinforcement learning (RL) architecture with an autonomous trading agent such that, investment decisions and actions are made periodically, based on a global objective, with autonomy. In particular, without relying on a purely model-free RL agent, we train our trading agent using a novel RL architecture consisting of an infused prediction module (IPM), a generative adversarial data augmentation module (DAM) and a behavior cloning module (BCM). Our model-based approach works with both on-policy or off-policy RL algorithms. We further design the back-testing and execution engine which interact with the RL agent in real time. Using historical {\em real} financial market data, we simulate trading with practical constraints, and demonstrate that our proposed model is robust, profitable and risk-sensitive, as compared to baseline trading strategies and model-free RL agents from prior work.","Pengqian Yu,Pengqian Yu,Joon Sern Lee,Ilya Kulyatin,Zekun Shi,Sakyasingha Dasgupta,Sakyasingha Dasgupta",arXiv: Learning,2019,,,
71,A Deep Reinforcement Learning Framework for Continuous Intraday Market Bidding,"The large integration of variable energy resources is expected to shift a large part of the energy exchanges closer to real-time, where more accurate forecasts are available. In this context, the short-term electricity markets and in particular the intraday market are considered a suitable trading floor for these exchanges to occur. A key component for the successful renewable energy sources integration is the usage of energy storage. In this paper, we propose a novel modelling framework for the strategic participation of energy storage in the European continuous intraday market where exchanges occur through a centralized order book. The goal of the storage device operator is the maximization of the profits received over the entire trading horizon, while taking into account the operational constraints of the unit. The sequential decision-making problem of trading in the intraday market is modelled as a Markov Decision Process. An asynchronous distributed version of the fitted Q iteration algorithm is chosen for solving this problem due to its sample efficiency. The large and variable number of the existing orders in the order book motivates the use of high-level actions and an alternative state representation. Historical data are used for the generation of a large number of artificial trajectories in order to address exploration issues during the learning process. The resulting policy is back-tested and compared against a benchmark strategy that is the current industrial standard. Results indicate that the agent converges to a policy that achieves in average higher total revenues than the benchmark strategy.","Ioannis Boukas,Damien Ernst,Thibaut Théate,Adrien Bolland,Bertrand Cornélusse,Alexandre Huynen,Martin Buchwald,Christelle Wynants",,2019,,,
72,Deep Hedging: Hedging Derivatives Under Generic Market Frictions Using Reinforcement Learning,"This article discusses a new application of reinforcement learning: to the problem of hedging a portfolio of “over-the-counter” derivatives under under market frictions such as trading costs and liquidity constraints. It is an extended version of our recent work https://www.ssrn.com/abstract=3120710, here using notation more common in the machine learning literature. The objective is to maximize a non-linear risk-adjusted return function by trading in liquid hedging instruments such as equities or listed options. The approach presented here is the first efficient and model-independent algorithm which can be used for such problems at scale.","Hans Buehler,Hans Buehler,Lukas Gonon,Josef Teichmann,Ben Wood,Baranidharan Mohan,Jonathan Kochems",,2019,,,
73,ABIDES: Towards High-Fidelity Market Simulation for AI Research.,"We introduce ABIDES, an Agent-Based Interactive Discrete Event Simulation environment. ABIDES is designed from the ground up to support AI agent research in market applications. While simulations are certainly available within trading firms for their own internal use, there are no broadly available high-fidelity market simulation environments. We hope that the availability of such a platform will facilitate AI research in this important area. ABIDES currently enables the simulation of tens of thousands of trading agents interacting with an exchange agent to facilitate transactions. It supports configurable pairwise network latencies between each individual agent as well as the exchange. Our simulator's message-based design is modeled after NASDAQ's published equity trading protocols ITCH and OUCH. We introduce the design of the simulator and illustrate its use and configuration with sample code, validating the environment with example trading scenarios. The utility of ABIDES is illustrated through experiments to develop a market impact model. We close with discussion of future experimental problems it can be used to explore, such as the development of ML-based trading algorithms.","David Byrd,Maria Hybinette,Tucker Hybinette Balch,Tucker Balch,Tucker Hybinette Balch",arXiv: Multiagent Systems,2019,,,
74,Learning to Trade with Deep Actor Critic Methods,"In this paper, we propose a new trading framework to apply deep actor critic methods to financial trading problems. Different from traditional actor critic methods, our model use not only actor but also critic to make the final decision. And for generalization purpose, a siamese structure in which the actor and the critic share the same LSTM features extraction part is adopted. The extracted features are then passed to different dense connected networks to compute q values and policy logits. The experiment results on different periods of CSI 300 prove that DACT has significantly better performances than B&H, DQT and DDRT. Furthermore, the idea of exploring based on the ensemble of actor and critic is valuable for other reinforcement learning problems.","Jinke Li,Ruonan Rao,Jun Shi",,2018,,,
75,FeUdal Networks for Hierarchical Reinforcement Learning,"We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.","Alexander Vezhnevets,Simon Osindero,Tom Schaul,Nicolas Heess,Max Jaderberg,David Silver,Koray Kavukcuoglu",arXiv: Artificial Intelligence,2017,,,
76,Optimistic Bull or Pessimistic Bear: Adaptive Deep Reinforcement Learning for Stock Portfolio Allocation,"Portfolio allocation is crucial for investment companies. However, getting the best strategy in a complex and dynamic stock market is challenging. In this paper, we propose a novel Adaptive Deep Deterministic Reinforcement Learning scheme (Adaptive DDPG) for the portfolio allocation task, which incorporates optimistic or pessimistic deep reinforcement learning that is reflected in the influence from prediction errors. Dow Jones 30 component stocks are selected as our trading stocks and their daily prices are used as the training and testing data. We train the Adaptive DDPG agent and obtain a trading strategy. The Adaptive DDPG's performance is compared with the vanilla DDPG, Dow Jones Industrial Average index and the traditional min-variance and mean-variance portfolio allocation strategies. Adaptive DDPG outperforms the baselines in terms of the investment return and the Sharpe ratio.","Xinyi Li,Yinchuan Li,Yinchuan Li,Yuancheng Zhan,Xiao-Yang Liu,Xiao-Yang Liu,Xiao-Yang Liu",arXiv: Statistical Finance,2019,,,
77,Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,"Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy - that is, succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as either off-policy Q-learning, or on-policy policy gradient methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.","Tuomas Haarnoja,Aurick Zhou,Pieter Abbeel,Sergey Levine",,2018,,,
78,RLlib: Abstractions for Distributed Reinforcement Learning,,"Eric Liang,Richard Liaw,Robert Nishihara,Philipp Moritz,Roy Fox,Ken Goldberg,Joseph E. Gonzalez,Michael I. Jordan,Ion Stoica",,2018,,,
79,An alternative softmax operator for reinforcement learning,"A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one's weight behind a single maximum utility decision. The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior. In this work, we study a differentiable softmax operator that, among other properties, is a non-expansion ensuring a convergent behavior in learning and planning. We introduce a variant of SARSA algorithm that, by utilizing the new operator, computes a Boltzmann policy with a state-dependent temperature parameter. We show that the algorithm is convergent and that it performs favorably in practice.","Kavosh Asadi,Michael L. Littman",,2017,,,
80,Hierarchical deep reinforcement learning: integrating temporal abstraction and intrinsic motivation,"Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. One of the key difficulties is insufficient exploration, resulting in an agent being unable to learn robust policies. Intrinsically motivated agents can explore new behavior for their own sake rather than to directly solve external goals. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical action-value functions, operating at different temporal scales, with goal-driven intrinsically motivated deep reinforcement learning. A top-level q-value function learns a policy over intrinsic goals, while a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse and delayed feedback: (1) a complex discrete stochastic decision process with stochastic transitions, and (2) the classic ATARI game -'Montezuma's Revenge'.","Tejas D. Kulkarni,Karthik Narasimhan,Ardavan Saeedi,Joshua B. Tenenbaum",,2016,,,
81,Attention is All you Need,"The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.","Ashish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszkoreit,Llion Jones,Aidan N. Gomez,Lukasz Kaiser,Illia Polosukhin",,2017,,,
82,Universal Features of Price Formation in Financial Markets: Perspectives From Deep Learning,"Using a large-scale Deep Learning approach applied to a high-frequency database containing billions of electronic market quotes and transactions for US equities, we uncover nonparametric evidence for the existence of a universal and stationary price formation mechanism relating the dynamics of supply and demand for a stock, as revealed through the order book, to subsequent variations in its market price. We assess the model by testing its out-of-sample predictions for the direction of price moves given the history of price and order flow, across a wide range of stocks and time periods. The universal price formation model is shown to exhibit a remarkably stable out-of-sample prediction accuracy across time, for a wide range of stocks from different sectors. Interestingly, these results also hold for stocks which are not part of the training sample, showing that the relations captured by the model are universal and not asset-specific. The universal model --- trained on data from all stocks --- outperforms, in terms of out-of-sample prediction accuracy, asset-specific linear and nonlinear models trained on time series of any given stock, showing that the universal nature of price formation weighs in favour of pooling together financial data from various stocks, rather than designing asset- or sector-specific models as commonly done. Standard data normalizations based on volatility, price level or average spread, or partitioning the training data into sectors or categories such as large/small tick stocks, do not improve training results. On the other hand, inclusion of price and order flow history over many past observations is shown to improve forecasting performance, showing evidence of path-dependence in price dynamics.","Justin A. Sirignano,Rama Cont",Quantitative Finance,2019,,,
83,Addressing Function Approximation Error in Actor-Critic Methods,"In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.","Scott Fujimoto,Herke van Hoof,David Meger",,2018,,,
84,Experience Replay Optimization,"Experience replay enables reinforcement learning agents to memorize and reuse past experiences, just as humans replay memories for the situation at hand. Contemporary off-policy algorithms either replay past experiences uniformly or utilize a rule-based replay strategy, which may be sub-optimal. In this work, we consider learning a replay policy to optimize the cumulative reward. Replay learning is challenging because the replay memory is noisy and large, and the cumulative reward is unstable. To address these issues, we propose a novel experience replay optimization (ERO) framework which alternately updates two policies: the agent policy, and the replay policy. The agent is updated to maximize the cumulative reward based on the replayed data, while the replay policy is updated to provide the agent with the most useful experiences. The conducted experiments on various continuous control tasks demonstrate the effectiveness of ERO, empirically showing promise in experience replay learning to improve the performance of off-policy reinforcement learning algorithms.","Daochen Zha,Kwei-Herng Lai,Kaixiong Zhou,Xia Hu",,2019,,,
85,Reinforcement Learning for Portfolio Management.,"In this thesis, we develop a comprehensive account of the expressive power, modelling efficiency, and performance advantages of so-called trading agents (i.e., Deep Soft Recurrent Q-Network (DSRQN) and Mixture of Score Machines (MSM)), based on both traditional system identification (model-based approach) as well as on context-independent agents (model-free approach). The analysis provides conclusive support for the ability of model-free reinforcement learning methods to act as universal trading agents, which are not only capable of reducing the computational and memory complexity (owing to their linear scaling with the size of the universe), but also serve as generalizing strategies across assets and markets, regardless of the trading universe on which they have been trained. The relatively low volume of daily returns in financial market data is addressed via data augmentation (a generative approach) and a choice of pre-training strategies, both of which are validated against current state-of-the-art models. For rigour, a risk-sensitive framework which includes transaction costs is considered, and its performance advantages are demonstrated in a variety of scenarios, from synthetic time-series (sinusoidal, sawtooth and chirp waves), simulated market series (surrogate data based), through to real market data (S\&P 500 and EURO STOXX 50). The analysis and simulations confirm the superiority of universal model-free reinforcement learning agents over current portfolio management model in asset allocation strategies, with the achieved performance advantage of as much as 9.2\% in annualized cumulative returns and 13.4\% in annualized Sharpe Ratio.","Angelos Filos,Angelos Filos",arXiv: Portfolio Management,2019,,,
86,Quantitative Trading on Stock Market Based on Deep Reinforcement Learning,"With the development of computer science technology and artificial intelligence, quantitative trading attracts more investors due to its efficiency and stable performance. In this paper, we explore the potential of deep reinforcement learning in quantitative trading. A LSTM-based agent is proposed to learn the temporal pattern in data and automatically trades according to the current market condition and the historical data. The input to the agent is the raw financial data and the output of the agent is decision of trading. The goal of the agent is to maximize the ultimate profit. Besides, to reduce the influence of noise in the market and to improve the performance of the agent, we use several technical indicators as an extra input. The proposed system has been back-tested on the stock market. The results demonstrate that our method performs well in most conditions.","Jia Wu,Chen Wang,Lidong Xiong,Hongyong Sun",,2019,,,
87,Modern Perspectives on Reinforcement Learning in Finance,"We give an overview and outlook of the field of reinforcement learning as it applies to solving financial applications of intertemporal choice. In finance, common problems of this kind include pricing and hedging of contingent claims, investment and portfolio allocation, buying and selling a portfolio of securities subject to transaction costs, market making, asset liability management and optimization of tax consequences, to name a few. Reinforcement learning allows us to solve these dynamic optimization problems in an almost model-free way, relaxing the assumptions often needed for classical approaches.

A main contribution of this article is the elucidation of the link between these dynamic optimization problem and reinforcement learning, concretely addressing how to formulate expected intertemporal utility maximization problems using modern machine learning techniques.","Petter N. Kolm,Petter N. Kolm,Gordon Ritter,Gordon Ritter",,2019,,,
88,Adaptive Quantitative Trading: An Imitative Deep Reinforcement Learning Approach,"In recent years, considerable efforts have been devoted to developing AI techniques for finance research and applications. For instance, AI techniques (e.g., machine learning) can help traders in quantitative trading (QT) by automating two tasks: market condition recognition and trading strategies execution. However, existing methods in QT face challenges such as representing noisy high-frequent financial data and finding the balance between exploration and exploitation of the trading agent with AI techniques. To address the challenges, we propose an adaptive trading model, namely iRDPG, to automatically develop QT strategies by an intelligent trading agent. Our model is enhanced by deep reinforcement learning (DRL) and imitation learning techniques. Specifically, considering the noisy financial data, we formulate the QT process as a Partially Observable Markov Decision Process (POMDP). Also, we introduce imitation learning to leverage classical trading strategies useful to balance between exploration and exploitation. For better simulation, we train our trading agent in the real financial market using minute-frequent data. Experimental results demonstrate that our model can extract robust market features and be adaptive in different markets.","Yang Liu,Yang Liu,Yang Liu,Yang Liu,Yang Liu,Qi Liu,Qi Liu,Qi Liu,Qi Liu,Hongke Zhao,Zhen Pan,Zhen Pan,Chuanren Liu,Chuanren Liu,Chuanren Liu",,2020,,,
89,Reinforcement-learning based portfolio management with augmented asset movement prediction states,"Portfolio management (PM) is a fundamental financial planning task that aims to achieve investment goals such as maximal profits or minimal risks. Its decision process involves continuous derivation of valuable information from various data sources and sequential decision optimization, which is a prospective research direction for reinforcement learning (RL). In this paper, we propose SARL, a novel State-Augmented RL framework for PM. Our framework aims to address two unique challenges in financial PM: (1) data heterogeneity -- the collected information for each asset is usually diverse, noisy and imbalanced (e.g., news articles); and (2) environment uncertainty -- the financial market is versatile and non-stationary. To incorporate heterogeneous data and enhance robustness against environment uncertainty, our SARL augments the asset information with their price movement prediction as additional states, where the prediction can be solely based on financial data (e.g., asset prices) or derived from alternative sources such as news. Experiments on two real-world datasets, (i) Bitcoin market and (ii) HighTech stock market with 7-year Reuters news articles, validate the effectiveness of SARL over existing PM approaches, both in terms of accumulated profits and risk-adjusted profits. Moreover, extensive simulations are conducted to demonstrate the importance of our proposed state augmentation, providing new insights and boosting performance significantly over standard RL-based PM method and other baselines.","Yunan Ye,Yunan Ye,Yunan Ye,Yunan Ye,Hengzhi Pei,Boxin Wang,Boxin Wang,Pin-Yu Chen,Pin-Yu Chen,Yada Zhu,Yada Zhu,Jun Xiao,Jun Xiao,Jun Xiao,Jun Xiao,Bo Li,Bo Li,Bo Li,Bo Li,Bo Li",,2020,,,
90,Sentiment and Knowledge Based Algorithmic Trading with Deep Reinforcement Learning.,"Algorithmic trading, due to its inherent nature, is a difficult problem to tackle; there are too many variables involved in the real world which make it almost impossible to have reliable algorithms for automated stock trading. The lack of reliable labelled data that considers physical and physiological factors that dictate the ups and downs of the market, has hindered the supervised learning attempts for dependable predictions. To learn a good policy for trading, we formulate an approach using reinforcement learning which uses traditional time series stock price data and combines it with news headline sentiments, while leveraging knowledge graphs for exploiting news about implicit relationships.","Abhishek Nan,Anandh Perumal,Osmar R. Zaïane Zaïane,Osmar R. Zaïane",arXiv: Artificial Intelligence,2020,,,
91,An Application of Deep Reinforcement Learning to Algorithmic Trading,"This scientific research paper presents an innovative approach based on deep reinforcement learning (DRL) to solve the algorithmic trading problem of determining the optimal trading position at any point in time during a trading activity in stock markets. It proposes a novel DRL trading strategy so as to maximise the resulting Sharpe ratio performance indicator on a broad range of stock markets. Denominated the Trading Deep Q-Network algorithm (TDQN), this new trading strategy is inspired from the popular DQN algorithm and significantly adapted to the specific algorithmic trading problem at hand. The training of the resulting reinforcement learning (RL) agent is entirely based on the generation of artificial trajectories from a limited set of stock market historical data. In order to objectively assess the performance of trading strategies, the research paper also proposes a novel, more rigorous performance assessment methodology. Following this new performance assessment approach, promising results are reported for the TDQN strategy.","Thibaut Théate,Damien Ernst",arXiv: Trading and Market Microstructure,2020,,,
92,D4RL: Datasets for Deep Data-Driven Reinforcement Learning,"The offline reinforcement learning (RL) problem, also known as batch RL, refers to the setting where a policy must be learned from a static dataset, without additional online data collection. This setting is compelling as potentially it allows RL methods to take advantage of large, pre-collected datasets, much like how the rise of large datasets has fueled results in supervised learning in recent years. However, existing online RL benchmarks are not tailored towards the offline setting, making progress in offline RL difficult to measure. In this work, we introduce benchmarks specifically designed for the offline setting, guided by key properties of datasets relevant to real-world applications of offline RL. Examples of such properties include: datasets generated via hand-designed controllers and human demonstrators, multi-objective datasets where an agent can perform different tasks in the same environment, and datasets consisting of a mixtures of policies. To facilitate research, we release our benchmark tasks and datasets with a comprehensive evaluation of existing algorithms and an evaluation protocol together with an open-source codebase. We hope that our benchmark will focus research effort on methods that drive improvements not just on simulated tasks, but ultimately on the kinds of real-world problems where offline RL will have the largest impact.","Justin Fu,Aviral Kumar,Ofir Nachum,George Tucker,Sergey Levine",arXiv: Learning,2020,,,
93,Application of Deep Q-Network in Portfolio Management,"Machine Learning algorithms and Neural Networks are widely applied to many different areas such as stock market prediction, facial recognition and automatic machine translation. This paper introduces a novel strategy based on the classic Deep Reinforcement Learning algorithm, Deep QNetwork, for stock market portfolio management. It is a type of deep neural network which is optimized by Q Learning. To adapt the Deep Q-Network for stock market production, we first discretize the action space so that portfolio management becomes a problem that Deep Q-Network can solve. Following this, we combine the Convolutional Neural Network and dueling Q-Net to enhance the recognition ability of the algorithm. We choose five low-relevant American stocks to test our model. It is found that the Deep Q-Network based strategy outperforms the ten other traditional strategies. The profit of Deep Q-Network algorithm is 30% more than the profit of other strategies. Moreover, the Sharpe ratio and Max Drawdown demonstrates that the risk of policy associated with Deep Q-Network is the lowest.","Ziming Gao,Yuan Gao,Yi Hu,Zhengyong Jiang,Jionglong Su",,2020,,,
94,Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning,"Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 6 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.","Tianhe Yu,Deirdre Quillen,Zhanpeng He,Ryan Julian,Karol Hausman,Chelsea Finn,Sergey Levine",,2019,,,
95,OpenAI Gym,"OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.","Greg Brockman,Vicki Cheung,Ludwig Pettersson,Jonas Schneider,John Schulman,Jie Tang,Wojciech Zaremba",,2016,,,
96,A multi-layer and multi-ensemble stock trader using deep learning and deep reinforcement learning,"The adoption of computer-aided stock trading methods is gaining popularity in recent years, mainly because of their ability to process efficiently past information through machine learning to predict future market behavior. Several approaches have been proposed to this task, with the most effective ones using fusion of a pile of classifiers decisions to predict future stock values. However, using prices information in single supervised classifiers has proven to lead to poor results, mainly because market history is not enough to be an indicative of future market behavior. In this paper, we propose to tackle this issue by proposing a multi-layer and multi-ensemble stock trader. Our method starts by pre-processing data with hundreds of deep neural networks. Then, a reward-based classifier acts as a meta-learner to maximize profit and generate stock signals through different iterations. Finally, several metalearner trading decisions are fused in order to get a more robust trading strategy, using several trading agents to take a final decision. We validate the effectiveness of the approach in a real-world trading scenario, by extensively testing it on the Standard & Poor’s 500 future market and the J.P. Morgan and Microsoft stocks. Experimental results show that the proposed method clearly outperforms all the considered baselines (which still performs very well in the analysed period), and even the conventional Buy-and-Hold strategy, which replicates the market behaviour.","Salvatore Carta,Andrea Corriga,Anselmo Ferreira,Alessandro Sebastian Podda,Diego Reforgiato Recupero",Applied Intelligence,2020,,,
97,Maximum Mutation Reinforcement Learning for Scalable Control,"Advances in Reinforcement Learning (RL) have successfully tackled sample efficiency and overestimation bias. However, these methods often fall short of scalable performance. On the other hand, genetic methods provide scalability but depict hyperparameter sensitivity to evolutionary operations. We present the Evolution-based Soft Actor-Critic (ESAC), a scalable RL algorithm. Our contributions are threefold; ESAC (1) abstracts exploration from exploitation by combining Evolution Strategies (ES) with Soft Actor-Critic (SAC), (2) provides dominant skill transfer between offsprings by making use of soft winner selections and genetic crossovers in hindsight and (3) improves hyperparameter sensitivity in evolutions using Automatic Mutation Tuning (AMT). AMT gradually replaces the entropy framework of SAC allowing the population to succeed at the task while acting as randomly as possible, without making use of backpropagation updates. On a range of challenging control tasks consisting of high-dimensional action spaces and sparse rewards, ESAC demonstrates state-of-the-art performance and sample efficiency equivalent to SAC. ESAC demonstrates scalability comparable to ES on the basis of hardware resources and algorithm overhead. A complete implementation of ESAC with notes on reproducibility and videos can be found at the project website https://karush17.github.io/esac-web/.","Karush Suri,Xiao Qi Shi,Konstantinos N. Plataniotis,Yuri A. Lawryshyn,Yuri Lawryshyn",arXiv: Learning,2020,,,
98,TensorLayer: A Versatile Library for Efficient Deep Learning Development,"Recently we have observed emerging uses of deep learning techniques in multimedia systems. Developing a practical deep learning system is arduous and complex. It involves labor-intensive tasks for constructing sophisticated neural networks, coordinating multiple network models, and managing a large amount of training-related data. To facilitate such a development process, we propose TensorLayer which is a Python-based versatile deep learning library. TensorLayer provides high-level modules that abstract sophisticated operations towards neuron layers, network models, training data and dependent training jobs. In spite of offering simplicity, it has transparent module interfaces that allows developers to flexibly embed low-level controls within a backend engine, with the aim of supporting fine-grain tuning towards training. Real-world cluster experiment results show that TensorLayeris able to achieve competitive performance and scalability in critical deep learning tasks. TensorLayer was released in September 2016 on GitHub. Since after, it soon become one of the most popular open-sourced deep learning library used by researchers and practitioners.","Hao Dong,Akara Supratak,Luo Mai,Fangde Liu,Axel Oehmichen,Simiao Yu,Yike Guo",,2017,,,
99,Deep Reinforcement Learning: A Brief Survey,"Deep reinforcement learning (DRL) is poised to revolutionize the field of artificial intelligence (AI) and represents a step toward building autonomous systems with a higherlevel understanding of the visual world. Currently, deep learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to play video games directly from pixels. DRL algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of RL, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep RL, including the deep Q-network (DQN), trust region policy optimization (TRPO), and asynchronous advantage actor critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via RL. To conclude, we describe several current areas of research within the field.","Kai Arulkumaran,Marc Peter Deisenroth,Miles Brundage,Miles Brundage,Anil A. Bharath",IEEE Signal Processing Magazine,2017,,,
100,Autonomous navigation of stratospheric balloons using reinforcement learning.,"Efficiently navigating a superpressure balloon in the stratosphere1 requires the integration of a multitude of cues, such as wind speed and solar elevation, and the process is complicated by forecast errors and sparse wind measurements. Coupled with the need to make decisions in real time, these factors rule out the use of conventional control techniques2,3. Here we describe the use of reinforcement learning4,5 to create a high-performing flight controller. Our algorithm uses data augmentation6,7 and a self-correcting design to overcome the key technical challenge of reinforcement learning from imperfect data, which has proved to be a major obstacle to its application to physical systems8. We deployed our controller to station Loon superpressure balloons at multiple locations across the globe, including a 39-day controlled experiment over the Pacific Ocean. Analyses show that the controller outperforms Loon's previous algorithm and is robust to the natural diversity in stratospheric winds. These results demonstrate that reinforcement learning is an effective solution to real-world autonomous control problems in which neither conventional methods nor human intervention suffice, offering clues about what may be needed to create artificially intelligent agents that continuously interact with real, dynamic environments.","Marc G. Bellemare,Candido Salvatore J,Salvatore Candido,Pablo Samuel Castro,Gong Jun,Marlos C Machado,Marlos C. Machado,Subhodeep Moitra,Ponda Sameera Sylvia,Sameera S. Ponda,Ziyu Wang",Nature,2020,,,
101,Get real: realism metrics for robust limit order book market simulations,"Machine learning (especially reinforcement learning) methods for trading are increasingly reliant on simulation for agent training and testing. Furthermore, simulation is important for validation of hand-coded trading strategies and for testing hypotheses about market structure. A challenge, however, concerns the robustness of policies validated in simulation because the simulations lack fidelity. In fact, researchers have shown that many market simulation approaches fail to reproduce statistics and stylized facts seen in real markets. As a step towards addressing this we surveyed the literature to collect a set of reference metrics and applied them to real market data and simulation output. Our paper provides a comprehensive catalog of these metrics including mathematical formulations where appropriate. Our results show that there are still significant discrepancies between simulated markets and real ones. However, this work serves as a benchmark against which we can measure future improvement.","Svitlana Vyetrenko,David Byrd,Nick Petosa,Mahmoud Mahfouz,Danial Dervovic,Danial Dervovic,Danial Dervovic,Manuela Veloso,Tucker Hybinette Balch,Tucker Balch,Tucker Hybinette Balch",,2019,,,
102,101 Formulaic Alphas,"We present explicit formulas - that are also computer code - for 101 real-life quantitative trading alphas. Their average holding period approximately ranges 0.6-6.4 days. The average pair-wise correlation of these alphas is low, 15.9%. The returns are strongly correlated with volatility, but have no significant dependence on turnover, directly confirming an earlier result based on a more indirect empirical analysis. We further find empirically that turnover has poor explanatory power for alpha correlations.",Zura Kakushadze,,2016,,,
103,Time series momentum.,"We document significant ‘‘time series momentum’’ in equity index, currency, commodity, and bond futures for each of the 58 liquid instruments we consider. We find persistence in returns for one to 12 months that partially reverses over longer horizons, consistent with sentiment theories of initial under-reaction and delayed over-reaction. A diversified portfolio of time series momentum strategies across all asset classes delivers substantial abnormal returns with little exposure to standard asset pricing factors and performs best during extreme markets. Examining the trading activities of speculators and hedgers, we find that speculators profit from time series momentum at the expense of hedgers.","Tobias J. Moskowitz,Yao Hua Ooi,Lasse Heje Pedersen",Journal of Financial Economics,2012,,,
104,Robust Market Making via Adversarial Reinforcement Learning,"We show that adversarial reinforcement learning (ARL) can be used to produce market marking agents that are robust to adversarial and adaptively-chosen market conditions. To apply ARL, we turn the well-studied single-agent model of Avellaneda and Stoikov [2008] into a discrete-time zero-sum game between a market maker and adversary. The adversary acts as a proxy for other market participants that would like to profit at the market maker's expense. We empirically compare two conventional single-agent RL agents with ARL, and show that our ARL approach leads to: 1) the emergence of risk-averse behaviour without constraints or domain-specific penalties; 2) significant improvements in performance across a set of standard metrics, evaluated with or without an adversary in the test environment, and; 3) improved robustness to model uncertainty. We empirically demonstrate that our ARL method consistently converges, and we prove for several special cases that the profiles that we converge to correspond to Nash equilibria in a simplified single-stage game.","Thomas Spooner,Rahul Savani",,2020,,,
105,A Deep Reinforcement Learning Framework for Optimal Trade Execution,"In this article, we propose a deep reinforcement learning based framework to learn to minimize trade execution costs by splitting a sell order into child orders and execute them sequentially over a fixed period. The framework is based on a variant of the Deep Q-Network (DQN) algorithm that integrates the Double DQN, Dueling Network, and Noisy Nets. In contrast to previous research work, which uses implementation shortfall as the immediate rewards, we use a shaped reward structure, and we also incorporate the zero-ending inventory constraint into the DQN algorithm by slightly modifying the Q-function updates relative to standard Q-learning at the final step.","Siyu Lin,Peter A. Beling,Peter A. Beling",,2021,,,
106,On-line Q-learning Using Connectionist Systems,,G. A. Rummery,CTIT technical reports series,1994,,,
107,REST: Relational Event-driven Stock Trend Forecasting,"Stock trend forecasting, aiming at predicting the stock future trends, is crucial for investors to seek maximized profits from the stock market. Many event-driven methods utilized the events extracted from news, social media, and discussion board to forecast the stock trend in recent years. However, existing event-driven methods have two main shortcomings: 1) overlooking the influence of event information differentiated by the stock-dependent properties; 2) neglecting the effect of event information from other related stocks. In this paper, we propose a relational event-driven stock trend forecasting (REST) framework, which can address the shortcoming of existing methods. To remedy the first shortcoming, we propose to model the stock context and learn the effect of event information on the stocks under different contexts. To address the second shortcoming, we construct a stock graph and design a new propagation layer to propagate the effect of event information from related stocks. The experimental studies on the real-world data demonstrate the efficiency of our REST framework. The results of investment simulation show that our framework can achieve a higher return of investment than baselines.","Wentao Xu,Weiqing Liu,Chang Xu,Jiang Bian,Jian Yin,Jian Yin,Tie-Yan Liu",,2021,,,
108,Multi-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis,"Liquidation is the process of selling a large number of shares of one stock sequentially within a given time frame, taking into consideration the costs arising from market impact and a trader's risk aversion. The main challenge in optimizing liquidation is to find an appropriate modeling system that can incorporate the complexities of the stock market and generate practical trading strategies. In this paper, we propose to use multi-agent deep reinforcement learning model, which better captures high-level complexities comparing to various machine learning methods, such that agents can learn how to make the best selling decisions. First, we theoretically analyze the Almgren and Chriss model and extend its fundamental mechanism so it can be used as the multi-agent trading environment. Our work builds the foundation for future multi-agent environment trading analysis. Secondly, we analyze the cooperative and competitive behaviours between agents by adjusting the reward functions for each agent, which overcomes the limitation of single-agent reinforcement learning algorithms. Finally, we simulate trading and develop an optimal trading strategy with practical constraints by using a reinforcement learning method, which shows the capabilities of reinforcement learning methods in solving realistic liquidation problems.","Wenhang Bao,Xiao-yang Liu,Xiao-Yang Liu",Research Papers in Economics,2019,,,
109,Regularization of Neural Networks using DropConnect,"We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.","Li Wan,Matthew D. Zeiler,Sixin Zhang,Sixin Zhang,Sixin Zhang,Yann L. Cun,Rob Fergus",,2013,,,
110,Efficient sequential access pattern mining for web recommendations,"Sequential access pattern mining discovers interesting and frequent user access patterns from web logs. Most of the previous studies have adopted Apriori-like sequential pattern mining techniques, which faced the problem on requiring expensive multiple scans of databases. More recent algorithms that are based on the Web Access Pattern tree (or WAP-tree) can achieve an order of magnitude faster than traditional Apriori-like sequential pattern mining techniques. However, the use of conditional search strategies in WAP-tree based mining algorithms requires re-construction of large numbers of intermediate conditional WAP-trees during mining process, which is also very costly. In this paper, we propose an efficient sequential access pattern mining algorithm, known as CSB-mine (Conditional Sequence Base mining algorithm). The proposed CSB-mine algorithm is based directly on the conditional sequence bases of each frequent event which eliminates the need for constructing WAP-trees. This can improve the efficiency of the mining process significantly compared with WAP-tree based mining algorithms, especially when the support threshold becomes smaller and the size of database gets larger. In this paper, the proposed CSB-mine algorithm and its performance will be discussed. In addition, we will also discuss a sequential access-based web recommender system that has incorporated the CSB-mine algorithm for web recommendations.","Baoyao Zhou,Siu Cheung Hui,Alvis C. M. Fong",International Journal of Knowledge-based and Intelligent Engineering Systems,2006,,,
111,Original Contribution: Stacked generalization,"This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.",David H. Wolpert,Neural Networks,1992,,,
112,Technical Note : \cal Q -Learning,"\cal Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.

This paper presents and proves in detail a convergence theorem for \cal Q-learning based on that outlined in Watkins (1989). We show that \cal Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many \cal Q values can be changed each iteration, rather than just one.","Chris Watkins,Peter Dayan",Machine Learning,1992,,,
113,Sequential model-based optimization for general algorithm configuration,"State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.","Frank Hutter,Holger H. Hoos,Kevin Leyton-Brown",,2011,,,
114,Modeling purposeful adaptive behavior with the principle of maximum causal entropy,"Predicting human behavior from a small amount of training examples is a challenging machine learning problem. In this thesis, we introduce the principle of maximum causal entropy, a general technique for applying information theory to decision-theoretic, game-theoretic, and control settings where relevant information is sequentially revealed over time. This approach guarantees decision-theoretic performance by matching purposeful measures of behavior (Abbeel & Ng, 2004), and/or enforces game-theoretic rationality constraints (Aumann, 1974), while otherwise being as uncertain as possible, which minimizes worst-case predictive log-loss (Grunwald & Dawid, 2003). 
We derive probabilistic models for decision, control, and multi-player game settings using this approach. We then develop corresponding algorithms for efficient inference that include relaxations of the Bellman equation (Bellman, 1957), and simple learning algorithms based on convex optimization. We apply the models and algorithms to a number of behavior prediction tasks. Specifically, we present empirical evaluations of the approach in the domains of vehicle route preference modeling using over 100,000 miles of collected taxi driving data, pedestrian motion modeling from weeks of indoor movement data, and robust prediction of game play in stochastic multi-player games.","J. Andrew Bagnell,Brian D. Ziebart",,2010,,,
115,Frequent Pattern Mining,"This comprehensive reference consists of 18 chapters from prominent researchers in the field. Each chapter is self-contained, and synthesizes one aspect of frequent pattern mining. An emphasis is placed on simplifying the content, so that students and practitioners can benefit from the book. Each chapter contains a survey describing key research on the topic, a case study and future directions. Key topics include: Pattern Growth Methods, Frequent Pattern Mining in Data Streams, Mining Graph Patterns, Big Data Frequent Pattern Mining, Algorithms for Data Clustering and more. Advanced-level students in computer science, researchers and practitioners from industry will find this book an invaluable reference.","Charu C. Aggarwal,Shi Han,Jiawei Han,Yingnong Dang,Dongmei Zhang,Song Ge",,2014,,,
116,Kernel Methods in Finance,"Kernel methods (Cristianini and Shawe-Taylor 2000; Herbrich 2002; Scholkopf and Smola 2002; Shawe-Taylor and Cristianini 2004) can be regarded as machine learning techniques which are “kernelised” versions of other fundamental machine learning methods. The latter include traditional methods for linear dimensionality reduction such as principal component analysis (PCA) (Jolliffe 1986), methods for linear regression and methods for linear classification such as linear support vector machines (Cristianini and Shawe-Taylor 2000; Boser et al. 1992; Vapnik 2006b). For all these methods corresponding “kernel versions” have been developed which can turn them into non-linear methods. Kernel methods are very powerful, precise tools that open the door to a large variety of complex non-linear tasks which previously were beyond the horizon of feasibility, or could not appropriately be analysed with traditional machine learning techniques. However, with kernelisation come a number of new tasks and challenges that need to be addressed and considered. For example, for each application of a kernel method a suitable kernel and associated kernel parameters have to be selected. Also, high-dimensional nonlinear data can be extremely complex and can feature counter-intuitive pitfalls (Verleysen and Francois 2005).","Stephan K. Chalup,Andreas Mitschele",,2008,,,
117,Multi-label ensemble learning,"Multi-label learning aims at predicting potentially multiple labels for a given instance. Conventional multi-label learning approaches focus on exploiting the label correlations to improve the accuracy of the learner by building an individual multi-label learner or a combined learner based upon a group of single-label learners. However, the generalization ability of such individual learner can be weak. It is well known that ensemble learning can effectively improve the generalization ability of learning systems by constructing multiple base learners and the performance of an ensemble is related to the both accuracy and diversity of base learners. In this paper, we study the problem of multilabel ensemble learning. Specifically, we aim at improving the generalization ability of multi-label learning systems by constructing a group of multilabel base learners which are both accurate and diverse. We propose a novel solution, called EnML, to effectively augment the accuracy as well as the diversity of multi-label base learners. In detail, we design two objective functions to evaluate the accuracy and diversity of multilabel base learners, respectively, and EnML simultaneously optimizes these two objectives with an evolutionary multi-objective optimization method. Experiments on real-world multi-label learning tasks validate the effectiveness of our approach against other well-established methods.","Chuan Shi,Xiangnan Kong,Philip S. Yu,Bai Wang",,2011,,,
118,Time Series Data,,"Paul S. P. Cowpertwait,Andrew V. Metcalfe,Andrew V Metcalfe,Andrew V Metcalfe,Andrew Metcalfe",,2009,,,
119,Using sequential pattern mining for links recommendation in adaptive hypermedia educational systems,"Universidad de Cordoba, Campus de Rabanales, Edificio C-2, 14071 Cordoba, Spain In this paper we are going to describe a data mining tool, whose aim is to help authors or teachers to dis-covery interesting information from students’ usage information. The teacher can use this information to improve and personalize adaptive hypermedia courses to theirs students. We propose to use sequential pattern mining algorithms to discover the most used path by the students and from this information can recommend links to the new students automatically meanwhile they browse in the course. We have developed a specific author tool in order to help the teacher to apply all the data mining process.","C. Romero Morales,Antonio R. Porras,A. R. Porras Pérez,S. Ventura Soto,C. Hervás Martínez",,2006,,,
120,Cross-Domain Collaborative Filtering with Factorization Machines,"Factorization machines offer an advantage over other existing collaborative filtering approaches to recommendation. They make it possible to work with any auxiliary information that can be encoded as a real-valued feature vector as a supplement to the information in the user-item matrix. We build on the assumption that different patterns characterize the way that users interact with i.e., rate or download items of a certain type e.g., movies or books. We view interactions with a specific type of item as constituting a particular domain and allow interaction information from an auxiliary domain to inform recommendation in a target domain. Our proposed approach is tested on a data set from Amazon and compared with a state-of-the-art approach that has been proposed for Cross-Domain Collaborative Filtering. Experimental results demonstrate that our approach, which has a lower computational complexity, is able to achieve performance improvements.","Babak Loni,Yue Shi,Martha Larson,Alan Hanjalic",,2014,,,
121,Reinforcement Learning and Markov Decision Processes,"Situated in between supervised learning and unsupervised learning, the paradigm of reinforcement learning deals with learning in sequential decision making problems in which there is limited feedback. This text introduces the intuitions and concepts behind Markov decision processes and two classes of algorithms for computing optimal behaviors: reinforcement learning and dynamic programming. First the formal framework of Markov decision process is defined, accompanied by the definition of value functions and policies. The main part of this text deals with introducing foundational classes of algorithms for learning optimal behaviors, based on various definitions of optimality with respect to the goal of learning sequential decisions. Additionally, it surveys efficient extensions of the foundational algorithms, differing mainly in the way feedback given by the environment is used to speed up learning, and in the way they concentrate on relevant parts of the problem. For both model-based and model-free settings these efficient extensions have shown useful in scaling up to larger problems.","Martijn van Otterlo,Marco Wiering",,2012,,,
122,Personalized Recommendation Based on Behavior Sequence Similarity Measures,"Personalized recommendation is attracting more and more attentions nowadays. There are many kinds of algorithms for making predictions for the target users, and among them Collaborative Filtering CF is widely adopted. In some domains, a user's behavior sequences reflect his/her preferences over items so that users who have similar behavior sequences may indicate they have similar preference models. Based on this fact, we discuss how to improve the collaborative filtering algorithm by using user behavior sequence similarity. We proposed a new Behavior Sequence Similarity Measurement BSSM approach. Then, different ways to combine BSSM with CF algorithm are presented. Experiments on two real test data sets prove that more precise and stable recommendation performances can be achieved.","Yuqi Zhang,Jian Cao",,2013,,,
123,LOBSTER: Limit Order Book Reconstruction System,"The rise of order-driven markets in recent years created considerable challenges for researchers, who have to cope with extremely large amounts of data produced daily by the markets. It is our goal to spare academic researchers the tedious task of technical pre-processing of the data and thus enable them to focus on economic research. Our order book reconstruction system LOBSTER is based on the generalized order-processing algorithm common for majority of order-driven markets and so it is easily adjustable to process data produced in any order-driven market. Efficient data structures result in very convincing performance, with large datasets produced on the fly. Currently LOBSTER uses ITCH data from NASDAQ to accurately replicate the limit order book for any NASDAQ-traded stock to any desired level. Data from more venues will be added in the future. The system is accessible via the Internet, which makes it very convenient for researchers from around the world.","Ruihong Huang,Ruihong Huang,Tomas Polak",,2011,,,
124,Untersuchungen zu dynamischen neuronalen Netzen,,Sepp Hochreiter,,1991,,,
125,Optimization of Automated Trading System's Interaction with Market Environment,"This work is focused on the automated trading systems (ATS) design and optimization. In a preparation phase before use, an optimization of interaction of such systems with its intended market environment should be done. The technical analysis indicators are most frequently used in ATS. Optimization is done by testing of different settings of MACD indicator and optimal settings depend heavily on market parameters. The intended use of the ATS is to perform its activity independently to some extend (depending on user’s preferences). The main aim is to enhance automated trading system performance, in order to improve its usefulness and acceptability for the user. Paper will be focused on futures markets only (Chicago and New York), but results are applicable to other areas of trading as well.",Petr Tucnik,,2010,,,
126,Hybrid Recommender Systems: Survey and Experiments,"Recommender systems represent user preferences for the purpose of suggesting items to purchase or examine. They have become fundamental applications in electronic commerce and information access, providing suggestions that effectively prune large information spaces so that users are directed toward those items that best meet their needs and preferences. A variety of techniques have been proposed for performing recommendation, including content-based, collaborative, knowledge-based and other techniques. To improve performance, these methods have sometimes been combined in hybrid recommenders. This paper surveys the landscape of actual and possible hybrid recommenders, and introduces a novel hybrid, EntreeC, a system that combines knowledge-based recommendation and collaborative filtering to recommend restaurants. Further, we show that semantic ratings obtained from the knowledge-based part of the system enhance the effectiveness of collaborative filtering.",Robin Burke,User Modeling and User-adapted Interaction,2002,,,
127,Q-Learning-Based Financial Trading Systems with Applications,The design of financial trading systems (FTSs) is a subject of high interest both for the academic environment and for the professional one due to the promises by machine learning methodologies. In this paper we consider the Reinforcement Learning-based policy evaluation approach known as Q-Learning algorithm (QLa). QLa is an algorithm which real-time optimizes its behavior in relation to the responses it gets from the environment in which it operates. In particular: first we introduce the essential aspects of QLa which are of interest for our purposes, second we present some original FTSs based on differently configured QLas, then we apply such FTSs to an artificial time series of daily stock prices and to six real ones from the Italian stock market belonging to the FTSE MIB basket. The results we achieve are generally satisfactory.,"Marco Corazza,Francesco Bertoluzzo",,,
128,Convolutional Channel Features,"Deep learning methods are powerful tools but often suffer from expensive computation and limited flexibility. An alternative is to combine light-weight models with deep representations. As successful cases exist in several visual problems, a unified framework is absent. In this paper, we revisit two widely used approaches in computer vision, namely filtered channel features and Convolutional Neural Networks (CNN), and absorb merits from both by proposing an integrated method called Convolutional Channel Features (CCF). CCF transfers low-level features from pre-trained CNN models to feed the boosting forest model. With the combination of CNN features and boosting forest, CCF benefits from the richer capacity in feature representation compared with channel features, as well as lower cost in computation and storage compared with end-to-end CNN methods. We show that CCF serves as a good way of tailoring pre-trained CNN models to diverse tasks without fine-tuning the whole network to each task by achieving state-of-the-art performances in pedestrian detection, face detection, edge detection and object proposal generation.","Bin Yang,Junjie Yan,Zhen Lei,Stan Z. Li",,2015,,,
129,Learning with symmetric label noise: the importance of being unhinged,"Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the unhinged loss' SLN-robustness is borne out in practice. So, with apologies to Wilde [1895], while the truth is rarely pure, it can be simple.","Brendan van Rooyen,Aditya Krishna Menon,Robert C. Williamson",,2015,,,
130,Meta-heuristic based decision support for portfolio optimization with a case study on tracking error minimization in passive portfolio management,"In this paper we describe the concept and design of a meta-heuristic based decision support system generator (DSS-generator) for portfolio optimization. We report extensively on experience with the application of a specific DSS that has been customized for controlling and optimizing passively managed stock funds. Here, the constraints from the law on investment trust companies as well as several fund specific guidelines prohibit that the benchmark can be identically reproduced. For measuring the performance of the portfolio a tracking error model with data stemming from a factor model is applied. Our results show that the system provides proposals for the fund manager in acceptable time which are feasible with respect to the guidelines and excellent in quality.","Ulrich Derigs,Nils-H. Nickel",OR Spectrum,2003,,,
131,Application of Credit Card Fraud Detection: Based on Bagging Ensemble Classifier,"Abstract   Credit card fraud is increasing considerably with the development of modern technology and the global superhighways of communication. Credit card fraud costs consumers and the financial company billions of dollars annually, and fraudsters continuously try to find new rules and tactics to commit illegal actions. Thus, fraud detection systems have become essential for banks and financial institution, to minimize their losses. However, there is a lack of published literature on credit card fraud detection techniques, due to the unavailable credit card transactions dataset for researchers. The most commonly techniques used fraud detection methods are Naive Bayes (NB), Support Vector Machines (SVM), K-Nearest Neighbor algorithms (KNN). These techniques can be used alone or in collaboration using ensemble or meta-learning techniques to build classifiers. But amongst all existing method, ensemble learning methods are identified as popular and common method, not because of its quite straightforward implementation, but also due to its exceptional predictive performance on practical problems. In this paper we trained various data mining techniques used in credit card fraud detection and evaluate each methodology based on certain design criteria. After several trial and comparisons; we introduced the bagging classifier based on decision three, as the best classifier to construct the fraud detection model. The performance evaluation is performed on real life credit card transactions dataset to demonstrate the benefit of the bagging ensemble algorithm.","Masoumeh Zareapoor,Masoumeh Zareapoor,Pourya Shamsolmoali,Pourya Shamsolmoali",Procedia Computer Science,2015,,,
132,Active portfolio management : a quantitative approach for providing superior returns and controlling risk,"Introduction. Part I: Foundations. Consensus Expected Returns: The Capital Asset Pricing Model. Risk. Exceptional Return, Benchmarks, and Value Added. Residual Risk and Return: The Information Ratio. The Fundamental Law of Active Management. Part II: Expected Returns and Valuation. Expected Returns and the Arbitrage Pricing Theory. Valuation in Theory. Valuation in Practice. Part III: Information Processing. Forecasting Basics. Advanced Forecasting. Information Analysis. The Information Horizon. Part IV: Implementation. Portfolio Construction. Long/Short Investing. Transaction Costs, Turnover, and Trading. Performance Analysis. Asset Allocation. Benchmark Timing. The Historical Record for Active Management. Open Questions. Summary. Appendice A: Standard Notation. B: Glossary. C: Return and Statistics Basics.","Richard C. Grinold,Richard C. Grinold,Ronald N. Kahn",,2000,,,
133,The ETF Handbook: How to Value and Trade Exchange Traded Funds,"Preface.  Acknowledgments.  Part One Introduction to the ETF Marketplace.  Chapter 1 Development of an ETF.  Market Access or Out-Performance?  Index Tracking or Actively Managed?  Underlying Assets.  Rebalancing and Index Changes.  ETF Basket.  Conclusion.  Chapter 2 Structure of an ETF.  Categorizing Exchange-Traded Products.  ETF Regulation.  Exchange-Traded Notes (ETNs).  Taxation.  When Structural Issues Arise.  Conclusion.  Chapter 3 Bringing an ETF to the Market.  Partnering with an Exchange.  Lead Market Maker.  ETF Incubation.  Comparing Fees by Structure.  Marketing and Launch.  Conclusion.  Chapter 4 Investment Companies, Now and in the Future.  In the Beginning, There Were Closed-End Funds.  Mutual Funds.  Actively Managed ETFs.  ETFs within the Portfolio.  Closing of ETFs.  Conclusion: The Future of ETFs.  Part Two Exchange- Traded Fund Valuation.  Chapter 5 ETFs with Domestic Constituents.  Calculating the Net Asset Value.  Discounts and Premiums.  Calculating the Intra-day Indicative Value.  Conclusion.  Chapter 6 ETFs with International Constituents.  International ETFs.  Providing Liquidity.  Conclusion.  Chapter 7 Fixed-Income and Currency ETFs.  Fixed Income.  Currency ETFs.  Conclusion.  Chapter 8 Leveraged, Inverse, and Commodity Products.  Introduction to Leveraged Products.  Understanding Inverse ETFs.  Commodity ETVs.  Conclusion.  Part Three ETF Trading and Execution.  Chapter 9 Trading Volumes and ETF Liquidity: Keys to Unlocking Value from the ETF Structure.  How is an ETF Different from a Stock?  A Brief Look at Equity Trading Volumes.  A Detailed Look at ETF Trading Volumes.  ETF Money Flows.  Conclusion.  Chapter 10 ETF Trading Business: Assessing and Providing Liquidity.  Trading Model.  Measuring Potential Available ETF Liquidity.  Requirements for an ETF Trading Business.  Conclusion.  Chapter 11: Execution: Handling Client Order Flow and Achieving Execution in ETFs.  Time Frames and Order Types.  Market Orders.  Limit Orders.  Algorithms.  Risk Markets (Utilizing Broker-Dealer Capital).  Creations and Redemptions.  Examples of Executions in the Market.  Conclusion.  Chapter 12: Market Participants and Their Trading Strategies.  Broker-Dealer Facilitation Desks.  Electronic Market Making.  Liquidity Aggregators.  Trading Strategies.  Conclusion.  Appendix A: List of ETF Issuers.  Appendix B: Research and Data Providers.  Appendix C: ETF-Related Web Sites and Blogs.  Appendix D: List of ETFs in Registration.  Notes.  About the Author.  Index.",David J Abner,,2016,,,
134,Reinforcement Learning in Continuous State and Action Spaces,"Many traditional reinforcement-learning algorithms have been designed for problems with small finite state and action spaces. Learning in such discrete problems can been difficult, due to noise and delayed reinforcements. However, many real-world problems have continuous state or action spaces, which can make learning a good decision policy even more involved. In this chapter we discuss how to automatically find good decision policies in continuous domains. Because analytically computing a good policy from a continuous model can be infeasible, in this chapter we mainly focus on methods that explicitly update a representation of a value function, a policy or both. We discuss considerations in choosing an appropriate representation for these functions and discuss gradient-based and gradient-free ways to update the parameters. We show how to apply these methods to reinforcement-learning problems and discuss many specific algorithms. Amongst others, we cover gradient-based temporal-difference learning, evolutionary strategies, policy-gradient algorithms and (natural) actor-critic methods. We discuss the advantages of different approaches and compare the performance of a state-of-the-art actor-critic method and a state-of-the-art evolutionary strategy empirically.",Hado van Hasselt,,2012,,,
135,New Insights into Decision Trees Ensembles,"Les ensembles d’arbres constituent a l’heure actuelle l’une des methodes d’apprentissage statistique les plus performantes. Toutefois, leurs proprietes theoriques, ainsi que leurs performances empiriques restent sujettes a de nombreuses questions. Nous proposons dans cette these d’apporter un nouvel eclairage a ces methodes. Plus particulierement, apres avoir evoque les aspects theoriques actuels (chapitre 1) de trois schemas ensemblistes principaux (Forets aleatoires, Boosting et Discrimination Stochastique), nous proposerons une analyse tendant vers l’existence d’un point commun au bien fonde de ces trois principes (chapitre 2). Ce principe tient compte de l’importance des deux premiers moments de la marge dans l’obtention d’un ensemble ayant de bonnes performances. De la, nous en deduisons un nouvel algorithme baptise OSS (Oriented Sub-Sampling) dont les etapes sont en plein accord et decoulent logiquement du cadre que nous introduisons. Les performances d’OSS sont empiriquement superieures a celles d’algorithmes en vogue comme les Forets aleatoires et AdaBoost. Dans un troisieme volet (chapitre 3), nous analysons la methode des Forets aleatoires en adoptant un point de vue « noyau ». Ce dernier permet d’ameliorer la comprehension des forets avec, en particulier la comprehension et l’observation du mecanisme de regularisation de ces techniques. Le fait d’adopter un point de vue noyau permet d’ameliorer les Forets aleatoires via des methodes populaires de post-traitement comme les SVM ou l’apprentissage de noyaux multiples. Ceux-ci demontrent des performances nettement superieures a l’algorithme de base, et permettent egalement de realiser un elagage de l’ensemble en ne conservant qu’une petite partie des classifieurs le composant.",Vincent Pisetta,,2012,,,
136,Commodities’ price trend forecasting by a neuro-fuzzy controller,"This paper presents a novel technique to forecast the price trend (direction) of 25 different commodities, listed on international markets, using a neuro-fuzzy controller. The forecasting system is based on two independent adaptive neural fuzzy inference systems (ANFISs) that form an inverse controller for each commodity. The ANFIS controller belongs to direct control and is based on inverse learning, also known as general learning. Daily data return sets, for the period 14th October 2009 until 28th September 2012 for 25 different commodities, are used to learn and evaluate the proposed system. The results of the trading simulation and the experimental investigations carried out in the laboratory are provided. The forecast accuracy of the proposed technique is evaluated by out-of-sample tests. The return on equity based on the hit rate and the comparison of equity with the buy and hold strategy are the central evaluation criteria. The results are very encouraging, showing high accuracy of the hit rate reaching 68.33 % and a notable superiority of the return on equity when compared with the buy and hold strategy. Also the performance of the model is compared with that of other approaches.","George S. Atsalakis,Dimitrios Frantzis,Constantin Zopounidis",Energy Systems,2016,,,
137,Personalized Recommendation Based on Weighted Sequence Similarity,"The sequential pattern mining-based recommendation has recently become a popular research topic in the field of recommender system. However, this kind of methods usually relies on frequency counts of sequences, which makes low-frequency sequences contribute little for the final recommend results. To solve this problem, in this paper, we propose a weighted sequence similarity-based method, called Personalized Recommendation based on Sequence Similarity (PRSS), for personalized recommendation. First, item-sequence weight model is introduced, which can reflect different importance of different items to different sequences. Then, target users’ sequence is compared with historical sequences using similarity function. Finally, the maximal common subsequence is proposed to rank candidate sequences and make recommendation. Experimental results show that PRSS generates more accurate recommendation for the target users.","Wei Song,Kai Yang",,2014,,,
138,Financial Stock Market Forecast using Data Mining Techniques,"The automated computer programs using data mining and predictive technologies do a fare amount of trades in the markets. Data mining is well founded on the theory that the historic data holds the essential memory for predicting the future direction. This technology is designed to help investors discover hidden patterns from the historic data that have probable predictive capability in their investment decisions. The prediction of stock markets is regarded as a challenging task of financial time series prediction. Data analysis is one way of predicting if future stocks prices will increase or decrease. Five methods of analyzing stocks were combined to predict if the day's closing price would increase or decrease. These methods were Typical Price (TP), Bollinger Bands, Relative Strength Index (RSI), CMI and Moving Average (MA). This paper discussed various techniques which are able to predict with future closing stock price will increase or decrease better than level of significance. Also, it investigated various global events and their issues predicting on stock markets. It supports numerically and graphically.","K. Senthamarai Kannan,K. Senthamarai Kannan,K. Senthamarai Kannan,K. Senthamarai Kannan,P. Sailapathi Sekar,P. Arumugam,P. Arumugam,P. Arumugam",,2010,,,
139,Training very deep networks,"Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.","Rupesh Kumar Srivastava,Klaus Greff,Jürgen Schmidhuber",,2015,,,
140,An intelligent short term stock trading fuzzy system for assisting investors in portfolio management,"The proposed short-term fuzzy system uses a set of appropriate technical indicators.The returns of the proposed system are higher than those of the B&H strategy.The proposed system avoids big losses during bear markets.During bull markets the system produces lower returns than the B&H strategy.Transaction costs significantly affect the performance of the proposed system. Financial markets are complex systems influenced by many interrelated economic, political and psychological factors and characterised by inherent nonlinearities. Recently, there have been many efforts towards stock market prediction, applying various fuzzy logic techniques and using technical analysis methods.This paper presents a short term trading fuzzy system using a novel trading strategy and an ""amalgam"" between altered commonly used technical indicators and rarely used ones, in order to assist investors in their portfolio management. The sample consists of daily data from the general index of the Athens Stock Exchange over a period of more than 15 years (15/11/1996 to 5/6/2012), which was also divided into distinctive groups of bull and bear market periods.The results suggest that, with or without taking into consideration transaction costs, the return of the proposed fuzzy model is superior to the returns of the buy and hold strategy. ?he proposed system can be characterised as conservative, since it produces smaller losses during bear market periods and smaller gains during bull market periods compared with the buy and hold strategy.","Konstandinos Chourmouziadis,Prodromos D. Chatzoglou",Expert Systems With Applications,2016,,,
141,Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering,"In this paper, we present the mQA model, which is able to answer questions about the content of an image. The answer can be a sentence, a phrase or a single word. Our model contains four components: a Long Short-Term Memory (LSTM) to extract the question representation, a Convolutional Neural Network (CNN) to extract the visual representation, an LSTM for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer. We construct a Freestyle Multilingual Image Question Answering (FM-IQA) dataset to train and evaluate our mQA model. It contains over 150,000 images and 310,000 freestyle Chinese question-answer pairs and their English translations. The quality of the generated answers of our mQA model on this dataset is evaluated by human judges through a Turing Test. Specifically, we mix the answers provided by humans and our model. The human judges need to distinguish our model from the human. They will also provide a score (i.e. 0, 1, 2, the larger the better) indicating the quality of the answer. We propose strategies to monitor the quality of this evaluation process. The experiments show that in 64.7% of cases, the human judges cannot distinguish our model from humans. The average score is 1.454 (1.918 for human). The details of this work, including the FM-IQA dataset, can be found on the project page: this http URL","Haoyuan Gao,Junhua Mao,Jie Zhou,Jie Zhou,Jie Zhou,Jie Zhou,Jie Zhou,Zhiheng Huang,Lei Wang,Lei Wang,Wei Xu",arXiv: Computer Vision and Pattern Recognition,2015,,,
142,Learning representations by back-propagating errors,"We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.","David E. Rumelhart,Geoffrey E. Hinton,Ronald J. Williams,Ronald J. Williams",Nature,1988,,,
143,Error-correcting output coding corrects bias and variance,"Previous research has shown that a technique called error-correcting output coding (ECOC) can dramatically improve the classification accuracy of supervised learning algorithms that learn to classify data points into one of k ≫ 2 classes. This paper presents an investigation of why the ECOC technique works, particularly when employed with decision-tree learning algorithms. It shows that the ECOC method— like any form of voting or committee—can reduce the variance of the learning algorithm. Furthermore—unlike methods that simply combine multiple runs of the same learning algorithm—ECOC can correct for errors caused by the bias of the learning algorithm. Experiments show that this bias correction ability relies on the non-local behavior of C4.5.","Eun Bae Kong,Thomas G. Dietterich",,1995,,,
144,Transfer learning in collaborative filtering for sparsity reduction,"Data sparsity is a major problem for collaborative filtering (CF) techniques in recommender systems, especially for new users and items. We observe that, while our target data are sparse for CF systems, related and relatively dense auxiliary data may already exist in some other more mature application domains. In this paper, we address the data sparsity problem in a target domain by transferring knowledge about both users and items from auxiliary data sources. We observe that in different domains the user feedbacks are often heterogeneous such as ratings vs. clicks. Our solution is to integrate both user and item knowledge in auxiliary data sources through a principled matrix-based transfer learning framework that takes into account the data heterogeneity. In particular, we discover the principle coordinates of both users and items in the auxiliary data matrices, and transfer them to the target domain in order to reduce the effect of data sparsity. We describe our method, which is known as coordinate system transfer or CST, and demonstrate its effectiveness in alleviating the data sparsity problem in collaborative filtering. We show that our proposed method can significantly outperform several state-of-the-art solutions for this problem.","Weike Pan,Evan Wei Xiang,Nathan N. Liu,Qiang Yang",,2010,,,
145,Gaussian processes in machine learning,We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.,Carl Edward Rasmussen,Lecture Notes in Computer Science,2003,,,
146,Incorporating pageview weight into an association-rule-based web recommendation system,"Web recommendation systems based on web usage mining try to mine users' behavior patterns from web access logs, and recommend pages to the online user by matching the user's browsing behavior with the mined historical behavior patterns. Recommendation approaches proposed in previous works, however, do not distinguish the importance of different pageviews, and all the visited pages are treated equally whatever their usefulness to the user. We propose to use pageview duration to judge its usefulness to a user, and try to give more consideration to more useful pageviews, in order to better capture the user's information need and recommend pages more useful to the user. In this paper we try to incorporate pageview weight into the Association Rule (AR) based model and develop a Weighted Association Rule (WAR) model. Comparative experiment of the two shows a significant improvement in the recommendation effectiveness with the proposed WAR model.","Liang Yan,Chunping Li",,2006,,,
147,Efficient Global Optimization of Expensive Black-Box Functions,"In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.","Donald R. Jones,Matthias Schonlau,William J. Welch",Journal of Global Optimization,1998,,,
148,Classification with Noisy Labels by Importance Reweighting,"In this paper, we study a classification problem in which sample labels are randomly corrupted. In this scenario, there is an unobservable sample with noise-free labels. However, before being observed, the true labels are independently flipped with a probability   $\rho \in [0,0.5)$      , and the random label noise can be class-conditional. Here, we address two fundamental problems raised by this scenario. The first is how to best use the abundant surrogate loss functions designed for the traditional classification problem when there is label noise. We prove that any surrogate loss function can be used for classification with noisy labels by using importance reweighting, with consistency assurance that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample. The other is the open problem of how to obtain the noise rate    $\rho$      . We show that the rate is upper bounded by the conditional probability    $P(\hat{Y}|X)$       of the noisy sample. Consequently, the rate can be estimated, because the upper bound can be easily reached in classification problems. Experimental results on synthetic and real datasets confirm the efficiency of our methods.","Tongliang Liu,Dacheng Tao",IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,,,
149,Bias plus variance decomposition for zero-one loss functions,,"Ron Kohavi,David H. Wolpert",,1996,,,
150,Intraday FX Trading: An Evolutionary Reinforcement Learning Approach,"We have previously described trading systems based on unsupervised learning approaches such as reinforcement learning and genetic algorithms which take as input a collection of commonly used technical indicators and generate profitable trading decisions from them. This article demonstrates the advantages of applying evolutionary algorithms to the reinforcement learning problem using a hybrid credit assignment approach. In earlier work, the temporal difference reinforcement learning approach suffered from problems with overfitting the in-sample data. This motivated the present approach.Technical analysis has been shown previously to have predictive value regarding future movements of foreign exchange prices and this article presents methods for automated high-frequency FX trading based on evolutionary reinforcement learning about signals from a variety of technical indicators. These methods are applied to GBPUSD, USDCHF and USDJPY exchange rates at various frequencies. Statistically significant profits are made consistently at transaction costs of up to 4bp for the hybrid system while the standard RL is only able to trade profitably up to about 1bp slippage per trade.","Michael A. H. Dempster,Y. S. Romahi",,2002,,,
151,A Unifeid Bias-Variance Decomposition and its Applications,"This paper presents a unified bias-variance decomposition that is applicable to squared loss, zero-one loss, variable misclassification costs, and other loss functions. The unified decomposition sheds light on a number of significant issues: the relation between some of the previously-proposed decompositions for zero-one loss and the original one for squared loss, the relation between bias, variance and Schapire et al.’s (1997) notion of margin, and the nature of the trade-off between bias and variance in classification. While the biasvariance behavior of zero-one loss and variable misclassification costs is quite different from that of squared loss, this difference derives directly from the different definitions of loss. We have applied the proposed decomposition to decision tree learning, instancebased learning and boosting on a large suite of benchmark data sets, and made several significant observations.",Pedro Domingos,,2000,,,
152,On Bayesian Methods for Seeking the Extremum,Many well known methods for seeking the extremum had been developed on the basis of quadratic approximation.,"Jonas Mockus,Jonas Mockus",,1974,,,
153,Online bagging and boosting,"Bagging and boosting are two of the most well-known ensemble learning methods due to their theoretical performance guarantees and strong experimental results. However, these algorithms have been used mainly in batch mode, i.e., they require the entire training set to be available at once and, in some cases, require random access to the data. In this paper, we present online versions of bagging and boosting that require only one pass through the training data. We build on previously presented work by describing some theoretical results. We also compare the online and batch algorithms experimentally in terms of accuracy and running time.","Nikunj C. Oza,N.C. Oza",,2005,,,
154,Content-based recommendation systems,"This chapter discusses content-based recommendation systems, i.e., systems that recommend an item to a user based upon a description of the item and a profile of the user's interests. Content-based recommendation systems may be used in a variety of domains ranging from recommending web pages, news articles, restaurants, television programs, and items for sale. Although the details of various systems differ, content-based recommendation systems share in common a means for describing the items that may be recommended, a means for creating a profile of the user that describes the types of items the user likes, and a means of comparing items to the user profile to determine what to recommend. The profile is often created and updated automatically in response to feedback on the desirability of items that have been presented to the user.","Michael J. Pazzani,Daniel Billsus",,2007,,,
155,Understanding the difficulty of training deep feedforward neural networks,"Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: WC Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a “better” basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact).","Xavier Glorot,Yoshua Bengio",,2010,,,
156,Ensemble Methods in Machine Learning,"Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.",Thomas G. Dietterich,,2000,,,
157,Incremental learning by heterogeneous bagging ensemble,"Classifier ensemble is a main direction of incremental learning researches, and many ensemble-based incremental learning methods have been presented. Among them, Learn++, which is derived from the famous ensemble algorithm, AdaBoost, is special. Learn++ can work with any type of classifiers, either they are specially designed for incremental learning or not, this makes Learn++ potentially supports heterogeneous base classifiers. Based on massive experiments we analyze the advantages and disadvantages of Learn++. Then a new ensemble incremental learning method, Bagging++, is presented, which is based on another famous ensemble method: Bagging. The experimental results show that Bagging ensemble is a promising method for incremental learning and heterogeneous Bagging++ has the better generalization and learning speed than other compared methods such as Learn++ and NCL.","Qiang Li Zhao,Yang Huang Jiang,Ming Xu",,2010,,,
158,"Convolutional networks for images, speech, and time series",,"Yann LeCun,Yoshua Bengio",,1998,,,
159,Stochastic Nonstationary Optimization for Finding Universal Portfolios,"We apply ideas from stochastic optimization for defining universal portfolios. Universal portfolios are that class of portfolios which are constructed directly from the available observations of the stocks behavior without any assumptions about their statistical properties. Cover [7] has shown that one can construct such portfolio using only observations of the past stock prices which generates the same asymptotic wealth growth as the best constant rebalanced portfolio which is constructed with the full knowledge of the future stock market behavior. In this paper we construct universal portfolios using a different set of ideas drawn from nonstationary stochastic optimization. Our portfolios yield the same asymptotic growth of wealth as the best constant rebalanced portfolio constructed with the perfect knowledge of the future and they are less demanding computationally compared to previously known universal portfolios. We also present computational evidence using New York Stock Exchange data which shows, among other things, superior performance of portfolios which explicitly take into account possible nonstationary market behavior. Copyright Kluwer Academic Publishers 2000","Alexei A. Gaivoronski,Fabio Stella",Annals of Operations Research,2000,,,
160,"Handbook of Computational Economics, Volume 2: Agent-Based Computational Economics","This handbook comprises 16 chapters surveying agent-based computational economics research, 6 shorter essays providing personal perspectives, and a ""getting started"" guide for newcomers to agent-based modeling in the social sciences. Research topics covered include: learning representations for computational agents; agent-based models and human-subject experiments; economic activity on fixed networks; endogenous formation of economic networks; social dynamics and the evolution of norms; heterogenous agent modeling in economics and finance; agent-based computational finance; agent-based models of innovation and technological change; agent-based models of organizations; market design using agent-based models; automated markets and trading agents; agent-based computational methods and models of politics; agent-based tools for exploring the governance of social-ecological systems; and computational laboratories for spatial agent-based modeling. Related work can be accessed at http://www2.econ.iastate.edu/tesfatsi/ace.htm","Leigh Tesfatsion,Kenneth L. Judd",,2006,,,
161,Where you like to go next: successive point-of-interest recommendation,"Personalized point-of-interest (POI) recommendation is a significant task in location-based social networks (LBSNs) as it can help provide better user experience as well as enable third-party services, e.g., launching advertisements. To provide a good recommendation, various research has been conducted in the literature. However, pervious efforts mainly consider the ""check-ins"" in a whole and omit their temporal relation. They can only recommend POI globally and cannot know where a user would like to go tomorrow or in the next few days. In this paper, we consider the task of successive personalized POI recommendation in LBSNs, which is a much harder task than standard personalized POI recommendation or prediction. To solve this task, we observe two prominent properties in the check-in sequence: personalized Markov chain and region localization. Hence, we propose a novel matrix factorization method, namely FPMC-LR, to embed the personalized Markov chains and the localized regions. Our proposed FPMC-LR not only exploits the personalized Markov chain in the check-in sequence, but also takes into account users' movement constraint, i.e., moving around a localized region. More importantly, utilizing the information of localized regions, we not only reduce the computation cost largely, but also discard the noisy information to boost recommendation. Results on two real-world LBSNs datasets demonstrate the merits of our proposed FPMC-LR.","Chen Cheng,Haiqin Yang,Michael R. Lyu,Irwin King",,2013,,,
162,Using Association Analysis of Web Data in Recommender Systems,"The numerous web sites existing nowadays make available more information than a user can manage. Thus, an essential requirement of current web applications is to provide users with instruments for personalized selective retrieval of web information. In this paper, a procedure for making personalized recommendations is proposed. The method is based on building a predictive model from an association model of Web data. It uses a set of association rules generated by a data mining algorithm that discovers knowledge in an incremental way. These rules provide models with relevant patterns that minimize the recommendation errors.","María N. Moreno,Francisco José García-Peñalvo,Francisco J. Garcia,M. José Polo,Vivian F. López",,2004,,,
163,AI-based Trading in Open Distributed Environments,"An open distributed environment can be perceived as a service market where services are freely offered and requested. Any infrastructure which seeks to provide appropriate mechanisms for such an environment has to include mediator functionality (i.e. a trader) that matches service requests and service offers. Commonly, the matching process is based upon some IDL–based service type definition, and the types of the various services have to be “standardized” and distributed a priori to all potential participants. We argue that such well defined “standards” are too inflexible and even contradict the idea of an open service market. Therefore we propose a new type notation based on conceptual graphs. The trader maintains a knowledge base about service types in form of conceptual graphs. During the trader operations the service type knowledge evolves as it is continuously refined and extended. Users of the trading service interact with the trader and formulate queries in a corresponding notation that allows for a conceptual specification of the desired service type. Adequate matching algorithms and protocols have been implemented.","Arno Puder,S. Markwitz,F. Gudermann,K. Geihs,Kurt Geihs",,1995,,,
164,Variance-Penalized Reinforcement Learning for Risk-Averse Asset Allocation,"The tasks of optimizing asset allocation considering transaction costs can be formulated into the framework of Markov Decision Processes (MDPs) and reinforcement learning. In this paper, a risk-averse reinforcement learning algorithm is proposed which improves asset allocation strategy of portfolio management systems. The proposed algorithm alternates policy evaluation phases which take into account the mean and variance of return under a given policy and policy improvement phases which follow the variance-penalized criterion. The algorithm is tested on trading systems for a single future corresponding to a Japanese stock index.","Makoto Sato,Shigenobu Kobayashi",,2000,,,
165,Context-Aware Recommender Systems,"Context-aware recommender systems (CARS) generate more relevant recommendations by adapting  them to the specific contextual situation of the user. This article explores how contextual  information can be used to create more intelligent and useful recommender systems.   It provides an overview of the multifaceted notion of context, discusses several  approaches for incorporating contextual information in recommendation process, and  illustrates the usage of such approaches in several application areas where different types of contexts are exploited.  The article concludes by discussing  the challenges and future research directions for context-aware recommender systems.","Gediminas Adomavicius,Bamshad Mobasher,Francesco Ricci,Alexander Tuzhilin",Ai Magazine,2011,,,
166,Expert-Driven Validation of Rule-Based User Models in Personalization Applications,"In many e-commerce applications, ranging from dynamic Web content presentation, to personalized ad targeting, to individual recommendations to the customers, it is important to build personalized profiles of individual users from their transactional histories. These profiles constitute models of individual user behavior and can be specified with sets of rules learned from user transactional histories using various data mining techniques. Since many discovered rules can be spurious, irrelevant, or trivial, one of the main problems is how to perform post-analysis of the discovered rules, i.e., how to validate user profiles by separating “good” rules from the “bad.” This validation process should be done with an explicit participation of the human expert. However, complications may arise because there can be very large numbers of rules discovered in the applications that deal with many users, and the expert cannot perform the validation on a rule-by-rule basis in a reasonable period of time. This paper presents a framework for building behavioral profiles of individual users. It also introduces a new approach to expert-driven validation of a very large number of rules pertaining to these users. In particular, it presents several types of validation operators, including rule grouping, filtering, browsing, and redundant rule elimination operators, that allow a human expert validate many individual rules at a time. By iteratively applying such operators, the human expert can validate a significant part of all the initially discovered rules in an acceptable time period. These validation operators were implemented as a part of a one-to-one profiling system. The paper also presents a case study of using this system for validating individual user rules discovered in a marketing application.","Gediminas Adomavicius,Alexander Tuzhilin",Data Mining and Knowledge Discovery,2001,,,
167,Support Vector Machine Ensemble with Bagging,"Even the support vector machine (SVM) has been proposed to provide a good generalization performance, the classification result of the practically implemented SVM is often far from the theoretically expected level because their implementations are based on the approximated algorithms due to the high complexity of time and space. To improve the limited classification performance of the real SVM, we propose to use the SVM ensembles with bagging (bootstrap aggregating). Each individual SVM is trained independently using the randomly chosen training samples via a bootstrap technique. Then, they are aggregated into to make a collective decision in several ways such as the majority voting, the LSE(least squares estimation)-based weighting, and the double-layer hierarchical combining. Various simulation results for the IRIS data classification and the hand-written digit recognitionshow that the proposed SVM ensembles with bagging outperforms a single SVM in terms of classification accuracy greatly.","Hyun-Chul Kim,Hyun-Chul Kim,Shaoning Pang,Hongmo Je,Daijin Kim,Sung Yang Bang,Sung Yang Bang",Lecture Notes in Computer Science,2002,,,
168,"Options, Futures, and Other Derivatives","Contents: Introduction. Futures Markets and the Use of Futures for Hedging. Forward and Futures Prices. Interest Rate Futures. Swaps. Options Markets. Properties of Stock Option Prices. Trading Strategies Involving Options. Introduction to Binomial Trees. Model of the Behavior of Stock Prices. The Black-Scholes Analysis. Options on Stock Indices, Currencies, and Futures Contracts. General Approach to Pricing Derivatives. The Management of Market Risk. Numerical Procedures. Interest Rate Derivatives and the Use of Black's Model. Interest Rate Derivatives and Models of the Yield Curve. Exotic Options. Alternatives to Black-Scholes for Option Pricing. Credit Risk and Regulatory Capital. Review of Key Concepts.",John C. Hull,,1989,,,
169,An Evaluation of Alternative Equity Indices - Part 1: Heuristic and Optimised Weighting Schemes,"There is now a dazzling array of alternatives to the market-cap approach to choosing constituent weights for equity indices. Using data on the 1,000 largest US stocks every year from 1968 to the end of 2011 we compare and contrast the performance of a set of alternative indexing approaches. The alternatives that we explore can be loosely categorised into two groups. First, a set of weighting techniques that Chow et al (2011) describe as “heuristic.” The second set are based upon “optimisation techniques,” since they all require the maximisation or minimisation of some mathematical function subject to a set of constraints to derive the constituent weights. We find that all of the alternative indices considered here would have produced a better risk-adjusted performance than could have been achieved by having a passive exposure to a market capitalisation-weighted index. However, the most important result of our work stems from our ten million Monte Carlo simulations. We find that choosing constituent weights randomly, that is, applying weights that could have been chosen by monkeys, would also have produced a far better risk-adjusted performance than that produced by a cap-weighted scheme.","Andrew Clare,Nick Motson,Steve Thomas,Stephen Thomas",,2013,,,
170,Randomizing Outputs to Increase Prediction Accuracy,"Bagging and boosting reduce error by changing both the inputs and outputs to form perturbed training sets, growing predictors on these perturbed training sets and combining them. An interesting question is whether it is possible to get comparable performance by perturbing the outputs alone. Two methods of randomizing outputs are experimented with. One is called output smearing and the other output flipping. Both are shown to consistently do better than bagging.",Leo Breiman,Machine Learning,2000,,,
171,Bollinger on Bollinger Bands,"'All in all, ""Bollinger on Bollinger Bands"" is a gem...within 10 minutes of opening it, it went on my list of the five best technical analysis books ever' - Active Trader. This is the first comprehensive traders' guide to using Bollinger Bands from the man who created them. It includes a handy Bollinger Bands reference card. Over the past two decades, thousands of veteran traders have come to view Bollinger Bands as the most representative and reliable tool for assessing expected price action. Now, in the long-anticipated ""Bollinger on Bollinger Bands"", John Bollinger himself explains how to use this extraordinary technique to effectively compare price and indicator movements.Traders can look to this techniques-oriented book for hundreds of valuable insights, including: analysis of the primary indicators derived from Bollinger Bands percentage b and BandWidth; how traders can use Bollinger Bands to work with instead of against commonly encountered trading patterns; strategic use of Bollinger Bands in short-, mid-, and long-term trading programs; three trading systems based on Bollinger Bands; and, the essence of successful investing is to determine when a stock's price is too high or too low and then act accordingly. While John Bollinger would be the first to argue that no techniques exist for definitively determining these levels, ""Bollinger on Bollinger Bands"" presents an insider's examination of the one tool that if its widespread popularity is an accurate judge of its effectiveness comes closer than anything else. Concise yet comprehensive, it is one of today's truly indispensable investment guidebooks. 'The purpose of this book is to help you avoid many of the common traps investors get caught in, including the buy-low, sell-high trap, where the investor buys only to watch the stock continue downward or sells only to watch the stock continue upward. Here, the traditional, emotional approach to the markets is replaced with a relative framework within which prices can be evaluated in a rigorous manner leading to a series of rational investment decisions without reference to absolute truths' - From the Introduction.In the 1970s, market newcomer John Bollinger couldn't find a system of investment analysis to fit his belief that all market events exist only in relation to one another and that there are no absolutes. So he created his own. That approach Rational Analysis led to the establishment of Bollinger Bands and ensured Bollinger's spot in investment analysis history. Now, in ""Bollinger on Bollinger Bands"", John Bollinger explains the market conditions that led to his initial discovery, and gives readers the inside story of the development and refinement of Bollinger Bands. He then goes on to present a relative decision framework built around Rational Analysis and Bollinger Bands, an extraordinarily powerful combination of technical and fundamental analysis that answers the question of whether prices are too high or too low for virtually any security or market environment.By understanding how to incorporate Bollinger's techniques into their own investment strategy, investors will greatly increase their ability to ignore often-costly emotions and arrive at rational decisions supported by both the facts and the underlying market environment. ""Bollinger on Bollinger Bands"" provides: the first authoritative examination of this revolutionary technical analysis tool; three simple systems for implementing Bollinger Bands; and, innovative methods for clarifying patterns and analyzing time frames and moving averages.The key to effective investment analysis is to as much as possible eliminate emotion and approach each trade on its technical and fundamental merits alone. Since their introduction, few analytical techniques have helped investors do this better or more consistently than Bollinger Bands. ""Bollinger on Bollinger Bands"" provides tips, guidelines, and rules for incorporating the bands into virtually any investment strategy. It is a watershed book, written by the only man truly qualified to claim a comprehensive knowledge of the topic John Bollinger himself.",John J. Bollinger,,2001,,,
172,Risk Sensitive Reinforcement Learning,"Most reinforcement learning algorithms optimize the expected return of a Markov Decision Problem. Practice has taught us the lesson that this criterion is not always the most suitable because many applications require robust control strategies which also take into account the variance of the return. Classical control literature provides several techniques to deal with risk-sensitive optimization goals like the so-called worst-case optimality criterion exclusively focusing on risk-avoiding policies or classical risk-sensitive control, which transforms the returns by exponential utility functions. While the first approach is typically too restrictive, the latter suffers from the absence of an obvious way to design a corresponding model-free reinforcement learning algorithm.

Our risk-sensitive reinforcement learning algorithm is based on a very different philosophy. Instead of transforming the return of the process, we transform the temporal differences during learning. While our approach reflects important properties of the classical exponential utility framework, we avoid its serious drawbacks for learning. Based on an extended set of optimality equations we are able to formulate risk-sensitive versions of various well-known reinforcement learning algorithms which converge with probability one under the usual conditions.","Ralph Neuneier,Oliver Mihatsch",,1998,,,
173,"Data Clustering: Theory, Algorithms, and Applications","Preface Part I. Clustering, Data and Similarity Measures: 1. Data clustering 2. DataTypes 3. Scale conversion 4. Data standardization and transformation 5. Data visualization 6. Similarity and dissimilarity measures Part II. Clustering Algorithms: 7. Hierarchical clustering techniques 8. Fuzzy clustering algorithms 9. Center Based Clustering Algorithms 10. Search based clustering algorithms 11. Graph based clustering algorithms 12. Grid based clustering algorithms 13. Density based clustering algorithms 14. Model based clustering algorithms 15. Subspace clustering 16. Miscellaneous algorithms 17. Evaluation of clustering algorithms Part III. Applications of Clustering: 18. Clustering gene expression data Part IV. Matlab and C++ for Clustering: 19. Data clustering in Matlab 20. Clustering in C/C++ A. Some clustering algorithms B. Thekd-tree data structure C. Matlab Codes D. C++ Codes Subject index Author index.","Guojun Gan,Chaoqun Ma,Jianhong Wu",,2007,,,
174,On Equilibria of Bid-Ask Markets,"Among his many contributions to economic theory, Kenneth Arrow’s studies of general equilibrium are especially important to the continuing development of the fine structure of market-mediated allocation processes. The paradigm of efficient decentralized allocation via market clearing prices developed from the Walrasian model in the long line of research given its greatest impetus by Arrow, Gerard Debreu, and their colleagues. The demonstration that ‘perfectly’ competitive complete markets, characterized by universal price-taking behaviour, can in principle (absent non-convexities, and so on) attain an efficient allocation set the cornerstone of the theory of markets. By establishing the standard against which further studies of imperfectly competitive and incomplete markets are compared, this accomplishment continues to shape the agenda of continuing research on competitive processes.","Robert B. Wilson,Robert Wilson,Robert B. Wilson",,1987,,,
175,Technical Analysis from A to Z,"Introduction to Technical Analysis. Some History. The Human Element. Fundamental Analysis. The Future Can Be Found in the Past. The Weatherman. The Roulette Wheel. Computerized Trading. Price Fields. Charts. Line Charts. Bar Charts. Semi-Log Versus Linear Scaling. Volume Bar Chart. Other Chart Types. Periodicity. The Time Element. Support and Resistance. Supply and Demand. Traders' Remorse. Resistance Becomes Support. Review. Trends. Moving Averages. Time Periods in Moving Averages. Merits. Traders' Remorse. Indicators. Moving Average Convergence/Divergence (MACD). Leading Versus Lagging Indicators. Trending Prices Versus Trading Prices. Divergences. Market Indicators. Categories of Market Indicators. Line Studies. A Sample Approach. Conclusion. Reference. Absolute Breadth Index. Accumulation/Distribution Line. Accumulation Swing Index. Advance/Decline Line. Advance/Decline Ratio. Advancing-Declining Issues. Advancing, Declining, Unchanged Volume. Andrew's Pitchfork. Arms Index (Trin). Aroon. Average True Range. Bollinger Bands. Breadth Thrust. Bull/Bear Ratio. Candlesticks, Japanese. Canslim. Chaikin Money Flow. Chaikin Oscillator. Chande Momentum Oscillator. Commodity Channel Index. Commodity Selection Index. Cycles. Demand Index. Detrended Price Oscillator. Directional Movement. Double Exponential Moving Average. Dow Theory. Dynamic Momentum Index. Ease of Movement. Efficient Market Theory. Elliot Wave Theory. Envelopes (Trading Bands). Equivolume. Fibonacci Studies. Forecast Oscillator. Four Percent Model. Fourier Transform. Fundamental Analysis. Gann Angles. Herrick Payoff Index. Inertia. Interest Rates. Intraday Momentum Index. Kagi. Klinger Oscillator. Large Block Ratio. linear Regression Indicator. Linear Regression Slope. Linear Regression Trendlines. Market Facilitation Index. Mass Index. McClellan Oscillator. McClellan Summation Index. Median Price. Member Short Ratio. Mesa Sine Wave. Momentum. Money Flow Index. Moving Average Convergence/Divergence. Moving Averages. Negative Volume Index. New Highs-Lows Cummulative. New Highs-Lows Ratio. New High-New Lows. Odd Lot Balance Index. Odd Lot Purchases/Sales. Odd Lot Short Ratio. Odds Probability Cones. On Balance Volume. Open Interest. Open-10 Trin. Option Analysis. Overbrought/Oversold. Parabolic Sar. Patterns. Percent Retracement. Performance. Point and Figure. Polarized Fractal Efficiency. Positive Volume Index. Price and Volume Trend. Price Channel. Price Oscillator. Price Rate-of-Change. Projection Bands. Projection Oscillator. Public Short Ratio. Puts/Calls Ratio. Qstick. Quadrant Lines. R-Squared. Raff Regression Channel. Random Walk Index. Range Indicator. Rectangle. Relative Momentum Index. Relative Strength, Comparative. Relative Strength Index. Relative Volatility Index. Renko. Speed Resistance Lines. Spreads. Standard Deviation. Standard Deviation Channel. Standard Error. Standard Error Bands. Standard Error Channel. Stix. Stochastic Momentum Index. Stochastic Oscillator. Swing Index. Tema. Three Line Break. Time Line Break. Time Series Forecast. Throne Levels. Total Short Ratio. Trade Volume Index. Trendlines. Trix. Typical Price. Ultimate Oscillator. Upside/Downside Ratio. Upside/Downside Volume. Vertical Horizontal Filter. Volatility, Chaikin's. Volume. Volume Oscillator. Volume-Rate-of-Change. Weighted Close. Wilder's Smoothing. Williams's Accumulation/Distribution. Williams's %R. Zig Zag.",Steven B. Achelis,,1994,,,
176,A Multi-agent Q-learning Framework for Optimizing Stock Trading Systems,"This paper presents a reinforcement learning framework for stock trading systems. Trading system parameters are optimized by Q-learning algorithm and neural networks are adopted for value approximation. In this framework, cooperative multiple agents are used to efficiently integrate global trend prediction and local trading strategy for obtaining better trading performance. Agents communicate with others sharing training episodes and learned policies, while keeping the overall scheme of conventional Q-learning. Experimental results on KOSPI 200 show that a trading system based on the proposed framework outperforms the market average and makes appreciable profits. Furthermore, in view of risk management, the system is superior to a system trained by supervised learning.","Jae Won Lee,Jangmin O",,2002,,,
177,The Encyclopedia of Technical Market Indicators,"This book deals with the advantages of using technical market indicators, which to use, how and when to use them, and why. This new edition offers an accumulated treasury of nearly all known technical market indicators, including many new ones and better ways to use long-established ones. It offers precise formulas, performance over all available market history, and how to maximize reward/risk potentials. Now you can apply tested indicators immediately to your own investment decision making. Praise for the first edition: 'Objective. Systematic. Honest. Pulls no punches. Thorough. Extensively researched. Provides in-depth coverage of more than one hundred indicators, clearly revealing both their strengths and weaknesses. Shows you how to select indicators that work best to achieve your objectives while taking the guesswork out of your investing and trading. The source for the actual facts about technical market timing indicators' - Alan R. Shaw, CMT, Managing Director, Senior Advisor, Technical Research Department, Salomon Smith Barney. 'The most thoroughly documented research that I have ever seen. Shows hard evidence. Highly recommended' - Gerald Appel, Signalert Corporation. 'Credible. Must reading' - Norman G. Fosback, The Institute for Econometric Research, Inc. 'Terrific. The most helpful text' - Paul Rabbitt, RabbittAnalytics.Com. The same technical market indicators used by top-performing traders and investors are available now. This book offers the necessary knowledge on how to formulate and test technical market indicators in an orderly, step-by-step fashion. Specific technical market indicator parameters shown in this book would have maximized reward/risk performance over actual market history. Technical market indicators offer: logical, practical, efficient, effective, systematic, and precisely quantified frameworks for organizing information about actual observed market behavior, providing a firm foundation for making speculative decisions, grounded on historical precedent; flexibility and adaptability to any time frame and any trading instrument; and, clear-cut, precise, and objective signals that allow us to confidently execute trades while eliminating uncertainty, guesswork, confusion, anxiety, and stress, and freeing us from forecasts, opinion, bias, ego, hope, greed, and fear. Technical market indicators offer: a sensible and orderly procedure for selecting specific decision rules that would have maximized reward/risk performance over actual past market behavior; simple, intuitive, easy-to-understand, and precisely defined formulas based on a manageable number of variables, enabling us to execute decisions with the timely and disciplined consistency that is vital for success in the financial markets; accessibility, based on readily available technology and data; conservation of capital and precisely defined methods for risk control; and, risk reduction means greater consistency of profitable returns. This book shows you how to find a trading system that is right for you and how to apply it for best results increased profit, decreased risk, and the self-confidence of gaining control over your investment decision making.",Robert W. Colby,,1988,,,
178,Variance and Bias for General Loss Functions,"When using squared error loss, bias and variance and their decomposition of prediction error are well understood and widely used concepts. However, there is no universally accepted definition for other loss functions. Numerous attempts have been made to extend these concepts beyond squared error loss. Most approaches have focused solely on 0-1 loss functions and have produced significantly different definitions. These differences stem from disagreement as to the essential characteristics that variance and bias should display. This paper suggests an explicit list of rules that we feel any “reasonable” set of definitions should satisfy. Using this framework, bias and variance definitions are produced which generalize to any symmetric loss function. We illustrate these statistics on several loss functions with particular emphasis on 0-1 loss. We conclude with a discussion of the various definitions that have been proposed in the past as well as a method for estimating these quantities on real data sets.",Gareth M. James,Machine Learning,2003,,,
179,Efficient Estimation of Word Representations in Vector Space,"We propose two novel model architectures for computing continuous vector
representations of words from very large data sets. The quality of these
representations is measured in a word similarity task, and the results are
compared to the previously best performing techniques based on different types
of neural networks. We observe large improvements in accuracy at much lower
computational cost, i.e. it takes less than a day to learn high quality word
vectors from a 1.6 billion words data set. Furthermore, we show that these
vectors provide state-of-the-art performance on our test set for measuring
syntactic and semantic word similarities.","Tomas Mikolov,Kai Chen,Kai Chen,Greg S. Corrado,Jeffrey Dean",,2013,,,
180,Multi-agent reinforcement learning: independent vs. cooperative agents,"Intelligent human agents exist in a cooperative social environment that facilitates learning. They learn not only by trial-and-error, but also through  cooperation  by sharing instantaneous information, episodic experience, and learned knowledge. The key investigations of this paper are, “Given the same number of reinforcement learning agents, will cooperative agents outperform independent agents who do not communicate during learning?” and “What is the price for such cooperation?” Using independent agents as a benchmark, cooperative agents are studied in following ways: (1) sharing sensation, (2) sharing episodes, and (3) sharing learned policies. This paper shows that (a) additional sensation from another agent is beneficial if it can be used efficiently, (b) sharing learned policies or episodes among agents speeds up learning at the cost of communication, and (c) for joint tasks, agents engaging in partnership can significantly outperform independent agents although they may learn slowly in the beginning. These tradeoff's are not just limited to multi-agent reinforcement learning.",Ming Tan,,1997,,,
181,Ensemble clustering using factor graph,"In this paper, we propose a new ensemble clustering approach termed ensemble clustering using factor graph (ECFG). Compared to the existing approaches, our approach has three main advantages: (1) the cluster number is obtained automatically and need not to be specified in advance; (2) the reliability of each base clustering can be estimated in an unsupervised manner and exploited in the consensus process; (3) our approach is efficient for processing ensembles with large data sizes and large ensemble sizes. In this paper, we introduce the concept of super-object, which serves as a compact and adaptive representation for the ensemble data and significantly facilitates the computation. Through the probabilistic formulation, we cast the ensemble clustering problem into a binary linear programming (BLP) problem. The BLP problem is NP-hard. To solve this optimization problem, we propose an efficient solver based on factor graph. The constrained objective function is represented as a factor graph and the max-product belief propagation is utilized to generate the solution insensitive to initialization and converged to the neighborhood maximum. Extensive experiments are conducted on multiple real-world datasets, which demonstrate the effectiveness and efficiency of our approach against the state-of-the-art approaches. HighlightsIntroduce the super-object representation to facilitate the consensus process.Probabilistically formulate the ensemble clustering problem into a BLP problem.Propose an efficient solver for the BLP problem based on factor graph.The cluster number of the consensus clustering is estimated automatically.Our method achieves the state-of-the-art performance in effectiveness and efficiency.","Dong Huang,Jian-Huang Lai,Jian-Huang Lai,Jian-Huang Lai,Jian-Huang Lai,Chang-Dong Wang",Pattern Recognition,2016,,,
182,Effective next-items recommendation via personalized sequential pattern mining,"Based on the intuition that frequent patterns can be used to predict the next few items that users would want to access, sequential pattern mining-based next-items recommendation algorithms have performed well in empirical studies including online product recommendation. However, most current methods do not perform personalized sequential pattern mining, and this seriously limits their capability to recommend the best next-items to each specific target user. In this paper, we introduce a personalized sequential pattern mining-based recommendation framework. Using a novel Competence Score measure, the proposed framework effectively learns user-specific sequence importance knowledge, and exploits this additional knowledge for accurate personalized recommendation. Experimental results on real-world datasets demonstrate that the proposed framework effectively improves the efficiency for mining sequential patterns, increases the user-relevance of the identified frequent patterns, and most importantly, generates significantly more accurate next-items recommendation for the target users.","Ghim-Eng Yap,Xiaoli Li,Philip S. Yu",,2012,,,
183,Rectified Linear Units Improve Restricted Boltzmann Machines,"Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these ""Stepped Sigmoid Units"" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.","Vinod Nair,Geoffrey E. Hinton",,2010,,,
184,Greedy function approximation: A gradient boosting machine.,"Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent boosting paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such TreeBoost models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.","Jerome H. Friedman,Jerome H. Friedman",Annals of Statistics,2001,,,
185,Very Deep Convolutional Networks for Large-Scale Image Recognition,"In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.","Karen Simonyan,Karen Simonyan,Andrew Zisserman",,2014,,,
186,On diversity and accuracy of homogeneous and heterogeneous ensembles,"The ensemble learning approach has been increasingly used in data mining for improving performance. However, the gain on the learning performance appears varying considerably from application to application. In some cases there were little or no gains achieved even when the same ensemble paradigms were used. This means that there are still some problems in understanding some basic and fundamental issues in ensemble methodology, especially on the factors that can affect the performance of an ensemble and the strategies for constructing effective ensembles. This paper attempts to address these issues. It first describes the possible influencing factors and then focuses on investigating the most important factor - diversity and its relationships with the accuracy of ensemble. In this study, two types of ensembles - homogeneous and heterogeneous ensembles are defined and constructed by using ten different learning algorithms and their diversity and accuracy are evaluated in order to find out which types of ensemble possess high diversity and are thus more accurate. For each of the ten learning algorithms, its ability for generating different types of diversity is estimated quantitatively by using ten common diversity measures and their characteristics are then analyzed to establish their correlation with ensemble performance. The study used fifteen popular data sets to verify the consistence and reliability of our experimental findings.","Shun Bian,Wenjia Wang",,2007,,,
187,Out of control: the rise of neo-biological civilization,"A quest for the basic principals defining artificial evolution (what will be common in the next century). From the former editor of CoEvolution Quarterly/Whole Earth Review, the first phantasmagoric look, not just at the science of spontaneous self-organization, but actually into the phenomenon itself.","Kevin Kelly,Kevin Kelly",,1994,,,
188,Robust median reversion strategy for on-line portfolio selection,"On-line portfolio selection has been attracting increasing interests from artificial intelligence community in recent decades. Mean reversion, as one most frequent pattern in financial markets, plays an important role in some state-of-the-art strategies. Though successful in certain datasets, existing mean reversion strategies do not fully consider noises and outliers in the data, leading to estimation error and thus non-optimal portfolios, which results in poor performance in practice. To overcome the limitation, we propose to exploit the reversion phenomenon by robust L1-median estimator, and design a novel on-line portfolio selection strategy named ""Robust Median Reversion"" (RMR), which makes optimal portfolios based on the improved reversion estimation. Empirical results on various real markets show that RMR can overcome the drawbacks of existing mean reversion algorithms and achieve significantly better results. Finally, RMR runs in linear time, and thus is suitable for large-scale trading applications.","Ding-jiang Huang,Junlong Zhou,Bin Li,Bin Li,Steven C. H. Hoi,Shuigeng Zhou",,2013,,,
189,Trust Region Policy Optimization,"In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.","John Schulman,Sergey Levine,Pieter Abbeel,Michael I. Jordan,Philipp Moritz",,2015,,,
190,Ensemble methods for wind and solar power forecasting—A state-of-the-art review,This paper reviews state-of-the-art on wind speed/power forecasting and solar irradiance forecasting with ensemble methods. The ensemble forecasting methods are grouped into two main categories: competitive ensemble forecasting and cooperative ensemble forecasting. The competitive ensemble forecasting is further categorized based on data diversity and parameter diversity. The cooperative ensemble forecasting is divided according to pre-processing and post-processing. Typical articles are discussed according to each category and their characteristics are highlighted. We also conduct comparisons based on reported results and comparisons based on simulations conducted by us. Suggestions for future research include ensemble of different paradigms and inter-category ensemble methods among others.,"Ye Ren,Ponnuthurai Nagaratnam Suganthan,Narasimalu Srikanth",Renewable & Sustainable Energy Reviews,2015,,,
191,Horizontal and Vertical Ensemble with Deep Representation for Classification.,"Representation learning, especially which by using deep learning, has been widely applied in classication. However, how to use limited size of labeled data to achieve good classication performance with deep neural network, and how can the learned features further improve classication remain indenite. In this paper, we propose Horizontal Voting Vertical Voting and Horizontal Stacked Ensemble methods to improve the classication performance of deep neural networks. In the ICML 2013 Black Box Challenge, via using these methods independently, Bing Xu achieved 3rd in public leaderboard, and 7th in private leaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th in private leaderboard.","Jingjing Xie,Bing Xu,Chuang Zhang,Zhang Chuang",arXiv: Learning,2013,,,
192,"Seasonality and Interconnectivity Within Cryptocurrencies - An Analysis on the Basis of Bitcoin, Litecoin and Namecoin","The market development of cryptocurrencies illustrates an institutional change how payments can be released and received without the need of any intermediary or trusted central party to clear virtual transactions. As academia focuses mostly on Bitcoin, the increased money demand within cryptocurrencies, its linkages, the wide range of possible channels to release and receive executed payments (payment patterns) and the wide range of different underlying motivations why cryptocurrencies are demanded to release payments (payment behavior) is still uncovered. One might assume that payment patterns and payment behavior converges in the future as soon as the experimenting phase would have cooled down. However we observe that Bitcoin shows a strong, Litecoin a weak and Namecoin no weekday seasonality. By analyzing observed number of payments directly (between these cryptocurrencies) and indirectly (via the Bitcoin exchange-rate) we find no relationship. We conclude on these findings that payment patterns and payment behaviors on the basis of cryptocurrencies Bitcoin, Litecoin and Namecoin continue to diverge.","Martin Haferkorn,Josué Manuel Quintana Diaz",,2014,,,
193,How many trees in a random forest,"Random Forest is a computationally efficient technique that can operate quickly over large datasets. It has been used in many recent research projects and real-world applications in diverse domains. However, the associated literature provides almost no directions about how many trees should be used to compose a Random Forest. The research reported here analyzes whether there is an optimal number of trees within a Random Forest, i.e., a threshold from which increasing the number of trees would bring no significant performance gain, and would only increase the computational cost. Our main conclusions are: as the number of trees grows, it does not always mean the performance of the forest is significantly better than previous forests (fewer trees), and doubling the number of trees is worthless. It is also possible to state there is a threshold beyond which there is no significant gain, unless a huge computational environment is available. In addition, it was found an experimental relationship for the AUC gain when doubling the number of trees in any forest. Furthermore, as the number of trees grows, the full set of attributes tend to be used within a Random Forest, which may not be interesting in the biomedical domain. Additionally, datasets' density-based metrics proposed here probably capture some aspects of the VC dimension on decision trees and low-density datasets may require large capacity machines whilst the opposite also seems to be true.","Thais Mayumi Oshiro,Pedro Santoro Perez,José Augusto Baranauskas",,2012,,,
194,Energy-based models for sparse overcomplete representations,"We present a new way of extending independent components analysis (ICA) to overcomplete representations. In contrast to the causal generative extensions of ICA which maintain marginal independence of sources, we define features as deterministic (linear) functions of the inputs. This assumption results in marginal dependencies among the features, but conditional independence of the features given the inputs. By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution. Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002). When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data. In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech, natural images, hand-written digits and faces.","Yee Whye Teh,Max Welling,Simon Osindero,Geoffrey E. Hinton",Journal of Machine Learning Research,2003,,,
195,Distilling the Knowledge in a Neural Network,"A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.","Geoffrey E. Hinton,Oriol Vinyals,Jeffrey Dean",arXiv: Machine Learning,2015,,,
196,An Evaluation of Alternative Equity Indices - Part 2: Fundamental Weighting Schemes,"In this paper we explore an alternative approach for determining constituent weights for equity indices. This approach makes use of alternative definitions of company size, and is referred to as Fundamental Indexation (Arnott et al (2005)). Based upon a data set that comprises the largest 1,000 US stocks for each year in our sample, our results show that between 1968 and 2011 the fundamental index alternatives that we consider have out-performed a comparable index constructed on the basis of the market capitalisation of the index constituents in risk-adjusted terms. Our Monte Carlo experiments show that this superior risk-adjusted performance cannot be attributed easily to luck. We also find that although the superior performance is achieved with higher constituent turnover than required using the Market-cap approach to index construction, the turnover is lower, and in some cases much lower, than required by some of the heuristic and optimised index construction techniques that we explored in our last paper. Finally, we find that although the application of a simple market-timing rule does not enhance the returns on these fundamentally-weighted indices very significantly, it does reduce the volatility of their returns and their maximum drawdown quite considerably.","Steve Thomas,Stephen Thomas,Andrew Clare,Nick Motson",,2013,,,
197,"On Bias, Variance, 0/1—Loss, and the Curse-of-Dimensionality","The classification problem is considered in which an output variable y assumes discrete values with respective probabilities that depend upon the simultaneous values of a set of input variables x = {x_1,....,x_n}. At issue is how error in the estimates of these probabilities affects classification error when the estimates are used in a classification rule. These effects are seen to be somewhat counter intuitive in both their strength and nature. In particular the bias and variance components of the estimation error combine to influence classification in a very different way than with squared error on the probabilities themselves. Certain types of (very high) bias can be canceled by low variance to produce accurate classification. This can dramatically mitigate the effect of the bias associated with some simple estimators like “naive” Bayes, and the bias induced by the curse-of-dimensionality on nearest-neighbor procedures. This helps explain why such simple methods are often competitive with and sometimes superior to more sophisticated ones for classification, and why “bagging/aggregating” classifiers can often improve accuracy. These results also suggest simple modifications to these procedures that can (sometimes dramatically) further improve their classification performance.","Jerome H. Friedman,Jerome H. Friedman",Data Mining and Knowledge Discovery,1997,,,
198,The Loss Surfaces of Multilayer Networks,"We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between largeand small-size networks where for the latter poor quality local minima have nonzero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.","Anna Choromanska,Mikael Henaff,Michael Mathieu,Gérard Ben Arous,Yann LeCun",,2015,,,
199,Coupled matrix factorization within non-IID context,"Recommender systems research has experienced different stages such as from user preference understanding to content analysis. Typical recommendation algorithms were built on the following bases: (1) assuming users and items are IID, namely independent and identically distributed, and (2) focusing on specific aspects such as user preferences or contents. In reality, complex recommendation tasks involve and request (1) personalized outcomes to tailor heterogeneous subjective preferences; and (2) explicit and implicit objective coupling relationships between users, items, and ratings to be considered as intrinsic forces driving preferences. This inevitably involves the non-IID complexity and the need of combining subjective preference with objective couplings hidden in recommendation applications. In this paper, we propose a novel generic coupled matrix factorization (CMF) model by incorporating non-IID coupling relations between users and items. Such couplings integrate the intra-coupled interactions within an attribute and inter-coupled interactions among different attributes. Experimental results on two open data sets demonstrate that the user/item couplings can be effectively applied in RS and CMF outperforms the benchmark methods.","Fangfang Li,Guandong Xu,Longbing Cao",,2015,,,
200,When Semi-supervised Learning Meets Ensemble Learning,Semi-supervised learning and ensemble learning are two important learning paradigms. The former attempts to achieve strong generalization by exploiting unlabeled data, the latter attempts to achieve strong generalization by using multiple learners. In this paper we advocate generating stronger learning systems by leveraging unlabeled data and classifier combination.,Zhi-Hua Zhou,,2009,,
201,Empirical evaluation of gated recurrent neural networks on sequence modeling,"In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.","Junyoung Chung,Caglar Gulcehre,Kyunghyun Cho,Yoshua Bengio",arXiv: Neural and Evolutionary Computing,2014,,,
202,General factorization framework for context-aware recommendations,"Context-aware recommendation algorithms focus on refining recommendations by considering additional information, available to the system. This topic has gained a lot of attention recently. Among others, several factorization methods were proposed to solve the problem, although most of them assume explicit feedback which strongly limits their real-world applicability. While these algorithms apply various loss functions and optimization strategies, the preference modeling under context is less explored due to the lack of tools allowing for easy experimentation with various models. As context dimensions are introduced beyond users and items, the space of possible preference models and the importance of proper modeling largely increases. In this paper we propose a general factorization framework (GFF), a single flexible algorithm that takes the preference model as an input and computes latent feature matrices for the input dimensions. GFF allows us to easily experiment with various linear models on any context-aware recommendation task, be it explicit or implicit feedback based. The scaling properties makes it usable under real life circumstances as well. We demonstrate the framework's potential by exploring various preference models on a 4-dimensional context-aware problem with contexts that are available for almost any real life datasets. We show in our experiments--performed on five real life, implicit feedback datasets--that proper preference modelling significantly increases recommendation accuracy, and previously unused models outperform the traditional ones. Novel models in GFF also outperform state-of-the-art factorization algorithms. We also extend the method to be fully compliant to the Multidimensional Dataspace Model, one of the most extensive data models of context-enriched data. Extended GFF allows the seamless incorporation of information into the factorization framework beyond context, like item metadata, social networks, session information, etc. Preliminary experiments show great potential of this capability.","Balázs Hidasi,Domonkos Tikk",Data Mining and Knowledge Discovery,2016,,,
203,Random k-Labelsets: An Ensemble Method for Multilabel Classification,"This paper proposes an ensemble method for multilabel classification. The RAndom k-labELsets (RAKEL) algorithm constructs each member of the ensemble by considering a small random subset of labels and learning a single-label classifier for the prediction of each element in the powerset of this subset. In this way, the proposed algorithm aims to take into account label correlations using single-label classifiers that are applied on subtasks with manageable number of labels and adequate number of examples per label. Experimental results on common multilabel domains involving protein, document and scene classification show that better performance can be achieved compared to popular multilabel classification approaches.","Grigorios Tsoumakas,Ioannis Vlahavas",,2007,,,
204,A simple testable model of double auction markets,Abstract   We propose a model of price formation in Double Auction markets which employs the strong simplifying assumption that agents neglect strategic feedback effects and regard themselves as playing a Game against Nature. Agents otherwise are strict expected utility maximizers employing Bayesian updating procedures. We prove the optimality of simple (‘aggressive reservation price’) strategies in our general model and propose a parametric form that yields very detailed and computable predictions of market behavior.,Daniel Friedman,Journal of Economic Behavior and Organization,1991,,,
205,ON-LINE PORTFOLIO SELECTION USING MULTIPLICATIVE UPDATES,"We present an on-line investment algorithm that achieves almost the same wealth as the best constant-rebalanced portfolio determined in hindsight from the actual market outcomes. The algorithm employs a multiplicative update rule derived using a framework introduced by Kivinen and Warmuth. Our algorithm is very simple to implement and requires only constant storage and computing time per stock in each trading period. We tested the performance of our algorithm on real stock data from the New York Stock Exchange accumulated during a 22-year period. On these data, our algorithm clearly outperforms the best single stock as well as Cover's universal portfolio selection algorithm. We also present results for the situation in which the investor has access to additional ""side information."" Copyright Blackwell Publishers Inc 1998.","David P. Helmbold,Robert E. Schapire,Yoram Singer,Manfred K. Warmuth",Mathematical Finance,1998,,,
206,Efficient Model Learning Methods for Actor–Critic Control,"We propose two new actor-critic algorithms for reinforcement learning. Both algorithms use local linear regression (LLR) to learn approximations of the functions involved. A crucial feature of the algorithms is that they also learn a process model, and this, in combination with LLR, provides an efficient policy update for faster learning. The first algorithm uses a novel model-based update rule for the actor parameters. The second algorithm does not use an explicit actor but learns a reference model which represents a desired behavior, from which desired control actions can be calculated using the inverse of the learned process model. The two novel methods and a standard actor-critic algorithm are applied to the pendulum swing-up problem, in which the novel methods achieve faster learning than the standard algorithm.","I. Grondman,M. Vaandrager,Lucian Busoniu,Robert Babuska,Erik Schuitema",,2012,,,
207,A stochastic programming model for money management,"Abstract   Portfolio managers in the new fixed-income securities have to cope with various forms of uncertainty, in addition to the usual interest rate changes. Uncertainy in the timing and amount of cashflows, changes in the default and other risk premia and so on, complicate the portfolio manager's problem. We develop here a multi-period, dynamic, portfolio optimization model to address this problem. The model specifies a sequence of investment decisions over time that maximize the expected utility of return at the end of the planning horizon. The model is a two-stage stochastic program with recourse. The dynamics of interest rates, cashflow uncertainty, and liquidity, default and other risk premia, are explicitly modeled through postulated scenarios. Simulation procedures are developed to generate these scenarios. The optimization models are then integrated with the simulation procedures. Extensive validation experiments are carried out to establish the effectiveness of the model in dealing with uncertainty. In particular the model is compared against the popular  portfolio immunization  strategy, and against a portfolio based on  mean-absolute deviation  optimization.","Bennett W. Golub,Martin R. Holmer,Raymond McKendall,Lawrence Pohlman,Stavros A. Zenios",European Journal of Operational Research,1995,,,
208,Web personalization expert with combining collaborative filtering and association rule mining technique,"Abstract   Web personalization has been providing electronic businesses with ways to keep existing customers and to obtain new ones. There are two approaches for providing personalized service: a content-based approach and a collaborative filtering approach. In the content-based approach, it is not easily applied to web objects (pages, images, sounds, etc) which are represented by multimedia data type information. Collaborative filtering approaches have cold-start problem. More serious weakness of collaborative filtering is that rating schemes can only be applied to homogenous domain information. In this paper, we present a framework of personalization expert by combining collaborative filtering method and association rule mining technique to overcome problems that traditional personalized systems have. Since multimedia data type web object cannot be easily analyzed, we adopted a collaborative filtering method that considers each object as an item, and attempts a personalized service. Similar users of each domain object are found as the result of the collaborative filtering method. These similar users’ web object access data is used by apriori algorithm to discover object association rules.","Chi-Hoon Lee,Yeong Hyeon Kim,Young-Myoung Kim,Phill Kyu Rhee",Expert Systems With Applications,2001,,,
209,Stock Price Prediction Using the ARIMA Model,Stock price prediction is an important topic in finance and economics which has spurred the interest of researchers over the years to develop better predictive models. The autoregressive integrated moving average (ARIMA) models have been explored in literature for time series prediction. This paper presents extensive process of building stock price predictive model using the ARIMA model. Published stock data obtained from New York Stock Exchange (NYSE) and Nigeria Stock Exchange (NSE) are used with stock price predictive model developed. Results obtained revealed that the ARIMA model has a strong potential for short-term prediction and can compete favourably with existing techniques for stock price prediction.,"Adebiyi A. Ariyo,Adewumi O. Adewumi,Charles K. Ayo",,2014,,,
210,A hybrid stock selection model using genetic algorithms and support vector regression,"In the areas of investment research and applications, feasible quantitative models include methodologies stemming from soft computing for prediction of financial time series, multi-objective optimization of investment return and risk reduction, as well as selection of investment instruments for portfolio management based on asset ranking using a variety of input variables and historical data, etc. Among all these, stock selection has long been identified as a challenging and important task. This line of research is highly contingent upon reliable stock ranking for successful portfolio construction. Recent advances in machine learning and data mining are leading to significant opportunities to solve these problems more effectively. In this study, we aim at developing a methodology for effective stock selection using support vector regression (SVR) as well as genetic algorithms (GAs). We first employ the SVR method to generate surrogates for actual stock returns that in turn serve to provide reliable rankings of stocks. Top-ranked stocks can thus be selected to form a portfolio. On top of this model, the GA is employed for the optimization of model parameters, and feature selection to acquire optimal subsets of input variables to the SVR model. We will show that the investment returns provided by our proposed methodology significantly outperform the benchmark. Based upon these promising results, we expect this hybrid GA-SVR methodology to advance the research in soft computing for finance and provide an effective solution to stock selection in practice.",Chien-Feng Huang,Applied Soft Computing,2012,,,
211,Reinforcement learning utilizes proxemics: An avatar learns to manipulate the position of people in immersive virtual reality,"A reinforcement learning (RL) method was used to train a virtual character to move participants to a specified location. The virtual environment depicted an alleyway displayed through a wide field-of-view head-tracked stereo head-mounted display. Based on proxemics theory, we predicted that when the character approached within a personal or intimate distance to the participants, they would be inclined to move backwards out of the way. We carried out a between-groups experiment with 30 female participants, with 10 assigned arbitrarily to each of the following three groups: In the Intimate condition the character could approach within 0.38m and in the Social condition no nearer than 1.2m. In the Random condition the actions of the virtual character were chosen randomly from among the same set as in the RL method, and the virtual character could approach within 0.38m. The experiment continued in each case until the participant either reached the target or 7 minutes had elapsed. The distributions of the times taken to reach the target showed significant differences between the three groups, with 9 out of 10 in the Intimate condition reaching the target significantly faster than the 6 out of 10 who reached the target in the Social condition. Only 1 out of 10 in the Random condition reached the target. The experiment is an example of applied presence theory: we rely on the many findings that people tend to respond realistically in immersive virtual environments, and use this to get people to achieve a task of which they had been unaware. This method opens up the door for many such applications where the virtual environment adapts to the responses of the human participants with the aim of achieving particular goals.","Iason Kastanis,Mel Slater",,2012,,,
212,Intelligent trading of seasonal effects: A decision support algorithm based on reinforcement learning,"Abstract   Seasonalities and empirical regularities on financial markets have been well documented in the literature for three decades. While one should suppose that documenting an arbitrage opportunity makes it vanish there are several regularities that have persisted over the years. These include, for example, upward biases at the turn-of-the-month, during exchange holidays and the pre-FOMC announcement drift. Trading regularities is already in and of itself an interesting strategy. However, unfiltered trading leads to potential large drawdowns. In the paper we present a decision support algorithm which uses the powerful ideas of reinforcement learning in order to improve the economic benefits of the basic seasonality strategy. We document the performance on two major stock indices.","Dennis Eilers,Christian L. Dunis,Christian L. Dunis,Hans-Jörg von Mettenheim,Michael H. Breitner",,2014,,,
213,On Estimating Regression,A study is made of certain properties of an approximation to the regression line on the basis of sampling data when the sample size increases unboundedly.,E. A. Nadaraya,Theory of Probability and Its Applications,1964,,,
214,Facial Expression Recognition via a Boosted Deep Belief Network,"A training process for facial expression recognition is usually performed sequentially in three individual stages: feature learning, feature selection, and classifier construction. Extensive empirical studies are needed to search for an optimal combination of feature representation, feature set, and classifier to achieve good recognition performance. This paper presents a novel Boosted Deep Belief Network (BDBN) for performing the three training stages iteratively in a unified loopy framework. Through the proposed BDBN framework, a set of features, which is effective to characterize expression-related facial appearance/shape changes, can be learned and selected to form a boosted strong classifier in a statistical way. As learning continues, the strong classifier is improved iteratively and more importantly, the discriminative capabilities of selected features are strengthened as well according to their relative importance to the strong classifier via a joint fine-tune process in the BDBN framework. Extensive experiments on two public databases showed that the BDBN framework yielded dramatic improvements in facial expression analysis.","Ping Liu,Shizhong Han,Zibo Meng,Yan Tong",,2014,,,
215,Multi-Agent Reinforcement Learning: A Survey,"Multi-agent systems are rapidly finding applications in a variety of domains, including robotics, distributed control, telecommunications, economics. Many tasks arising in these domains require that the agents learn behaviors online. A significant part of the research on multi-agent learning concerns reinforcement learning techniques. However, due to different viewpoints on central issues, such as the formal statement of the learning goal, a large number of different methods and approaches have been introduced. In this paper we aim to present an integrated survey of the field. First, the issue of the multi-agent learning goal is discussed, after which a representative selection of algorithms is reviewed. Finally, open issues are identified and future research directions are outlined","Lucian Busoniu,Robert Babuska,B. De Schutter,Bart De Schutter",,2006,,,
216,Boosting the margin: a new explanation for the effectiveness of voting methods,"One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition.","Robert E. Schapire,Yoav Freund,Peter L. Bartlett,Wee Sun Lee",Annals of Statistics,1998,,,
217,Neural network applications in finance: a review and analysis of literature (1990-1996),"Abstract   The field of neural network technology has been extensively studied in the last decade. This has led to considerable research on its use in various scientific applications and to the development of a diverse range of business applications. Consequently, an increasing amount of application efforts have concentrated on their development in the finance sector. In this paper, we investigated the trend of published applications for the period 1990–1996. The literature was examined according to (1) year of publication, (2) application area, (3) problem domain, (4) decision process phase, (5) level of management, (6) level of task interdependence, (7) means of development, (8) corporate/academic interaction in development, (9) technology/statistical technique integration, and (10) comparative study. Implications to neural networks developers/researchers and suggestions on future research are discussed.","Bo K. Wong,Yakup Selvi,Yakup Selvi",Information & Management,1998,,,
218,Reinforcement learning in robotics: A survey,"Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research.","Jens Kober,J. Andrew Bagnell,Jan Peters",The International Journal of Robotics Research,2013,,,
219,Deep stacking networks for information retrieval,"Deep stacking networks (DSN) are a special type of deep model equipped with parallel and scalable learning. We report successful applications of DSN to an information retrieval (IR) task pertaining to relevance prediction for sponsor search after careful regularization methods are incorporated to the previous DSN methods developed for speech and image classification tasks. The DSN-based system significantly outperforms the LambdaRank-based system which represents a recent state-of-the-art for IR in normalized discounted cumulative gain (NDCG) measures, despite the use of mean square error as DSN's training objective. We demonstrate desirable monotonic correlation between NDCG and classification rate in a wide range of IR quality. The weaker correlation and more flat relationship in the high IR-quality region suggest the need for developing new learning objectives and optimization methods.","Li Deng,Xiaodong He,Xiaodong He,Jianfeng Gao",,2013,,,
220,Big Data Opportunities and Challenges: Discussions from Data Analytics Perspectives [Discussion Forum],"""Big Data"" as a term has been among the biggest trends of the last three years, leading to an upsurge of research, as well as industry and government applications. Data is deemed a powerful raw material that can impact multidisciplinary research endeavors as well as government and business performance. The goal of this discussion paper is to share the data analytics opinions and perspectives of the authors relating to the new opportunities and challenges brought forth by the big data movement. The authors bring together diverse perspectives, coming from different geographical locations with different core research expertise and different affiliations and work experiences. The aim of this paper is to evoke discussion rather than to provide a comprehensive survey of big data research.","Zhi-Hua Zhou,Nitesh V. Chawla,Yaochu Jin,Graham J. Williams",IEEE Computational Intelligence Magazine,2014,,,
221,Context-aware music recommendation based on latenttopic sequential patterns,"Contextual factors can greatly influence the users' preferences in listening to music. Although it is hard to capture these factors directly, it is possible to see their effects on the sequence of songs liked by the user in his/her current interaction with the system. In this paper, we present a context-aware music recommender system which infers contextual information based on the most recent sequence of songs liked by the user. Our approach mines the top frequent tags for songs from social tagging Web sites and uses topic modeling to determine a set of latent topics for each song, representing different contexts. Using a database of human-compiled playlists, each playlist is mapped into a sequence of topics and frequent sequential patterns are discovered among these topics. These patterns represent frequent sequences of transitions between the latent topics representing contexts. Given a sequence of songs in a user's current interaction, the discovered patterns are used to predict the next topic in the playlist. The predicted topics are then used to post-filter the initial ranking produced by a traditional recommendation algorithm. Our experimental evaluation suggests that our system can help produce better recommendations in comparison to a conventional recommender system based on collaborative or content-based filtering. Furthermore, the topic modeling approach proposed here is also useful in providing better insight into the underlying reasons for song selection and in applications such as playlist construction and context prediction.","Negar Hariri,Bamshad Mobasher,Robin Burke",,2012,,,
222,Asset price bubbles and crashes with near-zero-intelligence traders,"We examine whether a simple agent-based model can generate asset price bubbles and crashes of the type observed in a series of laboratory asset market experiments beginning with the work of Smith, Suchanek and Williams (1988). We follow the methodology of Gode and Sunder (1993, 1997) and examine the outcomes that obtain when populations of zero-intelligence (ZI) budget constrained, artificial agents are placed in the various laboratory market environments that have given rise to price bubbles. We have to put more structure on the behavior of the ZI-agents in order to address features of the laboratory asset bubble environment. We show that our model of “near-zero-intelligence” traders, operating in the same double auction environments used in several different laboratory studies, generates asset price bubbles and crashes comparable to those observed in laboratory experiments and can also match other, more subtle features of the experimental data.","John Duffy,M. Utku Ünver",Economic Theory,2006,,,
223,Model the complex dependence structures of financial variables by using canonical vine,"Financial variables such as asset returns in the massive market contain various hierarchical and horizontal relationships forming complicated dependence structures. Modeling and mining of these structures is challenging due to their own high structural complexities as well as the stylized facts of the market data. This paper introduces a new canonical vine dependence model to identify the asymmetric and non-linear dependence structures of asset returns without any prior independence assumptions. To simplify the model while maintaining its merit, a partial correlation based method is proposed to optimize the canonical vine. Compared with the original canonical vine, the new model can still maintain the most important dependence but many unimportant nodes are removed to simplify the canonical vine structure. Our model is applied to construct and analyze dependence structures of European stocks as case studies. Its performance is evaluated by measuring portfolio of Value at Risk, a widely used risk management measure. In comparison to a very recent canonical vine model and the 'full' model, our experimental results demonstrate that our model has a much better quality of Value at Risk, providing insightful knowledge for investors to control and reduce the aggregation risk of the portfolio.","Wei Wei,Xuhui Fan,Jinyan Li,Longbing Cao",,2012,,,
224,Robustness of collaborative recommendation based on association rule mining,"Standard memory-based collaborative filtering algorithms, such as k-nearest neighbor, are quite vulnerable to profile injection attacks. Previous work has shown that some model-based techniques are more robust than k-nn. Model abstraction can inhibit certain aspects of an attack, providing an algorithmic approach to minimizing attack effectiveness. In this paper, we examine the robustness of a recommendation algorithm based on the data mining technique of association rule mining. Our results show that the Apriori algorithm offers large improvement in stability and robustness compared to k-nearest neighbor and other model-based techniques we have studied. Furthermore, our results show that Apriori can achieve comparable recommendation accuracy to k-nn.","J. J. Sandvig,Bamshad Mobasher,Robin Burke",,2007,,,
225,Planning and Chaos Theory,"Abstract Some of the ideas about “chaos” emerging in various fields of the natural, social, and applied sciences have major implications for planning. This article looks at chaos (defined as “order without predictability”) and shows how anyone with an ordinary microcomputer can investigate the nature and development of chaos. Chaos theory promises to have profound implications for what planners do and how they do it. It suggests that the world may be both easier and more difficult to understand than we tend to believe, that noisy and untidy cities may not be as dysfunctional as we often assume, and that the need for planning that is incremental and adaptive in nature may be more urgent than we tend to think.",T. J. Cartwright,Journal of The American Planning Association,1991,,,
226,Dynamic dance warping: Using dynamic time warping to compare dance movement performed under different conditions,"Dynamic time warping (DTW) is proposed as a technique to assess the difference between two dance performances in terms of timing and to provide further insight into dancer cognition. The DTW method is validated for use with dance performance motion tracking data by comparing its results with 'ground truth' results obtained from a comparison between videos of two motion tracked performances. The technique was extended to investigate two hypothesised processes that affect movement timing-scaling (a fixed ratio alteration) and lapsing (caused by insertion or deletion of movement material). As an example of the use of the technique, an ensemble contemporary dance work was performed with the motion of one of three dancers captured in two conditions - with no music (NM) and with music (WM) - with one repeat of the two conditions. The application of the DTW-based algorithm demonstrates that lapses explained much of the timing mismatch (9.6 out of 14 seconds), with a small proportion explained by scaling (a ratio of 0.976) consistent with previous research. However, after again performing the dance under NM and WM conditions the DTW technique demonstrated a non-trivial contribution of scaling in explaining time differences across the various combinations of conditions. In these comparisons, scaling cannot be eliminated as a possible underlying factor of timing error, and it may be that correct scaling (aiming for a ratio of 1) must be learned via practice.","Sam Ferguson,Emery Schubert,Catherine J. Stevens",,2014,,,
227,"Semi-Supervised Learning (Chapelle, O. et al., Eds.; 2006) [Book reviews]",This book addresses some theoretical aspects of semisupervised learning (SSL). The book is organized as a collection of different contributions of authors who are experts on this topic. The objectives of this book are to present a large overview of the SSL methods and to classify these methods into four classes that correspond to the first four main parts of the book (this would include generative models, low-density separation methods, graph-based methods, and algorithms). The last two parts are devoted to applications and perspectives of SSL. The book responds to its major objectives and could serve as a basis for an intermediate level graduate course on SSL. It may also serve as a useful self study and reference source for practicing engineers.,"O. Chapelle,Olivier Chapelle,Bernhard Schölkopf,Alexander Zien,A. Zien",IEEE Transactions on Neural Networks,2009
228,Coupled Attribute Similarity Learning on Categorical Data,"Attribute independence has been taken as a major assumption in the limited research that has been conducted on similarity analysis for categorical data, especially unsupervised learning. However, in real-world data sources, attributes are more or less associated with each other in terms of certain coupling relationships. Accordingly, recent works on attribute dependency aggregation have introduced the co-occurrence of attribute values to explore attribute coupling, but they only present a local picture in analyzing categorical data similarity. This is inadequate for deep analysis, and the computational complexity grows exponentially when the data scale increases. This paper proposes an efficient data-driven similarity learning approach that generates a coupled attribute similarity measure for nominal objects with attribute couplings to capture a global picture of attribute similarity. It involves the frequency-based intra-coupled similarity within an attribute and the inter-coupled similarity upon value co-occurrences between attributes, as well as their integration on the object level. In particular, four measures are designed for the inter-coupled similarity to calculate the similarity between two categorical values by considering their relationships with other attributes in terms of power set, universal set, joint set, and intersection set. The theoretical analysis reveals the equivalent accuracy and superior efficiency of the measure based on the intersection set, particularly for large-scale data sets. Intensive experiments of data structure and clustering algorithms incorporating the coupled dissimilarity metric achieve a significant performance improvement on state-of-the-art measures and algorithms on 13 UCI data sets, which is confirmed by the statistical analysis. The experiment results show that the proposed coupled attribute similarity is generic, and can effectively and efficiently capture the intrinsic and global interactions within and between attributes for especially large-scale categorical data sets. In addition, two new coupled categorical clustering algorithms, i.e., CROCK and CLIMBO are proposed, and they both outperform the original ones in terms of clustering quality on UCI data sets and bibliographic data.","Can Wang,Xiangjun Dong,Fei Zhou,Fei Zhou,Fei Zhou,Fei Zhou,Longbing Cao,Chi-Hung Chi",IEEE Transactions on Neural Networks,2015,,,
229,Towards a Generalized Singular Value Decomposition,"We suggest a form for, and give a constructive derivation of, the generalized singular value decomposition of any two matrices having the same number of columns. We outline its desirable characteristics and compare it to an earlier suggestion by Van Loan [SIAM J. Numer. Anal., 13 (1976), pp. 76–83]. The present form largely follows from the work of Van Loan, but is slightly more general and computationally more amenable than that in the paper cited. We also prove a useful extension of a theorem of Stewart [SIAM Rev. 19 (1977), pp. 634–662] on unitary decompositions of submatrices of a unitary matrix.","Christopher C. Paige,Michael A. Saunders",SIAM Journal on Numerical Analysis,1981,,,
230,Learning Hierarchical Representation Model for NextBasket Recommendation,"Next basket recommendation is a crucial task in market basket analysis. Given a user's purchase history, usually a sequence of transaction data, one attempts to build a recommender that can predict the next few items that the user most probably would like. Ideally, a good recommender should be able to explore the sequential behavior (i.e., buying one item leads to buying another next), as well as account for users' general taste (i.e., what items a user is typically interested in) for recommendation. Moreover, these two factors may interact with each other to influence users' next purchase. To tackle the above problems, in this paper, we introduce a novel recommendation approach, namely hierarchical representation model (HRM). HRM can well capture both sequential behavior and users' general taste by involving transaction and user representations in prediction. Meanwhile, the flexibility of applying different aggregation operations, especially nonlinear operations, on representations allows us to model complicated interactions among different factors. Theoretically, we show that our model subsumes several existing methods when choosing proper aggregation operations. Empirically, we demonstrate that our model can consistently outperform the state-of-the-art baselines under different evaluation metrics on real-world transaction data.","Pengfei Wang,Jiafeng Guo,Yanyan Lan,Jun Xu,Shengxian Wan,Xueqi Cheng",,2015,,,
231,A survey of robot learning from demonstration,"We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research.","Brenna D. Argall,Sonia Chernova,Manuela Veloso,Brett Browning",Robotics and Autonomous Systems,2009,,,
232,Heterogeneous trading strategies with adaptive fuzzy Actor–Critic reinforcement learning: A behavioral approach,"The present study addresses the learning mechanism of boundedly rational agents in the dynamic and noisy environment of financial markets. The main objective is the development of a system that ""decodes"" the knowledge-acquisition strategy and the decision-making process of technical analysts called ""chartists"". It advances the literature on heterogeneous learning in speculative markets by introducing a trading system wherein market environment and agent beliefs are represented by fuzzy inference rules. The resulting functionality leads to the derivation of the parameters of the fuzzy rules by means of adaptive training. In technical terms, it expands the literature that has utilized Actor-Critic reinforcement learning and fuzzy systems in agent-based applications, by presenting an adaptive fuzzy reinforcement learning approach that provides with accurate and prompt identification of market turning points and thus higher predictability. The purpose of this paper is to illustrate this concretely through a comparative investigation against other well-established models. The results indicate that with the inclusion of transaction costs, the profitability of the novel system in case of NASDAQ Composite, FTSE100 and NIKKEI255 indices is consistently superior to that of a Recurrent Neural Network, a Markov-switching model and a Buy and Hold strategy. Overall, the proposed system via the reinforcement learning mechanism, the fuzzy rule-based state space modeling and the adaptive action selection policy, leads to superior predictions upon the direction-of-change of the market.",Stelios D. Bekiros,Journal of Economic Dynamics and Control,2010,,,
233,A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting,"In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone?Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.","Yoav Freund,Robert E. Schapire",,1997,,,
234,Playlist prediction via metric embedding,"Digital storage of personal music collections and cloud-based music services (e.g. Pandora, Spotify) have fundamentally changed how music is consumed. In particular, automatically generated playlists have become an important mode of accessing large music collections. The key goal of automated playlist generation is to provide the user with a coherent listening experience. In this paper, we present Latent Markov Embedding (LME), a machine learning algorithm for generating such playlists. In analogy to matrix factorization methods for collaborative filtering, the algorithm does not require songs to be described by features a priori, but it learns a representation from example playlists. We formulate this problem as a regularized maximum-likelihood embedding of Markov chains in Euclidian space, and show how the resulting optimization problem can be solved efficiently. An empirical evaluation shows that the LME is substantially more accurate than adaptations of smoothed n-gram models commonly used in natural language processing.","Shuo Chen,Josh L. Moore,Douglas Turnbull,Thorsten Joachims",,2012,,,
235,Combining Estimates in Regression and Classification,"Abstract We consider the problem of how to combine a collection of general regression fit vectors to obtain a better predictive model. The individual fits may be from subset linear regression, ridge regression, or something more complex like a neural network. We develop a general framework for this problem and examine a cross-validation—based proposal called “model mix” or “stacking” in this context. We also derive combination methods based on the bootstrap and analytic methods and compare them in examples. Finally, we apply these ideas to classification problems where the estimated combination weights can yield insight into the structure of the problem.","Michael LeBlanc,Robert Tibshirani",Journal of the American Statistical Association,1996,,,
236,Stock market trading rule discovery using technical charting heuristics,"Abstract   In this case study in knowledge engineering and data mining, we implement a recognizer for two variations of the ‘bull flag’ technical charting heuristic and use this recognizer to discover trading rules on the NYSE Composite Index. Out-of-sample results indicate that these rules are effective.","William Leigh,William Leigh,Naval Modani,Russell L. Purvis,Tom L. Roberts",Expert Systems With Applications,2002,,,
237,Technical analysis: An asset allocation perspective on the use of moving averages,"In this paper, we analyze the usefulness of technical analysis, specifically the widely employed moving average trading rule from an asset allocation perspective. We show that, when stock returns are predictable, technical analysis adds value to commonly used allocation rules that invest fixed proportions of wealth in stocks. When uncertainty exists about predictability, which is likely in practice, the fixed allocation rules combined with technical analysis can outperform the prior-dependent optimal learning rule when the prior is not too informative. Moreover, the technical trading rules are robust to model specification, and they tend to substantially outperform the model-based optimal trading strategies when the model governing the stock price is uncertain.","Yingzi Zhu,Guofu Zhou,Guofu Zhou",Journal of Financial Economics,2009,,,
238,Transfer learning in heterogeneous collaborative filtering domains,"A major challenge for collaborative filtering (CF) techniques in recommender systems is the data sparsity that is caused by missing and noisy ratings. This problem is even more serious for CF domains where the ratings are expressed numerically, e.g. as 5-star grades. We assume the 5-star ratings are unordered bins instead of ordinal relative preferences. We observe that, while we may lack the information in numerical ratings, we sometimes have additional auxiliary data in the form of binary ratings. This is especially true given that users can easily express themselves with their preferences expressed as likes or dislikes for items. In this paper, we explore how to use these binary auxiliary preference data to help reduce the impact of data sparsity for CF domains expressed in numerical ratings. We solve this problem by transferring the rating knowledge from some auxiliary data source in binary form (that is, likes or dislikes), to a target numerical rating matrix. In particular, our solution is to model both the numerical ratings and ratings expressed as like or dislike in a principled way. We present a novel framework of Transfer by Collective Factorization (TCF), in which we construct a shared latent space collectively and learn the data-dependent effect separately. A major advantage of the TCF approach over the previous bilinear method of collective matrix factorization is that we are able to capture the data-dependent effect when sharing the data-independent knowledge. This allows us to increase the overall quality of knowledge transfer. We present extensive experimental results to demonstrate the effectiveness of TCF at various sparsity levels, and show improvements of our approach as compared to several state-of-the-art methods.","Weike Pan,Qiang Yang",Artificial Intelligence,2013,,,
239,Random features for Kernel Deep Convex Network,"The recently developed deep learning architecture, a kernel version of the deep convex network (K-DCN), is improved to address the scalability problem when the training and testing samples become very large. We have developed a solution based on the use of random Fourier features, which possess the strong theoretical property of approximating the Gaussian kernel while rendering efficient computation in both training and evaluation of the K-DCN with large training samples. We empirically demonstrate that just like the conventional K-DCN exploiting rigorous Gaussian kernels, the use of random Fourier features also enables successful stacking of kernel modules to form a deep architecture. Our evaluation experiments on phone recognition and speech understanding tasks both show the computational efficiency of the K-DCN which makes use of random features. With sufficient depth in the K-DCN, the phone recognition accuracy and slot-filling accuracy are shown to be comparable or slightly higher than the K-DCN with Gaussian kernels while significant computational saving has been achieved.","Po-Sen Huang,Li Deng,Mark Hasegawa-Johnson,Xiaodong He,Xiaodong He",,2013,,,
240,Common risk factors in the returns on stocks and bonds,"This paper identities five common risk factors in the returns on stocks and bonds. There are three stock-market factors: an overall market factor and factors related to firm size and book-to-market equity. There are two bond-market factors. related to maturity and default risks. Stock returns have shared variation due to the stock-market factors, and they are linked to bond returns through shared variation in the bond-market factors. Except for low-grade corporates. the bond-market factors capture the common variation in bond returns. Most important. the five factors seem to explain average returns on stocks and bonds.","Eugene F. Fama,Kenneth R. French,Kenneth R. French",Journal of Financial Economics,1993,,,
241,A Review of Ensemble Methods in Bioinformatics,"Ensemble learning is an intensively studies technique in machine learning and pattern recognition. Recent work in computational biology has seen an increasing use of ensemble learning methods due to their unique advantages in dealing with small sample size, high-dimensionality, and complexity data structures. The aim of this article is two-fold. First, it is to provide a review of the most widely used ensemble learning methods and their application in various bioinformatics problems, including the main topics of gene expression, mass spectrometry-based proteomics, gene-gene interaction identification from genome-wide association studies, and prediction of regulatory elements from DNA and protein sequences. Second, we try to identify and summarize future trends of ensemble methods in bioinformatics. Promising directions such as ensemble of support vector machine, meta-ensemble, and ensemble based feature selection are discussed.","Pengyi Yang,Yee Hwa Yang,Bing Bing Zhou,Albert Y. Zomaya",Current Bioinformatics,2010,,,
242,Learning and generalization characteristics of the random vector functional-link net,Abstract   In this paper we explore and discuss the learning and generalization characteristics of the random vector version of the Functional-link net and compare these with those attainable with the GDR algorithm. This is done for a well-behaved deterministic function and for real-world data. It seems that ‘ overtraining ’ occurs for stochastic mappings. Otherwise there is saturation of training.,"Yoh-Han Pao,Yoh Han Pao,Gwang Hoon Park,Gwang Hoon Park,Gwang Hoon Park,Dejan J. Sobajic",Neurocomputing,1994,,,
243,Functional equations in the theory of dynamic programming III,,Richard E. Bellman,Rendiconti Del Circolo Matematico Di Palermo,1956,,,
244,Scenario generation and stochastic programming models for asset liability management,"In this paper, we develop and test scenario generation methods for asset liability management models. We propose a multi-stage stochastic programming model for a Dutch pension fund. Both randomly sampled event trees and event trees fitting the mean and the covariance of the return distribution are used for generating the coefficients of the stochastic program. In order to investigate the performance of the model and the scenario generation procedures we conduct rolling horizon simulations. The average cost and the risk of the stochastic programming policy are compared to the results of a simple fixed mix model. We compare the average switching behavior of the optimal investment policies. Our results show that the performance of the multi-stage stochastic program could be improved drastically by choosing an appropriate scenario generation method.",Roy Kouwenberg,European Journal of Operational Research,2001,,,
245,Exploiting unlabeled data in ensemble methods,"An adaptive semi-supervised ensemble method, ASSEMBLE, is proposed that constructs classification ensembles based on both labeled and unlabeled data. ASSEMBLE alternates between assigning ""pseudo-classes"" to the unlabeled data using the existing ensemble and constructing the next base classifier using both the labeled and pseudolabeled data. Mathematically, this intuitive algorithm corresponds to maximizing the classification margin in hypothesis space as measured on both the labeled and unlabeled of data. Unlike alternative approaches, ASSEMBLE does not require a semi-supervised learning method for the base classifier. ASSEMBLE can be used in conjunction with any cost-sensitive classification algorithm for both two-class and multi-class problems. ASSEMBLE using decision trees won the NIPS 2001 Unlabeled Data Competition. In addition, strong results on several benchmark datasets using both decision trees and neural networks support the proposed method.","Kristin P. Bennett,Kristin P. Bennett,Kristin P. Bennett,Ayhan Demiriz,Richard Maclin",,2002,,,
246,A canonical decomposition theory for metrics on a finite set,"Abstract   We consider specific additive decompositions  d  =  d  1  + … +  d   n   of metrics, defined on a finite set  X  (where a metric may give distance zero to pairs of distinct points). The simplest building stones are the  slit metrics , associated to splits (i.e., bipartitions) of the given set  X . While an additive decomposition of a Hamming metric into split metrics is in no way unique, we achieve uniqueness by restricting ourselves to  coherent  decompositions, that is, decompositions  d  =  d  1  + … +  d   n   such that for every map  f : X  →  R  with  f ( x ) +  f ( y ) ⩾  d ( x ,  y ) for all  x ,  y  ϵ  X  there exist maps  f  1 , …,  f   n  :  X  →  R  with  f  =  f  1  + … +  f   n   and  f   i  ( x ) +  f   i  ( y ) ⩾  d   i  ( x ,  y ) for all  i  = 1,…,  n  and all  x ,  y  ϵ  X . These coherent decompositions are closely related to a geometric decomposition of the injective hull of the given metric. A metric with a coherent decomposition into a (weighted) sum of split metrics will be called  totally split-decomposable . Tree metrics (and more generally, the sum of two tree metrics) are particular instances of totally split-decomposable metrics. Our main result confirms that every metric admits a coherent decomposition into a totally split-decomposable metric and a  split-prime  residue, where all the split summands and hence the decomposition can be determined in polynomial time, and that a family of splits can occur this way if and only if it does not induce on any four-point subset all three splits with block size two.","Hans-Jürgen Bandelt,Andreas W. M. Dress",Advances in Mathematics,1992,,,
247,Classifier chains for multi-label classification,"The widely known binary relevance method for multi-label classification, which considers each label as an independent binary problem, has often been overlooked in the literature due to the perceived inadequacy of not directly modelling label correlations. Most current methods invest considerable complexity to model interdependencies between labels. This paper shows that binary relevance-based methods have much to offer, and that high predictive performance can be obtained without impeding scalability to large datasets. We exemplify this with a novel classifier chains method that can model label correlations while maintaining acceptable computational complexity. We extend this approach further in an ensemble framework. An extensive empirical evaluation covers a broad range of multi-label datasets with a variety of evaluation metrics. The results illustrate the competitiveness of the chaining method against related and state-of-the-art methods, both in terms of predictive performance and time complexity.","Jesse Read,Bernhard Pfahringer,Geoffrey Holmes,Geoff Holmes,Eibe Frank",Machine Learning,2011,,,
248,Nonnegative Elastic Net and application in index tracking,"This paper deals with the model selection consistency of Nonnegative Elastic Net (proposed by imposing nonnegative constraint to the regression parameters) in general setting where p (the number of predictors), q (the number of predictors with non-zero coefficients in the true linear model) and n (sample size) all go to infinity. We prove that this method has nice property of variable selection consistency under NEIC condition. Comparing with Nonnegative-lasso, Nonnegative Elastic Net can select the true variables even when Nonnegative-lasso cannot. In Empirical Part, this method is applied to the constrained index tracking problem in stock market without short sales, i.e. tracking CSI 300 Index1CSI 300 Index is a capitalization-weighted stock market index designed to replicate the performance of 300 stocks traded in the Shanghai and Shenzhen stock exchanges.1 and SSE 180 Index2SSE 180 Index selects constituents with best representation through scientific and objective method. SSE is a benchmark index reflecting Shanghai market and serving as a performance benchmark for investment and a basis for financial innovation.2 by selecting about 30 stocks. The results indicate that Nonnegative Elastic Net outperforms Nonnegative-lasso in asset selection. A two-step method, Nonnegative Elastic Net combined with OLS produce better results than simple Nonnegative Elastic Net method.","Lan Wu,Yuehan Yang",Applied Mathematics and Computation,2014,,,
249,Integrative data analysis of multi-platform cancer data with a multimodal deep learning approach,"Identification of cancer subtypes plays an important role in revealing useful insights into disease pathogenesis and advancing personalized therapy. The recent development of high-throughput sequencing technologies has enabled the rapid collection of multi-platform genomic data (e.g., gene expression, miRNA expression, and DNA methylation) for the same set of tumor samples. Although numerous integrative clustering approaches have been developed to analyze cancer data, few of them are particularly designed to exploit both deep intrinsic statistical properties of each input modality and complex cross-modality correlations among multi-platform input data. In this paper, we propose a new machine learning model, called multimodal deep belief network (DBN), to cluster cancer patients from multi-platform observation data. In our integrative clustering framework, relationships among inherent features of each single modality are first encoded into multiple layers of hidden variables, and then a joint latent model is employed to fuse common features derived from multiple input modalities. A practical learning algorithm, called contrastive divergence (CD), is applied to infer the parameters of our multimodal DBN model in an unsupervised manner. Tests on two available cancer datasets show that our integrative data analysis approach can effectively extract a unified representation of latent features to capture both intra- and cross-modality correlations, and identify meaningful disease subtypes from multi-platform cancer data. In addition, our approach can identify key genes and miRNAs that may play distinct roles in the pathogenesis of different cancer subtypes. Among those key miRNAs, we found that the expression level of miR-29a is highly correlated with survival time in ovarian cancer patients. These results indicate that our multimodal DBN based data analysis approach may have practical applications in cancer pathogenesis studies and provide useful guidelines for personalized cancer therapy.","Muxuan Liang,Zhizhong Li,Ting Chen,Jianyang Zeng",IEEE/ACM Transactions on Computational Biology and Bioinformatics,2015,,,
250,Optimal control of execution costs,"We derive dynamic optimal trading strategies that minimize the expected cost of trading a large block of equity over a fixed time horizon. Specifically, given a fixed block SM of shares to be executed within a fixed finite number of periods „, and given a price-impact function that yields the execution price of an individual trade as a function of the shares traded and market conditions, we obtain the optimal sequence of trades as a function of market conditions — closed-form expressions in some cases — that minimizes the expected cost of executing SM within „ periods. Our analysis is extended to the portfolio case in which price impact across stocks can have an important e⁄ect on the total cost of trading a portfolio. ( 1998 Elsevier Science B.V. All rights reserved.","Dimitris Bertsimas,Andrew W. Lo",Journal of Financial Markets,1998,,,
251,Generating trading rules on the stock markets with genetic programming,"Technical analysis is aimed at devising trading rules capable of exploiting short-term fluctuations on the financial markets. Recent results indicate that this market timing approach may be a viable alternative to the buy-and-hold approach, where the assets are kept over a relatively long time period. In this paper, we propose genetic programming as a means to automatically generate such short-term trading rules on the stock markets. Rather than using a composite stock index for this purpose, the trading rules are adjusted to individual stocks. Computational results, based on historical pricing and transaction volume data, are reported for 14 Canadian companies listed on the Toronto stock exchange market.","Jean-Yves Potvin,Jean-Yves Potvin,Jean-Yves Potvin,Patrick Soriano,Maxime Vallée",Computers & Operations Research,2004,,,
252,A SURVEY OF CLUSTERING ENSEMBLE ALGORITHMS,"Cluster ensemble has proved to be a good alternative when facing cluster analysis problems. It consists of generating a set of clusterings from the same dataset and combining them into a final clustering. The goal of this combination process is to improve the quality of individual data clusterings. Due to the increasing appearance of new methods, their promising results and the great number of applications, we consider that it is necessary to make a critical analysis of the existing techniques and future projections. This paper presents an overview of clustering ensemble methods that can be very useful for the community of clustering practitioners. The characteristics of several methods are discussed, which may help in the selection of the most appropriate one to solve a problem at hand. We also present a taxonomy of these techniques and illustrate some important applications.","Sandro Vega-Pons,José Ruiz-Shulcloper",International Journal of Pattern Recognition and Artificial Intelligence,2011,,,
253,Continuous-Time Mean-Variance Portfolio Selection: A Stochastic LQ Framework,This  paper is concerned with a continuous-time mean-variance portfolio selection model that is formulated as a bicriteria optimization problem. The objective is to maximize the expected terminal return and minimize the variance of the terminal wealth. By putting weights on the two criteria one obtains a single objective stochastic control problem which is however not in the standard form due to the variance term involved. It is shown that this nonstandard problem can be ``embedded'' into a class of auxiliary stochastic linear-quadratic (LQ) problems. The stochastic LQ control model proves to be an appropriate and effective framework to study the mean-variance problem in light of the recent development on general stochastic LQ problems with indefinite control weighting matrices. This gives rise to the efficient frontier in a closed form for the original portfolio selection problem.,"Xun Yu Zhou,Duan Li",Applied Mathematics and Optimization,2000,,,
254,Irrational Behavior and Economic Theory,,Gary S. Becker,Journal of Political Economy,1962,,,
255,Surveying stock market forecasting techniques - Part II: Soft computing methods,"The key to successful stock market forecasting is achieving best results with minimum required input data. Given stock market model uncertainty, soft computing techniques are viable candidates to capture stock market nonlinear relations returning significant forecasting results with not necessarily prior knowledge of input data statistical distributions. This paper surveys more than 100 related published articles that focus on neural and neuro-fuzzy techniques derived and applied to forecast stock markets. Classifications are made in terms of input data, forecasting methodology, performance evaluation and performance measures used. Through the surveyed papers, it is shown that soft computing techniques are widely accepted to studying and evaluating stock market behavior.","George S. Atsalakis,Kimon P. Valavanis",Expert Systems With Applications,2009,,,
256,Ensemble deep learning for regression and time series forecasting,"In this paper, for the first time, an ensemble of deep learning belief networks (DBN) is proposed for regression and time series forecasting. Another novel contribution is to aggregate the outputs from various DBNs by a support vector regression (SVR) model. We show the advantage of the proposed method on three electricity load demand datasets, one artificial time series dataset and three regression datasets over other benchmark methods.","Xueheng Qiu,Le Zhang,Ye Ren,Ponnuthurai Nagaratnam Suganthan,Gehan A. J. Amaratunga",,2014,,,
257,Collaborative Filtering beyond the User-Item Matrix: A Survey of the State of the Art and Future Challenges,"Over the past two decades, a large amount of research effort has been devoted to developing algorithms that generate recommendations. The resulting research progress has established the importance of the user-item (U-I) matrix, which encodes the individual preferences of users for items in a collection, for recommender systems. The U-I matrix provides the basis for collaborative filtering (CF) techniques, the dominant framework for recommender systems. Currently, new recommendation scenarios are emerging that offer promising new information that goes beyond the U-I matrix. This information can be divided into two categories related to its source: rich side information concerning users and items, and interaction information associated with the interplay of users and items. In this survey, we summarize and analyze recommendation scenarios involving information sources and the CF algorithms that have been recently developed to address them. We provide a comprehensive introduction to a large body of research, more than 200 key references, with the aim of supporting the further development of recommender systems exploiting information beyond the U-I matrix. On the basis of this material, we identify and discuss what we see as the central challenges lying ahead for recommender system technology, both in terms of extensions of existing techniques as well as of the integration of techniques and technologies drawn from other research areas.","Yue Shi,Martha Larson,Alan Hanjalic",ACM Computing Surveys,2014,,,
258,An evolutionary trend reversion model for stock trading rule discovery,"Quantitative investment (QI) is certainly a hot topic in big data analysis. For knowledge discovery in huge, complex and nonlinear stock market data, the eXtended Classifier Systems (XCS) is quite suitable because of the excellent learning and explicit expression abilities derived from its intrinsic techniques that include classification rule mining, evolutionary learning and reinforcement learning. This paper presents an Evolutionary Trend Reversion Model (eTrendRev), which is based on the proposed XCS with learn mode (XCSL) and trend-reversion strategy. The eTrendRev is highlighted in three aspects: (1) the explicit rules generated by XCSL are more understandable than black-box models, such as neural networks, thus can provide justifiable knowledge to guide trading; (2) the original pure explore mode of XCS is substituted by the proposed learn mode, which is shown in this study to perform better and is more stable; (3) a variety of trend-reversion strategies are integrated and made dynamic through evolutionary learning. For model evaluation, experiments were carried out on the historical data of the Shanghai Composite Index and the NASDAQ Composite Index, and back-testing results indicate that eTrendRev can produce higher return with lower risk and recognize significant market turning points in a timely fashion. This study also confirms the profitability of using sole trend-reversion indicators in machine learning-based QI model.","Xiangzhou Zhang,Yong Hu,Yong Hu,Kang Xie,Wei-Guo Zhang,Lijun Su,Mei Liu",Knowledge Based Systems,2015,,,
259,Building Personalized Recommendation System in E-Commerce using Association Rule-Based Mining and Classification,"Due to the convenience of Internet, people can search for whatever information they need and buy whatever they want on the web. In the age of E-Commerce, it is difficult to provide support for customers to find the most valuable products that match their heterogeneous needs. Traditional approaches to this so-called personalization problem adopt predefined formats to describe the customer requirements. This always leads to distortion in eliciting requirement information and thus inaccurate recommendations. In this paper, we propose a personalized recommendation system using association rule mining and classification in e-commerce. Customer requirements are extracted from text documents and transformed into a set of significant phrases. Allowing the transformed transaction records, a set of association rule are mined from database using Apriori algorithm. CBA-CB algorithm is applied to produce the best rules out of the whole set of rules. The best classifiers are then generated after the test and validation of those rules, aimed to predict the item labels for new customer requirements and thus assigns the corresponding class labels to the customer. The system analysis and design of the proposed recommendation system as well as the implementation of prototype are also presented.",Xi-Zheng Zhang,,2007,,,
260,Spectral Ensemble Clustering,"Ensemble clustering, also known as consensus clustering, is emerging as a promising solution for multi-source and/or heterogeneous data clustering. The co-association matrix based method, which redefines the ensemble clustering problem as a classical graph partition problem, is a landmark method in this area. Nevertheless, the relatively high time and space complexity preclude it from real-life large-scale data clustering. We therefore propose SEC, an efficient Spectral Ensemble Clustering method based on co-association matrix. We show that SEC has theoretical equivalence to weighted K-means clustering and results in vastly reduced algorithmic complexity. We then derive the latent consensus function of SEC, which to our best knowledge is among the first to bridge co-association matrix based method to the methods with explicit object functions. The robustness and generalizability of SEC are then investigated to prove the superiority of SEC in theory. We finally extend SEC to meet the challenge rising from incomplete basic partitions, based on which a scheme for big data clustering can be formed. Experimental results on various real-world data sets demonstrate that SEC is an effective and efficient competitor to some state-of-the-art ensemble clustering methods and is also suitable for big data clustering.","Hongfu Liu,Tongliang Liu,Junjie Wu,Dacheng Tao,Yun Fu",,2015,,,
261,Personalized next-song recommendation in online karaokes,"In this paper, we propose Personalized Markov Embedding (PME), a next-song recommendation strategy for online karaoke users. By modeling the sequential singing behavior, we first embed songs and users into a Euclidean space in which distances between songs and users reflect the strength of their relationships. Then, given each user's last song, we can generate personalized recommendations by ranking the candidate songs according to the embedding. Moreover, PME can be trained without any requirement of content information. Finally, we perform an experimental evaluation on a real world data set provided by ihou.com which is an online karaoke website launched by iFLYTEK, and the results clearly demonstrate the effectiveness of PME.","Xiang Wu,Qi Liu,Qi Liu,Qi Liu,Qi Liu,Enhong Chen,Liang He,Liang He,Jingsong Lv,Can Cao,Guoping Hu",,2013,,,
262,Coupling Learning of Complex Interactions,"Abstract   Complex applications such as big data analytics involve different forms of coupling relationships that reflect interactions between factors related to technical, business (domain-specific) and environmental (including socio-cultural and economic) aspects. There are diverse forms of couplings embedded in poor-structured and ill-structured data. Such couplings are ubiquitous, implicit and/or explicit, objective and/or subjective, heterogeneous and/or homogeneous, presenting complexities to existing learning systems in statistics, mathematics and computer sciences, such as typical dependency, association and correlation relationships. Modeling and learning such couplings thus is fundamental but challenging. This paper discusses the concept of coupling learning, focusing on the involvement of coupling relationships in learning systems. Coupling learning has great potential for building a deep understanding of the essence of business problems and handling challenges that have not been addressed well by existing learning theories and tools. This argument is verified by several case studies on coupling learning, including handling coupling in recommender systems, incorporating couplings into coupled clustering, coupling document clustering, coupled recommender algorithms and coupled behavior analysis for groups.",Longbing Cao,Information Processing and Management,2015,,,
263,Online portfolio selection: A survey,"Online portfolio selection is a fundamental problem in computational finance, which has been extensively studied across several research communities, including finance, statistics, artificial intelligence, machine learning, and data mining. This article aims to provide a comprehensive survey and a structural understanding of online portfolio selection techniques published in the literature. From an online machine learning perspective, we first formulate online portfolio selection as a sequential decision problem, and then we survey a variety of state-of-the-art approaches, which are grouped into several major categories, including benchmarks, Follow-the-Winner approaches, Follow-the-Loser approaches, Pattern-Matching--based approaches, and Meta-Learning Algorithms. In addition to the problem formulation and related algorithms, we also discuss the relationship of these algorithms with the capital growth theory so as to better understand the similarities and differences of their underlying trading ideas. This article aims to provide a timely and comprehensive survey for both machine learning and data mining researchers in academia and quantitative portfolio managers in the financial industry to help them understand the state of the art and facilitate their research and practical applications. We also discuss some open issues and evaluate some emerging new trends for future research.","Bin Li,Bin Li,Steven C. H. Hoi",ACM Computing Surveys,2014,,,
264,Strategic sequential bidding in auctions using dynamic programming,"We develop a general framework in which real-time Dynamic Programming (DP) can be used to formulate agent bidding strategies in a broad class of auctions characterized by sequential bidding and continuous clearing. In this framework, states are represented primarily by an agent's holdings, and transition probabilities are estimated from the market event history, along the lines of the ""belief function"" approach of Gjerstad and Dickhaut [7]. We use the belief function, combined with a forecast of how it changes over time, as an approximate state-transition model in the DP formulation. The DP is then solved from scratch each time the agent has an opportunity to bid. The resulting algorithm optimizes cumulative long-term discounted profitability, whereas most previous strategies such as Gjerstad-Dickhaut (GD) merely optimize immediate profits.We test our algorithm in a simplified model of a Continuous Double Auction (CDA). Our results show that the DP-based approach reproduces the behavior of GD for small discount parameter γ, and is clearly superior for large values of γ close to 1. We suggest that this algorithm may offer the best performance of any published CDA bidding strategy. The framework our algorithm provides is extensible and can accommodate many market and research aspects.","Gerald Tesauro,Jonathan Bredin",,2002,,,
265,Sentiment analysis: A combined approach,"Sentiment analysis is an important current research area. This paper combines rule-based classification, supervised learning and machine learning into a new combined method. This method is tested on movie reviews, product reviews and MySpace comments. The results show that a hybrid classification can improve the classification effectiveness in terms of micro- and macro-averaged F1. F1 is a measure that takes both the precision and recall of a classifier’s effectiveness into account. In addition, we propose a semi-automatic, complementary approach in which each classifier can contribute to other classifiers to achieve a good level of effectiveness.","Rudy Prabowo,Mike Thelwall",Journal of Informetrics,2009,,,
266,RecSys Challenge 2015 and the YOOCHOOSE Dataset,"The 2015 ACM Recommender Systems Challenge offered the opportunity to work on a large-scale e-commerce dataset from a big retailer in Europe which is accepting recommender system as a service from YOOCHOOSE. Participants tackled the problem of predicting what items a user intends to purchase, if any, given a click sequence performed during an activity session on the e-commerce website. The challenge ran for seven months and was very successful, attracting 850 teams from 49 countries which submitted a total of 5,437 solutions. The winners of the challenge scored approximately 50% of the maximum score, which we considered as an impressive achievement. In this paper we provide a brief overview of the challenge and its results.","David Ben-Shimon,Alexander Tsikinovsky,Michael Friedmann,Bracha Shapira,Lior Rokach,Johannes Hoerle",,2015,,,
267,Optimization of trading systems and portfolios,"We propose to train trading systems and portfolios by optimizing objective functions that directly measure trading and investment performance. Rather than basing a trading system on forecasts or training via a supervised learning algorithm using labelled trading data, we train our systems using recurrent reinforcement learning algorithms. The objective functions that we consider as evaluation functions for reinforcement learning are profit or wealth, economic utility, the Sharpe ratio, and our proposed Differential Sharpe Ratio. The trading and portfolio management systems require prior decisions as input in order to properly take into account the effects of transactions costs, market impact, and taxes. This temporal dependence on system state requires the use of reinforcement versions of standard recurrent learning algorithms. We present empirical results in controlled experiments that demonstrate the efficacy of some of our methods. We find that maximizing the differential Sharpe ratio yields more consistent results than maximizing profits, and that both methods outperform a trading system based on forecasts that minimize MSE.","John Moody,Lizhong Wu",,1997,,,
268,Functional matrix factorizations for cold-start recommendation,"A key challenge in recommender system research is how to effectively profile new users, a problem generally known as cold-start recommendation. Recently the idea of progressively querying user responses through an initial interview process has been proposed as a useful new user preference elicitation strategy. In this paper, we present functional matrix factorization (fMF), a novel cold-start recommendation method that solves the problem of initial interview construction within the context of learning user and item profiles. Specifically, fMF constructs a decision tree for the initial interview with each node being an interview question, enabling the recommender to query a user adaptively according to her prior responses. More importantly, we associate latent profiles for each node of the tree --- in effect restricting the latent profiles to be a function of possible answers to the interview questions --- which allows the profiles to be gradually refined through the interview process based on user responses. We develop an iterative optimization algorithm that alternates between decision tree construction and latent profiles extraction as well as a regularization scheme that takes into account of the tree structure. Experimental results on three benchmark recommendation data sets demonstrate that the proposed fMF algorithm significantly outperforms existing methods for cold-start recommendation.","Ke Zhou,Ke Zhou,Shuang-Hong Yang,Hongyuan Zha",,2011,,,
269,An overview of the rationale for pharmacological strategies in type 2 diabetes: from the evidence to new perspectives.,"Summary  Therapeutic strategies in type 2 diabetic patients should not only integrate both the targets and indications of the different therapies but should be also a compromise between the patient's and physician's goals and willingnesses. The rationale for therapeutic targets is based on recommendations that differ from one country to another. Even though HbA 1c  remains the ""gold standard"", monitoring of blood glucose at fasting and postprandial time-points is a complementary tool for estimating both the quality and safety of diabetic control. Despite the lack of available strong evidence-based data it seems that achieving glucose levels  1c  and plasma glucose levels. The bridge between pathophysiological and clinical rationales can be obtained from the analysis of the relative contributions of fasting and postprandial glucose to the overall hyperglycaemia. In patients with HbA 1c   1c  > 7.3%. As a consequence of these observations, initiation of antidiabetic treatments or implementation of second-line therapies should be aimed at reducing either postprandial excursions or fasting hyperglycaemia according to whether HbA 1c  levels are found respectively below or above a cut-off value of 7.3%.","L. Monnier,M. Benichou,S Charra-Ebrard,C. Boegner,C. Colette,Claude Colette",Diabetes & Metabolism,2005,,,
270,ANFIS: adaptive-network-based fuzzy inference system,"The architecture and learning procedure underlying ANFIS (adaptive-network-based fuzzy inference system) is presented, which is a fuzzy inference system implemented in the framework of adaptive networks. By using a hybrid learning procedure, the proposed ANFIS can construct an input-output mapping based on both human knowledge (in the form of fuzzy if-then rules) and stipulated input-output data pairs. In the simulation, the ANFIS architecture is employed to model nonlinear functions, identify nonlinear components on-line in a control system, and predict a chaotic time series, all yielding remarkable results. Comparisons with artificial neural networks and earlier work on fuzzy modeling are listed and discussed. Other extensions of the proposed ANFIS and promising applications to automatic control and signal processing are also suggested. >","J.-S.R. Jang,J.-S. Jang,Jyh-Shing Roger Jang",,1993,,,
271,Stacked Extreme Learning Machines,"Extreme learning machine (ELM) has recently attracted many researchers’ interest due to its very fast learning speed, good generalization ability, and ease of implementation. It provides a unified solution that can be used directly to solve regression, binary, and multiclass classification problems. In this paper, we propose a stacked ELMs (S-ELMs) that is specially designed for solving large and complex data problems. The S-ELMs divides a single large ELM network into multiple stacked small ELMs which are serially connected. The S-ELMs can approximate a very large ELM network with small memory requirement. To further improve the testing accuracy on big data problems, the ELM autoencoder can be implemented during each iteration of the S-ELMs algorithm. The simulation results show that the S-ELMs even with random hidden nodes can achieve similar testing accuracy to support vector machine (SVM) while having low memory requirements. With the help of ELM autoencoder, the S-ELMs can achieve much better testing accuracy than SVM and slightly better accuracy than deep belief network (DBN) with much faster training speed.","Hongming Zhou,Guang-Bin Huang,Zhiping Lin,Han Wang,Yeng Chai Soh","IEEE Transactions on Systems, Man, and Cybernetics",2015,,,
272,Adaptive agents in a persistent shout double auction,"agents, double auction, electronic trading, negotiation, adaptive behaviour Cliff (1997) has demonstrated that simple, adaptive agents are able to trade in a form of double auction marketplace, in such a way that trade prices converge towards the equilibrium price of the marketplace. However, the marketplace within which the agents trade is unrealistic. In this paper, we consider a more realistic form of double auction market, the persistent shout double auction. We present agents based on the ZIP agents of Cliff (1997), but with an alternative set of heuristics for use within this auction. We demonstrate that the resulting agents achieve equilibrium significantly faster than ZIP agents do, maintain a more stable equilibrium, and are more robust to changes in learning rate.","Chris Preist,Maarten van Tol",,1998,,,
273,Sentiment classification: The contribution of ensemble learning,"With the rapid development of information technologies, user-generated contents can be conveniently posted online. While individuals, businesses, and governments are interested in evaluating the sentiments behind this content, there are no consistent conclusions on which sentiment classification technologies are best. Recent studies suggest that ensemble learning methods may have potential applicability in sentiment classification. In this study, we conduct a comparative assessment of the performance of three popular ensemble methods (Bagging, Boosting, and Random Subspace) based on five base learners (Naive Bayes, Maximum Entropy, Decision Tree, K Nearest Neighbor, and Support Vector Machine) for sentiment classification. Moreover, ten public sentiment analysis datasets were investigated to verify the effectiveness of ensemble learning for sentiment analysis. Based on a total of 1200 comparative group experiments, empirical results reveal that ensemble methods substantially improve the performance of individual base learners for sentiment classification. Among the three ensemble methods, Random Subspace has the better comparative results, although it was seldom discussed in the literature. These results illustrate that ensemble learning methods can be used as a viable method for sentiment classification.","Gang Wang,Gang Wang,Jianshan Sun,Jian Ma,Kaiquan Xu,Jibao Gu",,2014,,,
274,A hybrid stock trading system using genetic network programming and mean conditional value-at-risk,"This paper describes a hybrid stock trading system based on Genetic Network Programming (GNP) and Mean Conditional Value-at-Risk Model (GNP–CVaR). The proposed method, combining the advantages of evolutionary algorithms and statistical model, has provided useful tools to construct portfolios and generate effective stock trading strategies for investors with different risk-attitudes. Simulation results on five stock indices show that model based on GNP and maximum Sharpe Ratio portfolio performs the best in bull market, and that based on GNP and the global minimum risk portfolio performs the best in bear market. The portfolios constructed by Markowitz’s mean–variance model performs the same as mean-CVaR model. It is clarified that the proposed system significantly improves the function and efficiency of original GNP, which can help investors make profitable decisions.","Yan Chen,Xuancheng Wang",European Journal of Operational Research,2015,,,
275,Selective negative correlation learning approach to incremental learning,"Negative correlation learning (NCL) is a successful approach to constructing neural network ensembles. In batch learning mode, NCL outperforms many other ensemble learning approaches. Recently, NCL has also shown to be a potentially powerful approach to incremental learning, while the advantages of NCL have not yet been fully exploited. In this paper, we propose a selective NCL (SNCL) algorithm for incremental learning. Concretely, every time a new training data set is presented, the previously trained neural network ensemble is cloned. Then the cloned ensemble is trained on the new data set. After that, the new ensemble is combined with the previous ensemble and a selection process is applied to prune the whole ensemble to a fixed size. This paper is an extended version of our preliminary paper on SNCL. Compared to the previous work, this paper presents a deeper investigation into SNCL, considering different objective functions for the selection process and comparing SNCL to other NCL-based incremental learning algorithms on two more real world bioinformatics data sets. Experimental results demonstrate the advantage of SNCL. Further, comparisons between SNCL and other existing incremental learning algorithms, such Learn++ and ARTMAP, are also presented.","Ke Tang,Minlong Lin,Fernanda L. Minku,Xin Yao",Neurocomputing,2009,,,
276,Response models based on bagging neural networks,"Identifying customers who are likely to respond to a product offering is an important issue in direct marketing.Response models are typically built from historical purchase data. A popular method of choice, logistic regression, is easy to understand and build, but limited in that the model is linear in parameters. Neural networks are nonlinear and have been found to improve predictive accuracies for a variety of business applications. Neural networks have not always demonstrated clear supremacy over traditional statistics competitors, largely because of over-fitting and instability. Combining multiple networksmay alleviate these problems. A systematic method of combining neural networks is proposed, namely bagging or bootstrap aggregating, whereby overfitted multiple neural networks are trained with bootstrap replicas of the original data set and then averaged. We built response models using a publicly available DMEF data set with three methods: bagging neural networks, single neural networks, and conventional logistic regression. The proposed method not only improved but also stabilized the prediction accuracies over the other two.","Kyoungnam Ha,Sungzoon Cho,Douglas L. MacLachlan",Journal of Interactive Marketing,2005,,,
277,Predicting stock and stock price index movement using Trend Deterministic Data Preparation and machine learning techniques,"Four machine learning algorithms are used for prediction in stock markets.Focus is on data pre-processing to improve the prediction accuracy.Technical indicators are discretised by exploiting the inherent opinion.Prediction accuracy of algorithms increases when discrete data is used. This paper addresses problem of predicting direction of movement of stock and stock price index for Indian stock markets. The study compares four prediction models, Artificial Neural Network (ANN), Support Vector Machine (SVM), random forest and naive-Bayes with two approaches for input to these models. The first approach for input data involves computation of ten technical parameters using stock trading data (open, high, low & close prices) while the second approach focuses on representing these technical parameters as trend deterministic data. Accuracy of each of the prediction models for each of the two input approaches is evaluated. Evaluation is carried out on 10years of historical data from 2003 to 2012 of two stocks namely Reliance Industries and Infosys Ltd. and two stock price indices CNX Nifty and S&P Bombay Stock Exchange (BSE) Sensex. The experimental results suggest that for the first approach of input data where ten technical parameters are represented as continuous values, random forest outperforms other three prediction models on overall performance. Experimental results also show that the performance of all the prediction models improve when these technical parameters are represented as trend deterministic data.","Jigar Patel,Sahil Shah,Sahil Shah,Sahil Shah,Priyank Thakkar,Ketan Kotecha",Expert Systems With Applications,2015,,,
278,Critical limitations of consensus clustering in class discovery,"Consensus clustering (CC) has been adopted for unsupervised class discovery in many genomic studies. It calculates how frequently two samples are grouped together in repeated clustering runs, and uses the resulting pairwise ""consensus rates"" for visual demonstration that clusters exist, for comparing cluster stability, and for estimating the optimal cluster number (K). However, the sensitivity and specificity of CC have not been systemically assessed. Through simulations we find that CC is able to divide randomly generated unimodal data into apparently stable clusters for a range of K, essentially reporting chance partitions of cluster-less data. For data with known structure, the common implementations of CC perform poorly in identifying the true K. These results suggest that CC should be applied and interpreted with caution. We found that a new metric based on CC, the proportion of ambiguously clustered pairs (PAC), infers K equally or more reliably than similar methods in simulated data with known K. Our overall approach involves the use of realistic null distributions based on the observed gene-gene correlation structure in a given study, and the implementation of PAC to more accurately estimate K. We discuss the strength of our approach in the context of other ensemble-based methods.","Yasin Șenbabaoğlu,George Michailidis,George Michailidis,Jun Li",Scientific Reports,2015,,,
279,A flexible neural network-fuzzy mathematical programming algorithm for improvement of oil price estimation and forecasting,"This paper presents a flexible algorithm based on artificial neural network (ANN) and fuzzy regression (FR) to cope with optimum long-term oil price forecasting in noisy, uncertain, and complex environments. The oil supply, crude oil distillation capacity, oil consumption of non-OECD, USA refinery capacity, and surplus capacity are incorporated as the economic indicators. Analysis of variance (ANOVA) and Duncan's multiple range test (DMRT) are then applied to test the significance of the forecasts obtained from ANN and FR models. It is concluded that the selected ANN models considerably outperform the FR models in terms of mean absolute percentage error (MAPE). Moreover, Spearman correlation test is applied for verification and validation of the results. The proposed flexible ANN-FR algorithm may be easily modified to be applied to other complex, non-linear and uncertain datasets.","Ali Azadeh,Mohsen Ebrahimi Moghaddam,Mehdi Khakzad,V. Ebrahimipour",Computers & Industrial Engineering,2012,,,
280,CAPITAL ASSET PRICES: A THEORY OF MARKET EQUILIBRIUM UNDER CONDITIONS OF RISK*,"One of the problems which has plagued thouse attempting to predict the behavior of capital marcets is the absence of a body of positive of microeconomic theory dealing with conditions of risk/ Althuogh many usefull insights can be obtaine from the traditional model of investment under conditions of certainty, the pervasive influense of risk in finansial transactions has forced those working in this area to adobt models of price behavior which are little more than assertions. A typical classroom explanation of the determinationof capital asset prices, for example, usually begins with a carefull and relatively rigorous description of the process through which individuals preferences and phisical relationship to determine an equilibrium pure interest rate. This is generally followed by the assertion that somehow a market risk-premium is also determined, with the prices of asset adjusting accordingly to account for differences of their risk.",William F. Sharpe,Journal of Finance,1964,,,
281,CORN: Correlation-driven nonparametric learning approach for portfolio selection,"Machine learning techniques have been adopted to select portfolios from financial markets in some emerging intelligent business applications. In this article, we propose a novel learning-to-trade algorithm termed CORrelation-driven Nonparametric learning strategy (CORN) for actively trading stocks. CORN effectively exploits statistical relations between stock market windows via a nonparametric learning approach. We evaluate the empirical performance of our algorithm extensively on several large historical and latest real stock markets, and show that it can easily beat both the market index and the best stock in the market substantially (without or with small transaction costs), and also surpass a variety of state-of-the-art techniques significantly.","Bin Li,Bin Li,Steven C. H. Hoi,Vivekanand Gopalkrishnan",ACM Transactions on Intelligent Systems and Technology,2011,,,
282,Techniques and applications for sentiment analysis,The main applications and challenges of one of the hottest research areas in computer science.,Ronen Feldman,Communications of The ACM,2013,,,
283,Forecasting stock market movement direction with support vector machine,"Support vector machine (SVM) is a very specific type of learning algorithms characterized by the capacity control of the decision function, the use of the kernel functions and the sparsity of the solution. In this paper, we investigate the predictability of financial movement direction with SVM by forecasting the weekly movement direction of NIKKEI 225 index. To evaluate the forecasting ability of SVM, we compare its performance with those of Linear Discriminant Analysis, Quadratic Discriminant Analysis and Elman Backpropagation Neural Networks. The experiment results show that SVM outperforms the other classification methods. Further, we propose a combining model by integrating SVM with the other classification methods. The combining model performs best among all the forecasting methods.","Wei Huang,Yoshiteru Nakamori,Shouyang Wang",Computers & Operations Research,2005,,,
284,Motivated reinforcement learning for non-player characters in persistent computer game worlds,"Massively multiplayer online computer games are played in complex, persistent virtual worlds. Over time, the landscape of these worlds evolves and changes as players create and personalise their own virtual property. In contrast, many non-player characters that populate virtual game worlds possess a fixed set of pre-programmed behaviours and lack the ability to adapt and evolve in time with their surroundings. This paper presents motivated reinforcement learning agents as a means of creating non-player characters that can both evolve and adapt. Motivated reinforcement learning agents explore their environment and learn new behaviours in response to interesting experiences, allowing them to display progressively evolving behavioural patterns. In dynamic worlds, environmental changes provide an additional source of interesting experiences triggering further learning and allowing the agents to adapt their existing behavioural patterns in time with their surroundings.","Kathryn E. Merrick,Mary Lou Maher",,2006,,,
285,"Pricing and hedging derivative securities with neural networks: Bayesian regularization, early stopping, and bagging","We study the effectiveness of cross validation, Bayesian regularization, early stopping, and bagging to mitigate overfitting and improving generalization for pricing and hedging derivative securities with daily S&P 500 index daily call options from January 1988 to December 1993. Our results indicate that Bayesian regularization can generate significantly smaller pricing and delta-hedging errors than the baseline neural-network (NN) model and the Black-Scholes model for some years. While early stopping does not affect the pricing errors, it significantly reduces the hedging error (HE) in four of the six years we investigated. Although computationally most demanding, bagging seems to provide the most accurate pricing and delta hedging. Furthermore, the standard deviation of the MSPE of bagging is far less than that of the baseline model in all six years, and the standard deviation of the average HE of bagging is far less than that of the baseline model in five out of six years. We conclude that they be used at least in cases when no appropriate hints are available.","Ramazan Gençay,Min Qi",IEEE Transactions on Neural Networks,2001,,,
286,Order characteristics and stock price evolution An application to program trading,"Abstract   This paper is an econometric analysis of the information content of automated orders arriving at the NYSE. The model captures the joint behavior of automated orders and also the return on the stock index future and the futures-spot basis. The results indicate that orders contain information useful in predicting stock returns beyond the information contained in the reported trades. Furthermore, program and index-arbitrage orders contain information beyond that available from the futures return and basis, suggesting that these orders are not merely passive conveyors of common-factor information. Nonprogram, program, and index-arbitrage orders have roughly similar price impacts.",Joel Hasbrouck,Journal of Financial Economics,1996,,,
287,Deep Collaborative Filtering via Marginalized Denoising Auto-encoder,"Collaborative filtering (CF) has been widely employed within recommender systems to solve many real-world problems. Learning effective latent factors plays the most important role in collaborative filtering. Traditional CF methods based upon matrix factorization techniques learn the latent factors from the user-item ratings and suffer from the cold start problem as well as the sparsity problem. Some improved CF methods enrich the priors on the latent factors by incorporating side information as regularization. However, the learned latent factors may not be very effective due to the sparse nature of the ratings and the side information. To tackle this problem, we learn effective latent representations via deep learning. Deep learning models have emerged as very appealing in learning effective representations in many applications. In particular, we propose a general deep architecture for CF by integrating matrix factorization with deep feature learning. We provide a natural instantiations of our architecture by combining probabilistic matrix factorization with marginalized denoising stacked auto-encoders. The combined framework leads to a parsimonious fit over the latent features as indicated by its improved performance in comparison to prior state-of-art models over four large datasets for the tasks of movie/book recommendation and response prediction.","Sheng Li,Jaya Kawale,Yun Fu",,2015,,,
288,Improved short-term load forecasting using bagged neural networks,"Abstract   In this paper we present improved short-term load forecasting using bagged neural networks (BNNs). The BNNs consist of creating multiple sets of data by sampling randomly with replacement, training a neural network on each data set, and averaging the results obtained from each trained neural network. The bagging process reduces estimation errors and variation range of errors compared to using a single neural network for load forecasting. Examples with real data show the effectiveness of our proposed techniques by demonstrating that using BNNs can reduce load forecasting errors, compared to various existing techniques.","Ahmed Shaharyar Khwaja,Muhammad Naeem,Muhammad Naeem,Muhammad Naeem,Alagan Anpalagan,A. Venetsanopoulos,Anastasios N. Venetsanopoulos,Bala Venkatesh",Electric Power Systems Research,2015,,,
289,The perceptron: a probabilistic model for information storage and organization in the brain.,,"F. Rosenblatt,Frank Rosenblatt",Psychological Review,1958,,,
290,Speculative bubbles in Bitcoin markets? An empirical investigation into the fundamental value of Bitcoin,"Amid its rapidly increasing usage and immense public interest the subject of Bitcoin has raised profound economic and societal issues. In this paper we undertake economic and econometric modelling of Bitcoin prices. As with many asset classes we show that Bitcoin exhibits speculative bubbles. Further, we find empirical evidence that the fundamental price of Bitcoin is zero.","Eng-Tuck Cheah,John Fry,John Fry",Economics Letters,2015,,,
291,Generalized Autoencoder: A Neural Network Framework for Dimensionality Reduction,"The autoencoder algorithm and its deep version as traditional dimensionality reduction methods have achieved great success via the powerful representability of neural networks. However, they just use each instance to reconstruct itself and ignore to explicitly model the data relation so as to discover the underlying effective manifold structure. In this paper, we propose a dimensionality reduction method by manifold learning, which iteratively explores data relation and use the relation to pursue the manifold structure. The method is realized by a so called ""generalized autoencoder"" (GAE), which extends the traditional autoencoder in two aspects: (1) each instance xi is used to reconstruct a set of instances {xj} rather than itself. (2) The reconstruction error of each instance (| |xj--x'i| |2) is weighted by a relational function of xi and xj defined on the learned manifold. Hence, the GAE captures the structure of the data space through minimizing the weighted distances between reconstructed instances and the original ones. The generalized autoencoder provides a general neural network framework for dimensionality reduction. In addition, we propose a multilayer architecture of the generalized autoencoder called deep generalized autoencoder to handle highly complex datasets. Finally, to evaluate the proposed methods, we perform extensive experiments on three datasets. The experiments demonstrate that the proposed methods achieve promising performance.","Wei Wang,Wei Wang,Wei Wang,Yan Huang,Yan Huang,Yizhou Yu,Yizhou Wang,Liang Wang,Liang Wang,Liang Wang",,2014,,,
292,Factored MDPs for detecting topics of user sessions,Recommender systems aim to capture interests of users to provide tailored recommendations. User interests are however often unique and depend on many unobservable factors including a user's mood and the local weather. We take a contextual session-based approach and propose a sequential framework using factored Markov decision processes (fMDPs) to detect the user's goal (the topic) of a session. We show that an independence assumption on the attributes of items leads to a set of independent models that can be optimised efficiently. Our approach results in interpretable topics that can be effectively turned into recommendations. Empirical results on a real world click log from a large e-commerce company exhibit highly accurate topic prediction rates of about 90%. Translating our approach into a topic-driven recommender system outperforms several baseline competitors.,"Maryam Tavakol,Ulf Brefeld",,2014,,,
293,Ensemble Algorithms in Reinforcement Learning,"This paper describes several ensemble methods that combine multiple different reinforcement learning (RL) algorithms in a single agent. The aim is to enhance learning speed and final performance by combining the chosen actions or action probabilities of different RL algorithms. We designed and implemented four different ensemble methods combining the following five different RL algorithms: Q-learning, Sarsa, actor-critic (AC), QV-learning, and AC learning automaton. The intuitively designed ensemble methods, namely, majority voting (MV), rank voting, Boltzmann multiplication (BM), and Boltzmann addition, combine the policies derived from the value functions of the different RL algorithms, in contrast to previous work where ensemble methods have been used in RL for representing and learning a single value function. We show experiments on five maze problems of varying complexity; the first problem is simple, but the other four maze tasks are of a dynamic or partially observable nature. The results indicate that the BM and MV ensembles significantly outperform the single RL algorithms.","Marco Wiering,H. van Hasselt",,2008,,,
294,Robust Estimation of a Location Parameter,"This paper contains a new approach toward a theory of robust estimation; it treats in detail the asymptotic theory of estimating a location parameter for contaminated normal distributions, and exhibits estimators—intermediaries between sample mean and sample median—that are asymptotically most robust (in a sense to be specified) among all translation invariant estimators. For the general background, see Tukey (1960) (p. 448 ff.)",Peter J. Huber,Annals of Mathematical Statistics,1964,,,
295,A Survey of Actor-Critic Reinforcement Learning: Standard and Natural Policy Gradients,"Policy-gradient-based actor-critic algorithms are amongst the most popular algorithms in the reinforcement learning framework. Their advantage of being able to search for optimal policies using low-variance gradient estimates has made them useful in several real-life applications, such as robotics, power control, and finance. Although general surveys on reinforcement learning techniques already exist, no survey is specifically dedicated to actor-critic algorithms in particular. This paper, therefore, describes the state of the art of actor-critic algorithms, with a focus on methods that can work in an online setting and use function approximation in order to deal with continuous state and action spaces. After starting with a discussion on the concepts of reinforcement learning and the origins of actor-critic algorithms, this paper describes the workings of the natural gradient, which has made its way into many actor-critic algorithms over the past few years. A review of several standard and natural actor-critic algorithms is given, and the paper concludes with an overview of application areas and a discussion on open issues.","I. Grondman,Lucian Busoniu,Gabriel A. D. Lopes,Robert Babuska",,2012,,,
296,On the existence of optimal relaxed controls of stochastic partial differential equations,"This paper is concerned with control problem of systems governed by stochastic partial differential equations, the drift and diffusion terms of which are second- and first-order differential operators, respectively. The existence of an optimal relaxed control is studied in both cases where the systems are degenerate and nondegenerate. It is shown that the higher regularity conditions on the initial state, as required in the existing results, can be dispensed with if the Wiener process is one-dimensional. Some special cases of multidimensional Wiener process are also discussed, which in particular leads to an improvement of a recent result of Bensoussan and Nisio. The method is based on an analysis of the group generated by the first-order differential operator. As an application, an existence theorem of the optimal relaxed control is proved for partially observed diffusions with correlation between the controlled states and the observation noises.",Xun Yu Zhou,Siam Journal on Control and Optimization,1992,,,
297,Transductive multi-label ensemble classification for protein function prediction,"Advances in biotechnology have made available multitudes of heterogeneous proteomic and genomic data. Integrating these heterogeneous data sources, to automatically infer the function of proteins, is a fundamental challenge in computational biology. Several approaches represent each data source with a kernel (similarity) function. The resulting kernels are then integrated to determine a composite kernel, which is used for developing a function prediction model. Proteins are also found to have multiple roles and functions. As such, several approaches cast the protein function prediction problem within a multi-label learning framework. In our work we develop an approach that takes advantage of several unlabeled proteins, along with multiple data sources and multiple functions of proteins. We develop a graph-based transductive multi-label classifier (TMC) that is evaluated on a composite kernel, and also propose a method for data integration using the ensemble framework, called transductive multi-label ensemble classifier (TMEC). The TMEC approach trains a graph-based multi-label classifier for each individual kernel, and then combines the predictions of the individual models. Our contribution is the use of a bi-relational directed graph that captures relationships between pairs of proteins, between pairs of functions, and between proteins and functions. We evaluate the ability of TMC and TMEC to predict the functions of proteins by using two yeast datasets. We show that our approach performs better than recently proposed protein function prediction methods on composite and multiple kernels.","Guoxian Yu,Carlotta Domeniconi,Huzefa Rangwala,Guoji Zhang,Zhiwen Yu",,2012,,,
298,Handling sequential pattern decay: Developing a two-stage collaborative recommender system,"This study proposes a sequential pattern based collaborative recommender system that predicts the customer's time-variant purchase behavior in an e-commerce environment where the customer's purchase patterns may change gradually. A new two-stage recommendation process is developed to predict customer purchase behavior for the product categories, as well as for product items. The time window weight is introduced to produce sequential patterns closer to the current time period that possess a larger impact on the prediction than patterns relatively far from the current time period. This study is the first to propose time-decaying sequential patterns within a collaborative recommender system. The experimental results show that the proposed system outperforms the traditional collaborative system using a public food mart dataset and a synthetic dataset.","Cheng-Lung Huang,Wei-Liang Huang",Electronic Commerce Research and Applications,2009,,,
299,Collecting user access patterns for building user profiles and collaborative filtering,,Ahmad M. Ahmad Wasfi,,1998,,,
300,Forecasting stock market short-term trends using a neuro-fuzzy based methodology,"A neuro-fuzzy system composed of an Adaptive Neuro Fuzzy Inference System (ANFIS) controller used to control the stock market process model, also identified using an adaptive neuro-fuzzy technique, is derived and evaluated for a variety of stocks. Obtained results challenge the weak form of the Efficient Market Hypothesis (EMH) by demonstrating much improved and better predictions, compared to other approaches, of short-term stock market trends, and in particular the next day's trend of chosen stocks. The ANFIS controller and the stock market process model inputs are chosen based on a comparative study of fifteen different combinations of past stock prices performed to determine the stock market process model inputs that return the best stock trend prediction for the next day in terms of the minimum Root Mean Square Error (RMSE). Gaussian-2 shaped membership functions are chosen over bell shaped Gaussian and triangular ones to fuzzify the system inputs due to the lowest RMSE. Real case studies using data from emerging and well developed stock markets - the Athens and the New York Stock Exchange (NYSE) - to train and evaluate the proposed system illustrate that compared to the ''buy and hold'' strategy and several other reported methods, the proposed approach and the forecasting trade accuracy are by far superior.","George S. Atsalakis,Kimon P. Valavanis",Expert Systems With Applications,2009,,,
301,A hybrid of sequential rules and collaborative filtering for product recommendation,"Customers' purchase behavior may vary over time. Traditional collaborative filtering (CF) methods make recommendations to a target customer based on the purchase behavior of customers whose preferences are similar to those of the target customer; however, the methods do not consider how the customers' purchase behavior may vary over time. In contrast, the sequential rule-based recommendation method analyzes customers' purchase behavior over time to extract sequential rules in the form: purchase behavior in previous [emailprotected]?purchase behavior in the current period. If a target customer's purchase behavior history is similar to the conditional part of the rule, then his/her purchase behavior in the current period is deemed to be the consequent part of the rule. Although the sequential rule method considers the sequence of customers' purchase behavior over time, it does not utilize the target customer's purchase data for the current period. To resolve the above problems, this work proposes a novel hybrid recommendation method that combines the segmentation-based sequential rule method with the segmentation-based KNN-CF method. The proposed method uses customers' RFM (Recency, Frequency, and Monetary) values to cluster customers into groups with similar RFM values. For each group of customers, sequential rules are extracted from the purchase sequences of that group to make recommendations. Meanwhile, the segmentation-based KNN-CF method provides recommendations based on the target customer's purchase data for the current period. Then, the results of the two methods are combined to make final recommendations. Experiment results show that the hybrid method outperforms traditional CF methods.","Duen-Ren Liu,Chin-Hui Lai,Wang-Jung Lee",Information Sciences,2009,,,
302,Multimodal fusion for multimedia analysis: a survey,"This survey aims at providing multimedia researchers with a state-of-the-art overview of fusion strategies, which are used for combining multiple modalities in order to accomplish various multimedia analysis tasks. The existing literature on multimodal fusion research is presented through several classifications based on the fusion methodology and the level of fusion (feature, decision, and hybrid). The fusion methods are described from the perspective of the basic concept, advantages, weaknesses, and their usage in various analysis tasks as reported in the literature. Moreover, several distinctive issues that influence a multimodal fusion process such as, the use of correlation and independence, confidence level, contextual information, synchronization between different modalities, and the optimal modality selection are also highlighted. Finally, we present the open issues for further research in the area of multimodal fusion.","Pradeep K. Atrey,M. Shamim Hossain,M. Anwar Hossain,Abdulmotaleb El Saddik,Mohan S. Kankanhalli",Multimedia Systems,2010,,,
303,Matrix Factorization Techniques for Recommender Systems,"As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels.","Yehuda Koren,Robert M. Bell,Robert M. Bell,R. Bell,Robert M. Bell,Chris Volinsky",IEEE Computer,2009,,,
304,Bayesian equilibrium in double auctions populated by biased heuristic traders,"We use computer simulation to examine three asset markets with imperfect information. In processing imperfect information, traders in the three markets are bayesian, empirical bayesian, and heuristic (representativeness and anchor-and-adjust) respectively. All three converge to the same bayesian equilibrium — although the latter two converge more slowly — without profit maximization, natural selection, arbitrage, or mutual cancellation of random actions. The results support Becker (1962) and Simon (1973) in that the rationality of the market emerges as a consequence of the market structure, and not from the rationality of individuals.","Karim Jamal,Shyam Sunder",Journal of Economic Behavior and Organization,1996,,,
305,Recurrent Deep-Stacking Networks for sequence classification,"Deep Stacking Networks (DSNs) are constructed by stacking shallow feed-forward neural networks on top of each other using concatenated features derived from the lower modules of the DSN and the raw input data. DSNs do not have recurrent connections, making them less effective to model and classify input data with temporal dependencies. In this paper, we embed recurrent connections into the DSN, giving rise to Recurrent Deep Stacking Networks (R-DSNs). Each module of the R-DSN consists of a special form of recurrent neural networks. Generalizing from the earlier DSN, the use of linearity in the output units of the R-DSN enables us to derive a closed form for computing the gradient of the cost function with respect to all network matrices without backpropagating errors. Each module in the R-DSN is initialized with an echo state network, where the input and recurrent weights are fixed to have the echo state property. Then all connection weights within the module are fine tuned using batch-mode gradient descent where the gradient takes an analytical form. Experiments are performed on the TIMIT dataset for frame-level phone state classification with 183 classes. The results show that the R-DSN gives higher classification accuracy over a single recurrent neural network without stacking.","Hamid Palangi,Li Deng,Rabab K. Ward",,2014,,,
306,The Sampling Error in Estimates of Mean‐Variance Efficient Portfolio Weights,"This paper presents an exact finite-sample statistical procedure for testing hypotheses about the weights of mean-variance efficient portfolios. The estimation and inference procedures on efficient portfolio weights are performed in the same way as for the coefficients in an OLS regression. OLS ""t""- and ""F""-statistics can be used for tests on efficient weights, and when returns are multivariate normal, these statistics have exact ""t"" and ""F"" distributions in a finite sample. Using 20 years of data on 11 country stock indexes, we find that the sampling error in estimates of the weights of a global efficient portfolio is large. Copyright The American Finance Association 1999.",Mark Britten-Jones,Journal of Finance,1999,,,
307,Ensemble approaches for regression: A survey,"The goal of ensemble regression is to combine several models in order to improve the prediction accuracy in learning problems with a numerical target variable. The process of ensemble learning can be divided into three phases: the generation phase, the pruning phase, and the integration phase. We discuss different approaches to each of these phases that are able to deal with the regression problem, categorizing them in terms of their relevant characteristics and linking them to contributions from different fields. Furthermore, this work makes it possible to identify interesting areas for future research.","João Mendes-Moreira,Carlos Soares,Alípio Mário Jorge,Jorge Freire de Sousa",ACM Computing Surveys,2012,,,
308,Ensemble learning via negative correlation,"This paper presents a learning approach, i.e. negative correlation learning, for neural network ensembles. Unlike previous learning approaches for neural network ensembles, negative correlation learning attempts to train individual networks in an ensemble and combines them in the same learning process. In negative correlation learning, all the individual networks in the ensemble are trained simultaneously and interactively through the correlation penalty terms in their error functions. Rather than producing unbiased individual networks whose errors are uncorrelated, negative correlation learning can create negatively correlated networks to encourage specialisation and cooperation among the individual networks. Empirical studies have been carried out to show why and how negative correlation learning works. The experimental results show that negative correlation learning can produce neural network ensembles with good generalisation ability.","Yong Liu,Xin Yao",Neural Networks,1999,,,
309,Predicting gene function using hierarchical multi-label decision tree ensembles,"Background
S. cerevisiae, A. thaliana and M. musculus are well-studied organisms in biology and the sequencing of their genomes was completed many years ago. It is still a challenge, however, to develop methods that assign biological functions to the ORFs in these genomes automatically. Different machine learning methods have been proposed to this end, but it remains unclear which method is to be preferred in terms of predictive performance, efficiency and usability.","Leander Schietgat,Celine Vens,Jan Struyf,Hendrik Blockeel,Dragi Kocev,Sašo Džeroski",BMC Bioinformatics,2010,,,
310,"Elliott Wave Theory and neuro-fuzzy systems, in stock market prediction: The WASP system","This paper presents the WASP (Wave Analysis Stock Prediction) system, a system based on the neuro-fuzzy architecture, which utilizes aspects from the Elliott Wave Theory, presented by Ralph Nelson Elliott. This theory has been found to be extremely useful and accurate, particularly in problems of forecasting. A neuro-fuzzy logic technique has been used to forecast the trend of the stock prices and the results derived are very encouraging.","George S. Atsalakis,Emmanouil Dimitrakakis,Constantinos Zopounidis",Expert Systems With Applications,2011,,,
311,Mining frequent patterns without candidate generation,"Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist prolific patterns and/or long patterns.  In this study, we propose a novel frequent pattern tree (FP-tree) structure, which is an extended prefix-tree structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent pattern mining methods.","Jiawei Han,Jian Pei,Yiwen Yin",,2000,,,
312,Latent Context-Aware Recommender Systems,"The emergence of smart mobile devices has given rise to the development of context-aware systems that utilize sensors to collect available data about users. This data, in turn, is used in order to improve various services for the user. The development of such applications is inherently complex, since these applications adapt to changing context information, such as: physical context, computational context, and user tasks. Context information is gathered from a variety of sources that differ in the quality of information they produce and that are often failure-prone. Our study is part of a growing research effort that examines how data collected from mobile devices can be utilized to infer users' behavior and environment. We propose novel approaches that use a rich set of mobile sensors in order to infer unexplored users' contexts in personal models. We also suggest utilizing these high dimensional sensors, which represent users' context for a CARS (context-aware recommender system). For this purpose, we suggest several methods for reducing the dimensionality space by extracting latent contexts from data collected by mobile device sensors. Latent contexts are hidden context patterns, modeled as numeric vectors that are learned for each user automatically, by utilizing unsupervised deep learning techniques on the collected data. We also describe a novel latent context recommendation technique that uses latent contexts and improves the accuracy of state-of-the-art CARS. A preliminary analysis reveals encouraging insights regarding the feasibility of latent contexts and their utilization for context-aware recommendation systems.",Moshe Unger,,2015,,,
313,"Combining multiple feature selection methods for stock prediction: Union, intersection, and multi-intersection approaches","To effectively predict stock price for investors is a very important research problem. In literature, data mining techniques have been applied to stock (market) prediction. Feature selection, a pre-processing step of data mining, aims at filtering out unrepresentative variables from a given dataset for effective prediction. As using different feature selection methods will lead to different features selected and thus affect the prediction performance, the purpose of this paper is to combine multiple feature selection methods to identify more representative variables for better prediction. In particular, three well-known feature selection methods, which are Principal Component Analysis (PCA), Genetic Algorithms (GA) and decision trees (CART), are used. The combination methods to filter out unrepresentative variables are based on union, intersection, and multi-intersection strategies. For the prediction model, the back-propagation neural network is developed. Experimental results show that the intersection between PCA and GA and the multi-intersection of PCA, GA, and CART perform the best, which are of 79% and 78.98% accuracy respectively. In addition, these two combined feature selection methods filter out near 80% unrepresentative features from 85 original variables, resulting in 14 and 17 important features respectively. These variables are the important factors for stock prediction and can be used for future investment decisions.","Chih-Fong Tsai,Yu-Chieh Hsiao",,2010,,,
314,Equilibrium points in n-person games,"One may define a concept of an n -person game in which each player has a finite set of pure strategies and in which a definite set of payments to the n players corresponds to each n -tuple of pure strategies, one strategy being taken for each player. For mixed strategies, which are probability distributions over the pure strategies, the pay-off functions are the expectations of the players, thus becoming polylinear forms …","John F. Nash,Jr. John F. Nash",Proceedings of the National Academy of Sciences of the United States of America,1950,,,
315,Arcing classifier (with discussion and a rejoinder by the author),"Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. One of the more effective is bagging. Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire propose an algorithm the basis of which is to adaptively resample and combine (hence the acronym arcing) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting. Arcing is more successful than bagging in test set error reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works. We introduce the definitions of bias and variance for a classifier as components of the test set error. Unstable classifiers can have low bias on a large range of data sets. Their problem is high variance. Combining multiple versions either through bagging or arcing reduces variance significantly.",Leo Breiman,Annals of Statistics,1998,,,
316,The impact of social and conventional media on firm equity value: A sentiment analysis approach,"This study aims to investigate the effect of social media and conventional media, their relative importance, and their interrelatedness on short term firm stock market performances. We use a novel and large-scale dataset that features daily media content across various conventional media and social media outlets for 824 public traded firms across 6 industries. Social media outlets include blogs, forums, and Twitter. Conventional media includes major newspapers, television broadcasting companies, and business magazines. We apply the advanced sentiment analysis technique that goes beyond the number of mentions (counts) to analyze the overall sentiment of each media resource toward a specific company on the daily basis. We use stock return and risk as the indicators of companies' short-term performances. Our findings suggest that overall social media has a stronger relationship with firm stock performance than conventional media while social and conventional media have a strong interaction effect on stock performance. More interestingly, we find that the impact of different types of social media varies significantly. Different types of social media also interrelate with conventional media to influence stock movement in various directions and degrees. Our study is among the first to examine the effect of multiple sources of social media along with the effect of conventional media and to investigate their relative importance and their interrelatedness. Our findings suggest the importance for firms to differentiate and leverage the unique impact of various sources of media outlets in implementing their social media marketing strategies.","Yang Yu,Wenjing Duan,Qing Cao",,2013,,,
317,HIBAG—HLA genotype imputation with attribute bagging,"Genotyping of classical human leukocyte antigen (HLA) alleles is an essential tool in the analysis of diseases and adverse drug reactions with associations mapping to the major histocompatibility complex (MHC). However, deriving high-resolution HLA types subsequent to whole-genome single-nucleotide polymorphism (SNP) typing or sequencing is often cost prohibitive for large samples. An alternative approach takes advantage of the extended haplotype structure within the MHC to predict HLA alleles using dense SNP genotypes, such as those available from genome-wide SNP panels. Current methods for HLA imputation are difficult to apply or may require the user to have access to large training data sets with SNP and HLA types. We propose HIBAG, HLA Imputation using attribute BAGging, that makes predictions by averaging HLA-type posterior probabilities over an ensemble of classifiers built on bootstrap samples. We assess the performance of HIBAG using our study data (n=2668 subjects of European ancestry) as a training set and HLA data from the British 1958 birth cohort study (n≈1000 subjects) as independent validation samples. Prediction accuracies for HLA-A, B, C, DRB1 and DQB1 range from 92.2% to 98.1% using a set of SNP markers common to the Illumina 1M Duo, OmniQuad, OmniExpress, 660K and 550K platforms. HIBAG performed well compared with the other two leading methods, HLA*IMP and BEAGLE. This method is implemented in a freely available HIBAG R package that includes pre-fit classifiers for European, Asian, Hispanic and African ancestries, providing a readily available imputation approach without the need to have access to large training data sets.","Xiuwen Zheng,Judong Shen,Judong Shen,Judong Shen,Charles J. Cox,Jon Wakefield,Jonathan Wakefield,Margaret G. Ehm,Matthew R. Nelson,Bruce S. Weir",Pharmacogenomics Journal,2014,,,
318,A hybrid genetic-neural architecture for stock indexes forecasting,"In this paper, a new approach for time series forecasting is presented. The forecasting activity results from the interaction of a population of experts, each integrating genetic and neural technologies. An expert of this kind embodies a genetic classifier designed to control the activation of a feedforward artificial neural network for performing a locally scoped forecasting activity. Genetic and neural components are supplied with different information: The former deal with inputs encoding information retrieved from technical analysis, whereas the latter process other relevant inputs, in particular past stock prices. To investigate the performance of the proposed approach in response to real data, a stock market forecasting system has been implemented and tested on two stock market indexes, allowing for account realistic trading commissions. The results pointed to the good forecasting capability of the approach, which repeatedly outperformed the ""Buy and Hold"" strategy.","Giuliano Armano,Michele Marchesi,Eduard Vieta,Andrea Murru",Information Sciences,2005,,,
319,Personalized point-of-interest recommendation by mining users' preference transition,"Location-based social networks (LBSNs) offer researchers rich data to study people's online activities and mobility patterns. One important application of such studies is to provide personalized point-of-interest (POI) recommendations to enhance user experience in LBSNs. Previous solutions directly predict users' preference on locations but fail to provide insights about users' preference transitions among locations. In this work, we propose a novel category-aware POI recommendation model, which exploits the transition patterns of users' preference over location categories to improve location recommendation accuracy. Our approach consists of two stages: (1) preference transition (over location categories) prediction, and (2) category-aware POI recommendation. Matrix factorization is employed to predict a user's preference transitions over categories and then her preference on locations in the corresponding categories. Real data based experiments demonstrate that our approach outperforms the state-of-the-art POI recommendation models by at least 39.75% in terms of recall.","Xin Liu,Xin Liu,Yong Liu,Yong Liu,Karl Aberer,Chunyan Miao",,2013,,,
320,Neighbourhood sampling in bagging for imbalanced data,"Abstract   Various approaches to extend bagging ensembles for class imbalanced data are considered. First, we review known extensions and compare them in a comprehensive experimental study. The results show that integrating bagging with under-sampling is more powerful than over-sampling. They also allow to distinguish Roughly Balanced Bagging as the most accurate extension. Then, we point out that complex and difficult distribution of the minority class can be handled by analyzing the content of a neighbourhood of examples. In our study we show that taking into account such local characteristics of the minority class distribution can be useful both for analyzing performance of ensembles with respect to data difficulty factors and for proposing new generalizations of bagging. We demonstrate it by proposing Neighbourhood Balanced Bagging, where sampling probabilities of examples are modified according to the class distribution in their neighbourhood. Two of its versions are considered: the first one keeping a larger size of bootstrap samples by hybrid over-sampling and the other reducing this size with stronger under-sampling. Experiments prove that the first version is significantly better than existing over-sampling bagging extensions while the other version is competitive to Roughly Balanced Bagging. Finally, we demonstrate that detecting types of minority examples depending on their neighbourhood may help explain why some ensembles work better for imbalanced data than others.","Jerzy Błaszczyński,Jerzy Stefanowski",Neurocomputing,2015,,,
321,Stock trading with cycles,"Research highlights? Reinforcement learning is used to formalize an automated process for determining stock cycles by tuningthe momentum and the average periods. ? The secondary and tertiary trends or short-term wave cycles are eliminated by a smoothing technique. ? The use of reinforcement learning (RL) as a non-arbitrage algorithmic trading system. ? Our study attempts to identify the change of a primary trend or a broad movement. ? Dynamic asset switching based on the detection of peaks and troughs within a portfolio of stock counters. Based on the principles of technical analysis, this paper proposes an artificial intelligence model, which employs the Adaptive Network Fuzzy Inference System (ANFIS) supplemented by the use of reinforcement learning (RL) as a non-arbitrage algorithmic trading system. The novel intelligent trading system is capable of identifying a change in a primary trend for trading and investment decisions. It dynamically determines the periods for momentum and moving averages using the RL paradigm and also appropriately shifting the cycle using ANFIS-RL to address the delay in the predicted cycle. This is used as a proxy to determine the best point in time to go LONG and visa versa for SHORT. When this is coupled with a group of stocks, we derive a simple form of ""riding the cycles - waves"". These are the derived features of the underlying stock movement. It provides a learning framework to trade on cycles. Initial experimental results are encouraging. Firstly, the proposed framework is able to outperform DENFIS and RSPOP in terms of true error and correlation. Secondly, based on the test trading with five US stocks, the proposed trading system is able to beat the market by about 50 percentage points over a period of 13years.","Zhiyong Tan,Chai Quek,Philip Cheng",Expert Systems With Applications,2011,,,
322,Algorithms for Reinforcement Learning,"Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distinguishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the algorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book, we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming.We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.",Csaba Szepesvári,,2010,,,
323,Algorithmic trading review,"The competitive nature of AT, the scarcity of expertise, and the vast profits potential, makes for a secretive community where implementation details are difficult to find.","Philip Treleaven,Michal Galas,Vidhi Lalchand,Vidhi Lalchand",Communications of The ACM,2013,,,
324,Neuro-dynamic trading methods,"Abstract   Investment strategies are usually based on forecasting models, and these are optimized with respect to past predictive performance. However, the main goal of most investors is the optimization of a risk-adjusted performance measure, such as the well-known Sharpe index. This issue has been approached by a few different studies within the area of Neurocomputing. The present paper briefly describes and empirically compares some of the models and methods proposed in those studies. Such adaptive methods can be computationally demanding, and convergence to high-quality solutions can be difficult to achieve, yet they can be very useful in automated trading systems, namely for portfolio management. In particular, the Q-learning algorithm, when combined with neural networks for value function approximation, seems to be a reasonably competitive approach, although not overall superior to alternative ones.","Patrícia Xufre Gonçalves da Silva Casqueiro,António J. L. Rodrigues",European Journal of Operational Research,2006,,,
325,Multi-label ensemble based on variable pairwise constraint projection,"Multi-label classification has attracted an increasing amount of attention in recent years. To this end, many algorithms have been developed to classify multi-label data in an effective manner. However, they usually do not consider the pairwise relations indicated by sample labels, which actually play important roles in multi-label classification. Inspired by this, we naturally extend the traditional pairwise constraints to the multi-label scenario via a flexible thresholding scheme. Moreover, to improve the generalization ability of the classifier, we adopt a boosting-like strategy to construct a multi-label ensemble from a group of base classifiers. To achieve these goals, this paper presents a novel multi-label classification framework named Variable Pairwise Constraint projection for Multi-label Ensemble (VPCME). Specifically, we take advantage of the variable pairwise constraint projection to learn a lower-dimensional data representation, which preserves the correlations between samples and labels. Thereafter, the base classifiers are trained in the new data space. For the boosting-like strategy, we employ both the variable pairwise constraints and the bootstrap steps to diversify the base classifiers. Empirical studies have shown the superiority of the proposed method in comparison with other approaches.","Ping Li,Hong Li,Min Wu,Min Wu",Information Sciences,2013,,,
326,Use of kernel deep convex networks and end-to-end learning for spoken language understanding,"We present our recent and ongoing work on applying deep learning techniques to spoken language understanding (SLU) problems. The previously developed deep convex network (DCN) is extended to its kernel version (K-DCN) where the number of hidden units in each DCN layer approaches infinity using the kernel trick. We report experimental results demonstrating dramatic error reduction achieved by the K-DCN over both the Boosting-based baseline and the DCN on a domain classification task of SLU, especially when a highly correlated set of features extracted from search query click logs are used. Not only can DCN and K-DCN be used as a domain or intent classifier for SLU, they can also be used as local, discriminative feature extractors for the slot filling task of SLU. The interface of K-DCN to slot filling systems via the softmax function is presented. Finally, we outline an end-to-end learning strategy for training the softmax parameters (and potentially all DCN and K-DCN parameters) where the learning objective can take any performance measure (e.g. the F-measure) for the full SLU system.","Li Deng,Gokhan Tur,Xiaodong He,Xiaodong He,Dilek Hakkani-Tur",,2012,,,
327,Neural networks and the bias/variance dilemma,"Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals.","Stuart Geman,Elie Bienenstock,René Doursat",Neural Computation,1992,,,
328,Multimedia classification and event detection using double fusion,"Multimedia Event Detection(MED) is a multimedia retrieval task with the goal of finding videos of a particular event in video archives, given example videos and event descriptions; different from MED, multimedia classification is a task that classifies given videos into specified classes. Both tasks require mining features of example videos to learn the most discriminative features, with best performance resulting from a combination of multiple complementary features. How to combine different features is the focus of this paper. Generally, early fusion and late fusion are two popular combination strategies. The former one fuses features before performing classification and the latter one combines output of classifiers from different features. Early fusion can better capture the relationship among features yet is prone to over-fit the training data. Late fusion deals with the over-fitting problem better but does not allow classifiers to train on all the data at the same time. In this paper, we introduce a fusion scheme named double fusion, which simply combines early fusion and late fusion together to incorporate their advantages. Results are reported on the TRECVID MED 2010, MED 2011, UCF50 and HMDB51 datasets. For the MED 2010 dataset, we get a mean minimal normalized detection cost (MMNDC) of 0.49, which exceeds the state-of-the-art performance by more than 12 percent. On the TRECVID MED 2011 test dataset, we achieve a MMNDC of 0.51, which is the second best among all 19 participants. On UCF50 and HMDB51, we obtain classification accuracy of 88.1 % and 48.7 % respectively, which are the best reported results to date.","Zhenzhong Lan,Lei Bao,Shoou-I Yu,Wei Liu,Alexander G. Hauptmann",Multimedia Tools and Applications,2014,,,
329,The Pricing of Options and Corporate Liabilities,"If options are correctly priced in the market, it should not be possible to make sure profits by creating portfolios of long and short positions in options and their underlying stocks. Using this principle, a theoretical valuation formula for options is derived. Since almost all corporate liabilities can be viewed as combinations of options, the formula and the analysis that led to it are also applicable to corporate liabilities such as common stock, corporate bonds, and warrants. In particular, the formula can be used to derive the discount that should be applied to a corporate bond because of the possibility of default.","Fischer Black,Myron S. Scholes",Journal of Political Economy,1973,,,
330,Stream-based active learning for sentiment analysis in the financial domain,"Studying the relationship between public sentiment and stock prices has been the focus of several studies. This paper analyzes whether the sentiment expressed in Twitter feeds, which discuss selected companies and their products, can indicate their stock price changes. To address this problem, an active learning approach was developed and applied to sentiment analysis of tweet streams in the stock market domain. The paper first presents a static Twitter data analysis problem, explored in order to determine the best Twitter-specific text preprocessing setting for training the Support Vector Machine (SVM) sentiment classifier. In the static setting, the Granger causality test shows that sentiments in stock-related tweets can be used as indicators of stock price movements a few days in advance, where improved results were achieved by adapting the SVM classifier to categorize Twitter posts into three sentiment categories of positive, negative and neutral (instead of positive and negative only). These findings were adopted in the development of a new stream-based active learning approach to sentiment analysis, applicable in incremental learning from continuously changing financial tweet streams. To this end, a series of experiments was conducted to determine the best querying strategy for active learning of the SVM classifier adapted to sentiment analysis of financial tweet streams. The experiments in analyzing stock market sentiments of a particular company show that changes in positive sentiment probability can be used as indicators of the changes in stock closing prices.","Jasmina Smailović,Miha Grčar,Nada Lavrač,Martin Žnidaršič",Information Sciences,2014,,,
331,Effective personalization based on association rule discovery from web usage data,"To engage visitors to a Web site at a very early stage (i.e., before registration or authentication), personalization tools must rely primarily on clickstream data captured in Web server logs. The lack of explicit user ratings as well as the sparse nature and the large volume of data in such a setting poses serious challenges to standard collaborative filtering techniques in terms of scalability and performance. Web usage mining techniques such as clustering that rely on offline pattern discovery from user transactions can be used to improve the scalability of collaborative filtering, however, this is often at the cost of reduced recommendation accuracy. In this paper we propose effective and scalable techniques for Web personalization based on association rule discovery from usage data. Through detailed experimental evaluation on real usage data, we show that the proposed methodology can achieve better recommendation effectiveness, while maintaining a computational advantage over direct approaches to collaborative filtering such as the k-nearest-neighbor strategy.","Bamshad Mobasher,Honghua Dai,Honghua Dai,Tao Luo,Miki Nakagawa",,2001,,,
332,Exploring social influence for recommendation: a generative model approach,"Social friendship has been shown beneficial for item recommendation for years. However, existing approaches mostly incorporate social friendship into recommender systems by heuristics. In this paper, we argue that social influence between friends can be captured quantitatively and propose a probabilistic generative model, called social influenced selection(SIS), to model the decision making of item selection (e.g., what book to buy or where to dine). Based on SIS, we mine the social influence between linked friends and the personal preferences of users through statistical inference. To address the challenges arising from multiple layers of hidden factors in SIS, we develop a new parameter learning algorithm based on expectation maximization (EM). Moreover, we show that the mined social influence and user preferences are valuable for group recommendation and viral marketing. Finally, we conduct a comprehensive performance evaluation using real datasets crawled from last.fm and whrrl.com to validate our proposal. Experimental results show that social influence captured based on our SIS model is effective for enhancing both item recommendation and group recommendation, essential for viral marketing, and useful for various user analysis.","Mao Ye,Xingjie Liu,Wang-Chien Lee",,2012,,,
333,A hybrid online-product recommendation system: Combining implicit rating-based collaborative filtering and sequential pattern analysis,"Many online shopping malls in which explicit rating information is not available still have difficulty in providing recommendation services using collaborative filtering (CF) techniques for their users. Applying temporal purchase patterns derived from sequential pattern analysis (SPA) for recommendation services also often makes users unhappy with the inaccurate and biased results obtained by not considering individual preferences. The objective of this research is twofold. One is to derive implicit ratings so that CF can be applied to online transaction data even when no explicit rating information is available, and the other is to integrate CF and SPA for improving recommendation quality. Based on the results of several experiments that we conducted to compare the performance between ours and others, we contend that implicit rating can successfully replace explicit rating in CF and that the hybrid approach of CF and SPA is better than the individual ones.","Keunho Choi,Donghee Yoo,Gunwoo Kim,Yongmoo Suh",Electronic Commerce Research and Applications,2012,,,
334,A utility measure for finding multiobjective shortest paths in urban multimodal transportation networks,"Abstract   In this paper we study the problem of finding Origin–Destination (O–D) shortest paths in urban multimodal transportation networks, aiming at minimizing the overall cost, time and users' discommodity associated with the required paths. We present an approach based on the classical shortest path problem on a network representing the urban multimodal transportation system, i.e. the private, the public and the pedestrian modalities. Our idea is to make use of an ad hoc utility function for weighing the arcs both with their cost and time and considering at the same time the preference of the users related to all the possible transportation modalities. In particular, a utility measure is presented taking into a proper account the different users' propensities. The proposed approach has been developed for analysing the urban transportation network of an Italian city; the first experimental results are given in the paper.","Paola Modesti,Anna Sciomachen,Anna Sciomachen",European Journal of Operational Research,1998,,,
335,Spammer detection and tagging based user generated video search system — A survey,"In this modern world everything is done using online. Most people tend to watch films, sports, songs and also play games in online. One such mostly used site to do all this activity is the You Tube. In such online sites there is a possibility to introduce unwanted information which leads to wastage of user time in searching the content. These unwanted information present in the online sites is referred to us the spammers. In the existing work related to the you Tube the user have to search for the videos which takes a long time and irrelevant videos not related to the user search is displayed. The aim of the proposed work is that, User-generated content (UGC) video systems by definition heavily depend on the input of their community of users and their social interactions for video diffusion and opinion sharing. As a UGC system can achieve a larger audience through improved connectivity, findings motivate to propose a mean to enhance the users' connectivity by taking benefit of friend recommendation and spammer detection of the online videos. The algorithm active lazy fuzzy classifier algorithm used to detect spammers. Two similarity metrics are constructed based on users' interests that are derived from their uploads and favorite tagging of videos. Two friend recommendation algorithms are then proposed. The algorithms use public information provided by users to suggest potential friends with similar interests as measured by the similarity metrics. This paper presents the survey of spammer detection and their technologies and also recommending videos to the user by means of friend function which reduces the time of searching the videos and various methods for rating the videos.","Lavanya Ramprasad,G. Amudha",,2014,,,
336,Retail spatial evolution: paving the way from traditional to metaverse retailing,"This paper examines the evolution of retailing, i.e. from traditional to electronic to metaverse retailing and sheds light on the ways metaverses influence that evolution. The spatial dimension is taken into consideration as retailers could operate simultaneously in three different, but intertwined spaces. Particular emphasis is paid to key promotional aspects and we highlight the key challenges and opportunities faced by traditional retailers, e-retailers and metaverse retailers. For the metaverse phenomenon, the authors analyse Second Life and a range of findings emerge. One key finding is that retailers need to employ a holistic and overarching approach when devising their promotional strategies, especially if they aim to operate at the metaverse stage as well. At the end, the authors recommend a range of future research avenues and note the immediate need for policy development dealing with the metaverse phenomenon.","Michael Bourlakis,Savvas Papagiannidis,Feng Li",Electronic Commerce Research,2009,,,
337,Principal component analysis,"Abstract   Principal component analysis of a data matrix extracts the dominant patterns in the matrix in terms of a complementary set of score and loading plots. It is the responsibility of the data analyst to formulate the scientific issue at hand in terms of PC projections, PLS regressions, etc. Ask yourself, or the investigator, why the data matrix was collected, and for what purpose the experiments and measurements were made. Specify before the analysis what kinds of patterns you would expect and what you would find exciting.  The results of the analysis depend on the scaling of the matrix, which therefore must be specified. Variance scaling, where each variable is scaled to unit variance, can be recommended for general use, provided that almost constant variables are left unscaled. Combining different types of variables warrants blockscaling.  In the initial analysis, look for outliers and strong groupings in the plots, indicating that the data matrix perhaps should be “polished” or whether disjoint modeling is the proper course.  For plotting purposes, two or three principal components are usually sufficient, but for modeling purposes the number of significant components should be properly determined, e.g. by cross-validation.  Use the resulting principal components to guide your continued investigation or chemical experimentation, not as an end in itself.","Svante Wold,Kim H. Esbensen,Paul Geladi",Chemometrics and Intelligent Laboratory Systems,1987,,,
338,Using support vector machine with a hybrid feature selection method to the stock trend prediction,"In this paper, we developed a prediction model based on support vector machine (SVM) with a hybrid feature selection method to predict the trend of stock markets. This proposed hybrid feature selection method, named F-score and Supported Sequential Forward Search (F_SSFS), combines the advantages of filter methods and wrapper methods to select the optimal feature subset from original feature set. To evaluate the prediction accuracy of this SVM-based model combined with F_SSFS, we compare its performance with back-propagation neural network (BPNN) along with three commonly used feature selection methods including Information gain, Symmetrical uncertainty, and Correlation-based feature selection via paired t-test. The grid-search technique using 5-fold cross-validation is used to find out the best parameter value of kernel function of SVM. In this study, we show that SVM outperforms BPN to the problem of stock trend prediction. In addition, our experimental results show that the proposed SVM-based model combined with F_SSFS has the highest level of accuracies and generalization performance in comparison with the other three feature selection methods. With these results, we claim that SVM combined with F_SSFS can serve as a promising addition to the existing stock trend prediction methods.",Ming-Chi Lee,Expert Systems With Applications,2009,,,
339,Sparse Coding-Inspired Optimal Trading System for HFT Industry,"The financial industry has witnessed an exceptionally fast progress of incorporating information processing techniques in designing knowledge-based automated systems for high-frequency trading (HFT). This paper proposes a sparse coding-inspired optimal trading (SCOT) system for real-time high-frequency financial signal representation and trading. Mathematically, SCOT simultaneously learns the dictionary, sparse features, and the trading strategy in a joint optimization, yielding optimal feature representations for the specific trading objective. The learning process is modeled as a bilevel optimization and solved by the online gradient descend method with fast convergence. In this dynamic context, the system is tested on the real financial market to trade the index futures in the Shanghai exchange center.","Yue Deng,Youyong Kong,Youyong Kong,Feng Bao,Qionghai Dai",IEEE Transactions on Industrial Informatics,2015,,,
340,Introduction to Linear Regression Analysis,"(2003). Introduction to Linear Regression Analysis. The American Statistician: Vol. 57, No. 1, pp. 67-67.",Sreenivasa Rao Jammalamadaka,The American Statistician,2003,,,
341,Dropout: a simple way to prevent neural networks from overfitting,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different ""thinned"" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.","Nitish Srivastava,Geoffrey E. Hinton,Alex Krizhevsky,Ilya Sutskever,Ruslan Salakhutdinov",Journal of Machine Learning Research,2014,,,
342,A deep architecture with bilinear modeling of hidden representations: Applications to phonetic recognition,"We develop and describe a novel deep architecture, the Tensor Deep Stacking Network (T-DSN), where multiple blocks are stacked one on top of another and where a bilinear mapping from hidden representations to the output in each block is used to incorporate higher-order statistics of the input features. A learning algorithm for the T-DSN is presented, in which the main parameter estimation burden is shifted to a convex sub-problem with a closed-form solution. Using an efficient and scalable parallel implementation, we train a T-DSN to discriminate standard three-state monophones in the TIMIT database. The T-DSN outperforms an alternative pretrained Deep Neural Network (DNN) architecture in frame-level classification (both state and phone) and in the cross-entropy measure. For continuous phonetic recognition, T-DSN performs equivalently to a DNN but without the need for a hard-to-scale, sequential fine-tuning step.","Brian Hutchinson,Li Deng,Dong Yu",,2012,,,
343,A Rule-Based Recommender System for Online Discussion Forums,"In this paper we present a rule-based personalization framework for encapsulating and combining personalization algorithms known from adaptive hypermedia and recommender systems. We show how this personalization framework can be integrated into existing systems by example of the educational online board Comtella-D, which exploits the framework for recommending relevant discussions to the users. In our evaluations we compare different recommender strategies, investigate usage behavior over time, and show that a small amount of user data is sufficient to generate precise recommendations.","Fabian Abel,Ig Ibert Bittencourt,Nicola Henze,Daniel Krause,Julita Vassileva",,2008,,,
344,An Introduction to Genetic Algorithms,"From the Publisher:

""This is the best general book on Genetic Algorithms written to date. It covers background, history, and motivation; it selects important, informative examples of applications and discusses the use of Genetic Algorithms in scientific models; and it gives a good account of the status of the theory of Genetic Algorithms. Best of all the book presents its material in clear, straightforward, felicitous prose, accessible to anyone with a college-level scientific background. If you want a broad, solid understanding of Genetic Algorithms -- where they came from, what's being done with them, and where they are going -- this is the book.
-- John H. Holland, Professor, Computer Science and Engineering, and Professor of Psychology, The University of Michigan; External Professor, the Santa Fe Institute. 
Genetic algorithms have been used in science and engineering as adaptive algorithms for solving practical problems and as computational models of natural evolutionary systems. This brief, accessible introduction describes some of the most interesting research in the field and also enables readers to implement and experiment with genetic algorithms on their own. It focuses in depth on a small set of important and interesting topics -- particularly in machine learning, scientific modeling, and artificial life -- and reviews a broad span of research, including the work of Mitchell and her colleagues. 
The descriptions of applications and modeling projects stretch beyond the strict boundaries of computer science to include dynamical systems theory, game theory, molecular biology, ecology, evolutionary biology, and population genetics, underscoring the exciting ""general purpose"" nature of genetic algorithms as search methods that can be employed across disciplines. 
An Introduction to Genetic Algorithms is accessible to students and researchers in any scientific discipline. It includes many thought and computer exercises that build on and reinforce the reader's understanding of the text. 
The first chapter introduces genetic algorithms and their terminology and describes two provocative applications in detail. The second and third chapters look at the use of genetic algorithms in machine learning (computer programs, data analysis and prediction, neural networks) and in scientific models (interactions among learning, evolution, and culture; sexual selection; ecosystems; evolutionary activity). Several approaches to the theory of genetic algorithms are discussed in depth in the fourth chapter. The fifth chapter takes up implementation, and the last chapter poses some currently unanswered questions and surveys prospects for the future of evolutionary computation.",Melanie Mitchell,,1996,,,
345,Diverse ensembles for active learning,"Query by Committee is an effective approach to selective sampling in which disagreement amongst an ensemble of hypotheses is used to select data for labeling. Query by Bagging and Query by Boosting are two practical implementations of this approach that use Bagging and Boosting, respectively, to build the committees. For effective active learning, it is critical that the committee be made up of consistent hypotheses that are very different from each other. D ECORATE  is a recently developed method that directly constructs such diverse committees using artificial training data. This paper introduces A CTIVE -D ECORATE , which uses D ECORATE  committees to select good training examples. Extensive experimental results demonstrate that, in general, A CTIVE -D ECORATE  outperforms both Query by Bagging and Query by Boosting.","Prem Melville,Raymond J. Mooney",,2004,,,
346,On bias plus variance,"This paper presents a Bayesian ""correction"" to the familiar quadratic loss bias-plus-variance formula. It then discusses some other loss-function-specific aspects of supervised learning. It ends by presenting a version of the bias-plus-variance formula appropriate for log loss.","David H. Wolpert,David H. Wolpert,David H. Wolpert",Neural Computation,1997,,,
347,Maximum entropy inverse reinforcement learning,"Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.

We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.","Brian D. Ziebart,Andrew L. Maas,J. Andrew Bagnell,Anind K. Dey",,2008,,,
348,VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text,"The inherent nature of social media content poses serious challenges to practical applications of sentiment analysis. We present VADER, a simple rule-based model for general sentiment analysis, and compare its effectiveness to eleven typical state-of-practice benchmarks including LIWC, ANEW, the General Inquirer, SentiWordNet, and machine learning oriented techniques relying on Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) algorithms. Using a combination of qualitative and quantitative methods, we first construct and empirically validate a goldstandard list of lexical features (along with their associated sentiment intensity measures) which are specifically attuned to sentiment in microblog-like contexts. We then combine these lexical features with consideration for five general rules that embody grammatical and syntactical conventions for expressing and emphasizing sentiment intensity. Interestingly, using our parsimonious rule-based model to assess the sentiment of tweets, we find that VADER outperforms individual human raters (F1 Classification Accuracy = 0.96 and 0.84, respectively), and generalizes more favorably across contexts than any of our benchmarks.","Clayton J. Hutto,Eric Gilbert",,2014,,,
349,Music Recommendation Based on Acoustic Features and User Access Patterns,"Music recommendation is receiving increasing attention as the music industry develops venues to deliver music over the Internet. The goal of music recommendation is to present users lists of songs that they are likely to enjoy. Collaborative-filtering and content-based recommendations are two widely used approaches that have been proposed for music recommendation. However, both approaches have their own disadvantages: collaborative-filtering methods need a large collection of user history data and content-based methods lack the ability of understanding the interests and preferences of users. To overcome these limitations, this paper presents a novel dynamic music similarity measurement strategy that utilizes both content features and user access patterns. The seamless integration of them significantly improves the music similarity measurement accuracy and performance. Based on this strategy, recommended songs are obtained by a means of label propagation over a graph representing music similarity. Experimental results on a real data set collected from http://www.newwisdom.net demonstrate the effectiveness of the proposed approach.","Bo Shao,Mitsunori Ogihara,Dingding Wang,Tao Li","IEEE Transactions on Audio, Speech, and Language Processing",2009,,,
350,Simple Technical Trading Rules and the Stochastic Properties of Stock Returns,"This paper tests two of the simplest and most popular trading rules--moving average and trading range break--by utilizing the Dow Jones Index from 1897 to 1986. Standard statistical analysis is extended through the use of bootstrap techniques. Overall, their results provide strong support for the technical strategies. The returns obtained from these strategies are not consistent with four popular null models: the random walk, the AR(1), the GARCH-M, and the Exponential GARCH. Buy signals consistently generate higher returns than sell signals, and further, the returns following buy signals are less volatile than returns following sell signals. Moreover, returns following sell signals are negative, which is not easily explained by any of the currently existing equilibrium models. Copyright 1992 by American Finance Association.","William A. Brock,Josef Lakonishok,Blake LeBaron",Journal of Finance,1992,,,
351,Ensembles of Neural Networks for Robust Reinforcement Learning,"Reinforcement learning algorithms that employ neural networks as function approximators have proven to be powerful tools for solving optimal control problems. However, their training and the validation of final policies can be cumbersome as neural networks can suffer from problems like local minima or over fitting. When using iterative methods, such as neural fitted Q-iteration, the problem becomes even more pronounced since the network has to be trained multiple times and the training process in one iteration builds on the network trained in the previous iteration. Therefore errors can accumulate. In this paper we propose to use ensembles of networks to make the learning process more robust and produce near-optimal policies more reliably. We name various ways of combining single networks to an ensemble that results in a final ensemble policy and show the potential of the approach using a benchmark application. Our experiments indicate that majority voting is superior to Q-averaging and using heterogeneous ensembles (different network topologies) is advisable.","Alexander Hans,Steffen Udluft",,2010,,,
352,On the Importance of the Pearson Correlation Coefficient in Noise Reduction,"Noise reduction, which aims at estimating a clean speech from noisy observations, has attracted a considerable amount of research and engineering attention over the past few decades. In the single-channel scenario, an estimate of the clean speech can be obtained by passing the noisy signal picked up by the microphone through a linear filter/transformation. The core issue, then, is how to find an optimal filter/transformation such that, after the filtering process, the signal-to-noise ratio (SNR) is improved but the desired speech signal is not noticeably distorted. Most of the existing optimal filters (such as the Wiener filter and subspace transformation) are formulated from the mean-square error (MSE) criterion. However, with the MSE formulation, many desired properties of the optimal noise-reduction filters such as the SNR behavior cannot be seen. In this paper, we present a new criterion based on the Pearson correlation coefficient (PCC). We show that in the context of noise reduction the squared PCC (SPCC) has many appealing properties and can be used as an optimization cost function to derive many optimal and suboptimal noise-reduction filters. The clear advantage of using the SPCC over the MSE is that the noise-reduction performance (in terms of the SNR improvement and speech distortion) of the resulting optimal filters can be easily analyzed. This shows that, as far as noise reduction is concerned, the SPCC-based cost function serves as a more natural criterion to optimize as compared to the MSE.","Jacob Benesty,Jingdong Chen,Jingdong Chen,Jingdong Chen,Jingdong Chen,Yiteng Huang","IEEE Transactions on Audio, Speech, and Language Processing",2008,,,
353,Learning Attribute-to-Feature Mappings for Cold-Start Recommendations,"Cold-start scenarios in recommender systems are situations in which no prior events, like ratings or clicks, are known for certain users or items. To compute predictions in such cases, additional information about users (user attributes, e.g. gender, age, geographical location, occupation) and items (item attributes, e.g. genres, product categories, keywords) must be used. We describe a method that maps such entity (e.g. user or item) attributes to the latent features of a matrix (or higher-dimensional) factorization model. With such mappings, the factors of a MF model trained by standard techniques can be applied to the new-user and the new-item problem, while retaining its advantages, in particular speed and predictive accuracy. We use the mapping concept to construct an attribute-aware matrix factorization model for item recommendation from implicit, positive-only feedback. Experiments on the new-item problem show that this approach provides good predictive accuracy, while the prediction time only grows by a constant factor.","Zeno Gantner,Lucas Drumond,Christoph Freudenthaler,Steffen Rendle,Lars Schmidt-Thieme",,2010,,,
354,A Multiagent Approach to $Q$ -Learning for Daily Stock Trading,"The portfolio management for trading in the stock market poses a challenging stochastic control problem of significant commercial interests to finance industry. To date, many researchers have proposed various methods to build an intelligent portfolio management system that can recommend financial decisions for daily stock trading. Many promising results have been reported from the supervised learning community on the possibility of building a profitable trading system. More recently, several studies have shown that even the problem of integrating stock price prediction results with trading strategies can be successfully addressed by applying reinforcement learning algorithms. Motivated by this, we present a new stock trading framework that attempts to further enhance the performance of reinforcement learning-based systems. The proposed approach incorporates multiple Q-learning agents, allowing them to effectively divide and conquer the stock trading problem by defining necessary roles for cooperatively carrying out stock pricing and selection decisions. Furthermore, in an attempt to address the complexity issue when considering a large amount of data to obtain long-term dependence among the stock prices, we present a representation scheme that can succinctly summarize the history of price changes. Experimental results on a Korean stock market show that the proposed trading framework outperforms those trained by other alternative approaches both in terms of profit and risk management.","Jae Won Lee,Jonghun Park,Jonghun Park,Jangmin O,Jangmin O,Jongwoo Lee,Euy-Seok Hong,Euyseok Hong",,2007,,,
355,Learn++: an incremental learning algorithm for supervised neural networks,"We introduce Learn++, an algorithm for incremental training of neural network (NN) pattern classifiers. The proposed algorithm enables supervised NN paradigms, such as the multilayer perceptron (MLP), to accommodate new data, including examples that correspond to previously unseen classes. Furthermore, the algorithm does not require access to previously used data during subsequent incremental learning sessions, yet at the same time, it does not forget previously acquired knowledge. Learn++ utilizes ensemble of classifiers by generating multiple hypotheses using training data sampled according to carefully tailored distributions. The outputs of the resulting classifiers are combined using a weighted majority voting procedure. We present simulation results on several benchmark datasets as well as a real-world classification task. Initial results indicate that the proposed algorithm works rather well in practice. A theoretical upper bound on the error of the classifiers constructed by Learn++ is also provided.","Robi Polikar,L. Upda,S.S. Upda,Vasant Honavar",,2001,,,
356,Profitability of the On-Balance Volume Indicator,"In the literature, there is a lack of empirical studies documenting the profitability of volume-based technical indicators. This paper evaluates the profitability of the On-Balance Volume (OBV) trading rule. Our result shows that the OBV trading rule is increasingly profitable and rewards investors with notable returns in the stock markets of Greater China.","William Wai Him Tsang,Terence Tai Leung Chong",Economics Bulletin,2009,,,
357,The Theory and Practice of Online Learning,"Neither an academic tome nor a prescriptive 'how to' guide, ""The Theory and Practice of Online Learning"" is an illuminating collection of essays by practitioners and scholars active in the complex field of distance education.Distance education has evolved significantly in its 150 years of existence. For most of this time, it was an individual pursuit defined by infrequent postal communication. But recently, three more developmental generations have emerged, supported by television and radio, teleconferencing, and computer conferencing. The early 21st century has produced a fifth generation, based on autonomous agents and intelligent, database-assisted learning, that has been referred to as Web 2.0.The second edition of ""The Theory and Practice of Online Learning"" features updates in each chapter, plus four new chapters on current distance education issues such as connectivism and social software innovations.",Terry Anderson,,2009,,,
358,ADASYN: Adaptive synthetic sampling approach for imbalanced learning,"This paper presents a novel adaptive synthetic (ADASYN) sampling approach for learning from imbalanced data sets. The essential idea of ADASYN is to use a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. As a result, the ADASYN approach improves learning with respect to the data distributions in two ways: (1) reducing the bias introduced by the class imbalance, and (2) adaptively shifting the classification decision boundary toward the difficult examples. Simulation analyses on several machine learning data sets show the effectiveness of this method across five evaluation metrics.","Haibo He,Yang Bai,E.A. Garcia,Shutao Li",,2008,,,
359,Coupled Behavior Analysis with Applications,"Coupled behaviors refer to the activities of one to many actors who are associated with each other in terms of certain relationships. With increasing network and community-based events and applications, such as group-based crime and social network interactions, behavior coupling contributes to the causes of eventual business problems. Effective approaches for analyzing coupled behaviors are not available, since existing methods mainly focus on individual behavior analysis. This paper discusses the problem of Coupled Behavior Analysis (CBA) and its challenges. A Coupled Hidden Markov Model (CHMM)-based approach is illustrated to model and detect abnormal group-based trading behaviors. The CHMM models cater for: 1) multiple behaviors from a group of people, 2) behavioral properties, 3) interactions among behaviors, customers, and behavioral properties, and 4) significant changes between coupled behaviors. We demonstrate and evaluate the models on order-book-level stock tick data from a major Asian exchange and demonstrate that the proposed CHMMs outperforms HMM-only for modeling a single sequence or combining multiple single sequences, without considering coupling relationships to detect anomalies. Finally, we discuss interaction relationships and modes between coupled behaviors, which are worthy of substantial study.","Longbing Cao,Yuming Ou,Philip S. Yu",IEEE Transactions on Knowledge and Data Engineering,2012,,,
360,Collaborative Filtering Recommender Systems,"Recommender systems are an important part of the information and e-commerce ecosystem. They represent a powerful method for enabling users to filter through large information and product spaces. Nearly two decades of research on collaborative filtering have led to a varied set of algorithms and a rich collection of tools for evaluating their performance. Research in the field is moving in the direction of a richer understanding of how recommender technology may be embedded in specific domains. The differing personalities exhibited by different recommender algorithms show that recommendation is not a one-size-fits-all problem. Specific tasks, information needs, and item domains represent unique problems for recommenders, and design and evaluation of recommenders needs to be done based on the user tasks to be supported. Effective deployments must begin with careful analysis of prospective users and their goals. Based on this analysis, system designers have a host of options for the choice of algorithm and for its embedding in the surrounding user experience. This paper discusses a~wide variety of the choices available and their implications, aiming to provide both practicioners and researchers with an introduction to the important issues underlying recommenders and current best practices for addressing these issues.","Michael D. Ekstrand,John Riedl,Joseph A. Konstan",Foundations and Trends in Human-computer Interaction,2011,,,
361,PAMR: Passive aggressive mean reversion strategy for portfolio selection,"This article proposes a novel online portfolio selection strategy named ""Passive Aggressive Mean Reversion"" (PAMR). Unlike traditional trend following approaches, the proposed approach relies upon the mean reversion relation of financial markets. Equipped with online passive aggressive learning technique from machine learning, the proposed portfolio selection strategy can effectively exploit the mean reversion property of markets. By analyzing PAMR's update scheme, we find that it nicely trades off between portfolio return and volatility risk and reflects the mean reversion trading principle. We also present several variants of PAMR algorithm, including a mixture algorithm which mixes PAMR and other strategies. We conduct extensive numerical experiments to evaluate the empirical performance of the proposed algorithms on various real datasets. The encouraging results show that in most cases the proposed PAMR strategy outperforms all benchmarks and almost all state-of-the-art portfolio selection strategies under various performance metrics. In addition to its superior performance, the proposed PAMR runs extremely fast and thus is very suitable for real-life online trading applications. The experimental testbed including source codes and data sets is available at http://www.cais.ntu.edu.sg/~chhoi/PAMR/ .","Bin Li,Bin Li,Peilin Zhao,Steven C. H. Hoi,Vivekanand Gopalkrishnan",Machine Learning,2012,,,
362,Towards deeper understanding: Deep convex networks for semantic utterance classification,"Following the recent advances in deep learning techniques, in this paper, we present the application of special type of deep architecture — deep convex networks (DCNs) — for semantic utterance classification (SUC). DCNs are shown to have several advantages over deep belief networks (DBNs) including classification accuracy and training scalability. However, adoption of DCNs for SUC comes with non-trivial issues. Specifically, SUC has an extremely sparse input feature space encompassing a very large number of lexical and semantic features. This is about a few thousand times larger than the feature space for acoustic modeling, yet with a much smaller number of training samples. Experimental results we obtained on a domain classification task for spoken language understanding demonstrate the effectiveness of DCNs. The DCN-based method produces higher SUC accuracy than the Boosting-based discriminative classifier with word trigrams.","Gokhan Tur,Li Deng,Dilek Hakkani-Tur,Xiaodong He,Xiaodong He",,2012,,,
363,Explorations in evolutionary design of online auction market mechanisms,"This paper describes the use of a genetic algorithm (GA) to find optimal parameter-values for trading agents that operate in virtual online auction “e-marketplaces”, where the rules of those marketplaces are also under simultaneous control of the GA. The aim is to use the GA to automatically design new mechanisms for agent-based e-marketplaces that are more efficient than online markets designed by (or populated by) humans. The space of possible auction-types explored by the GA includes the Continuous Double Auction (CDA) mechanism (as used in most of the world’s financial exchanges), and also two purely one-sided mechanisms. Surprisingly, the GA did not always settle on the CDA as an optimum. Instead, novel hybrid auction mechanisms were evolved, which are unlike any existing market mechanisms. In this paper we show that, when the market supply and demand schedules undergo sudden “shock” changes partway through the evaluation process, two-sided hybrid market mechanisms can evolve which may be unlike any human-designed auction and yet may also be significantly more efficient than any human designed market mechanism.",Dave Cliff,Electronic Commerce Research and Applications,2003,,,
364,Robot trajectory optimization using approximate inference,"The general stochastic optimal control (SOC) problem in robotics scenarios is often too complex to be solved exactly and in near real time. A classical approximate solution is to first compute an optimal (deterministic) trajectory and then solve a local linear-quadratic-gaussian (LQG) perturbation model to handle the system stochasticity. We present a new algorithm for this approach which improves upon previous algorithms like iLQG. We consider a probabilistic model for which the maximum likelihood (ML) trajectory coincides with the optimal trajectory and which, in the LQG case, reproduces the classical SOC solution. The algorithm then utilizes approximate inference methods (similar to expectation propagation) that efficiently generalize to non-LQG systems. We demonstrate the algorithm on a simulated 39-DoF humanoid robot.",Marc Toussaint,,2009,,,
365,Reinforcement learning: a survey,"This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ""reinforcement."" The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.","Leslie Pack Kaelbling,Michael L. Littman,Andrew W. Moore",Journal of Artificial Intelligence Research,1996,,,
366,Tensor Deep Stacking Networks,"A novel deep architecture, the tensor deep stacking network (T-DSN), is presented. The T-DSN consists of multiple, stacked blocks, where each block contains a bilinear mapping from two hidden layers to the output layer, using a weight tensor to incorporate higher order statistics of the hidden binary (([0,1])) features. A learning algorithm for the T-DSN's weight matrices and tensors is developed and described in which the main parameter estimation burden is shifted to a convex subproblem with a closed-form solution. Using an efficient and scalable parallel implementation for CPU clusters, we train sets of T-DSNs in three popular tasks in increasing order of the data size: handwritten digit recognition using MNIST (60k), isolated state/phone classification and continuous phone recognition using TIMIT (1.1 m), and isolated phone classification using WSJ0 (5.2 m). Experimental results in all three tasks demonstrate the effectiveness of the T-DSN and the associated learning methods in a consistent manner. In particular, a sufficient depth of the T-DSN, a symmetry in the two hidden layers structure in each T-DSN block, our model parameter learning algorithm, and a softmax layer on top of T-DSN are shown to have all contributed to the low error rates observed in the experiments for all three tasks.","Brian Hutchinson,Li Deng,Dong Yu",IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,,,
367,Coupled nominal similarity in unsupervised learning,"The similarity between nominal objects is not straightforward, especially in unsupervised learning. This paper proposes coupled similarity metrics for nominal objects, which consider not only intra-coupled similarity within an attribute (i.e., value frequency distribution) but also inter-coupled similarity between attributes (i.e. feature dependency aggregation). Four metrics are designed to calculate the inter-coupled similarity between two categorical values by considering their relationships with other attributes. The theoretical analysis reveals their equivalent accuracy and superior efficiency based on intersection against others, in particular for large-scale data. Substantial experiments on extensive UCI data sets verify the theoretical conclusions. In addition, experiments of clustering based on the derived dissimilarity metrics show a significant performance improvement.","Can Wang,Longbing Cao,Ming-Chun Wang,Jinjiu Li,Wei Wei,Yuming Ou",,2011,,,
368,Cross-domain collaborative filtering via bilinear multilevel analysis,"Cross-domain collaborative filtering (CDCF), which aims to leverage data from multiple domains to relieve the data sparsity issue, is becoming an emerging research topic in recent years. However, current CDCF methods that mainly consider user and item factors but largely neglect the heterogeneity of domains may lead to improper knowledge transfer issues. To address this problem, we propose a novel CDCF model, the Bilinear Multilevel Analysis (BLMA), which seamlessly introduces multilevel analysis theory to the most successful collaborative filtering method, matrix factorization (MF). Specifically, we employ BLMA to more efficiently address the determinants of ratings from a hierarchical view by jointly considering domain, community, and user effects so as to overcome the issues caused by traditional MF approaches. Moreover, a parallel Gibbs sampler is provided to learn these effects. Finally, experiments conducted on a realworld dataset demonstrate the superiority of the BLMA over other state-of-the-art methods.","Liang Hu,Jian Cao,Guandong Xu,Jie Wang,Zhiping Gu,Longbing Cao",,2013,,,
369,Statistical models of music-listening sessions in social media,"User experience in social media involves rich interactions with the media content and other participants in the community. In order to support such communities, it is important to understand the factors that drive the users' engagement. In this paper we show how to define statistical models of different complexity to describe patterns of song listening in an online music community. First, we adapt the LDA model to capture music taste from listening activities across users and identify both the groups of songs associated with the specific taste and the groups of listeners who share the same taste. Second, we define a graphical model that takes into account listening sessions and captures the listening moods of users in the community. Our session model leads to groups of songs and groups of listeners with similar behavior across listening sessions and enables faster inference when compared to the LDA model. Our experiments with the data from an online media site demonstrate that the session model is better in terms of the perplexity compared to two other models: the LDA-based taste model that does not incorporate cross-session information and a baseline model that does not use latent groupings of songs.","Elena Zheleva,John Guiver,Eduarda Mendes Rodrigues,Natasa Milic-Frayling",,2010,,,
370,Measuring fuzzy uncertainty,"First, this paper reviews several well known measures of fuzziness for discrete fuzzy sets. Then new multiplicative and additive classes are defined. We show that each class satisfies five well-known axioms for fuzziness measures, and demonstrate that several existing measures are relatives of these classes. The multiplicative class is based on nonnegative, monotone increasing concave functions. The additive class requires only nonnegative concave functions. Some relationships between several existing and the new measures are established, and some new properties are derived. The relative merits and drawbacks of different measures for applications are discussed. A weighted fuzzy entropy which is flexible enough to incorporate subjectiveness in the measure of fuzziness is also introduced. Finally, we comment on the construction of measures that may assess all of the uncertainties associated with a physical system. >","Nikhil R. Pal,James C. Bezdek",IEEE Transactions on Fuzzy Systems,1994,,,
371,Evolving neural networks through augmenting topologies,"An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.","Kenneth O. Stanley,Risto Miikkulainen",Evolutionary Computation,2002,,,
372,Experiments with a new boosting algorithm,"In an earlier paper, we introduced a new ""boosting"" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a ""pseudo-loss"" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's ""bagging"" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.","Yoav Freund,Robert E. Schapire",,1996,,,
373,Integrating microRNA target predictions for the discovery of gene regulatory networks: a semi-supervised ensemble learning approach.,"Background
MicroRNAs (miRNAs) are small non-coding RNAs which play a key role in the post-transcriptional regulation of many genes. Elucidating miRNA-regulated gene networks is crucial for the understanding of mechanisms and functions of miRNAs in many biological processes, such as cell proliferation, development, differentiation and cell homeostasis, as well as in many types of human tumors. To this aim, we have recently presented the biclustering method HOCCLUS2, for the discovery of miRNA regulatory networks. Experiments on predicted interactions revealed that the statistical and biological consistency of the obtained networks is negatively affected by the poor reliability of the output of miRNA target prediction algorithms. Recently, some learning approaches have been proposed to learn to combine the outputs of distinct prediction algorithms and improve their accuracy. However, the application of classical supervised learning algorithms presents two challenges: i) the presence of only positive examples in datasets of experimentally verified interactions and ii) unbalanced number of labeled and unlabeled examples.","Gianvito Pio,Donato Malerba,Domenica D'Elia,Michelangelo Ceci",BMC Bioinformatics,2014,,,
374,Bagging survival trees,"Predicted survival probability functions of censored event free survival are improved by bagging survival trees. We suggest a new method to aggregate survival trees in order to obtain better predictions for breast cancer and lymphoma patients. A set of survival trees based on B bootstrap samples is computed. We define the aggregated Kaplan–Meier curve of a new observation by the Kaplan–Meier curve of all observations identified by the B leaves containing the new observation. The integrated Brier score is used for the evaluation of predictive models. We analyse data of a large trial on node positive breast cancer patients conducted by the German Breast Cancer Study Group and a smaller ‘pilot’ study on diffuse large B-cell lymphoma, where prognostic factors are derived from microarray expression values. In addition, simulation experiments underline the predictive power of our proposal. Copyright © 2004 John Wiley & Sons, Ltd.","Torsten Hothorn,Berthold Lausen,Axel Benner,Martin Radespiel-Tröger",Statistics in Medicine,2004,,,
375,Non-IIDness Learning in Behavioral and Social Data,"Most of the classic theoretical systems and tools in statistics, data mining and machine learning are built on the fundamental assumption of IIDness, which assumes the independence and identical distribution of underlying objects, attributes and/or values. However, complex behavioral and social problems often exhibit strong couplings and heterogeneity between values, attributes and objects (i.e., non-IIDness). This fundamentally challenges the IIDness-based learning methodologies and techniques. This paper presents a high-level overview of the needs, challenges and opportunities of non-IIDness learning for handling complex behavioral and social problems. By reviewing the nature and issues of classic IIDness-based algorithms in frequent pattern mining, clustering and classification to complex behavioral and social applications, concepts, structures, frameworks and exemplartechniquesarediscussedfornon-IIDnesslearning.Casestudies,relatedworkandprospects of non-IIDness learning are presented. Non-IIDness learning is also a fundamental issue in big data analytics.",Longbing Cao,The Computer Journal,2014,,,
376,The random subspace method for constructing decision forests,"Much of previous attention on decision trees focuses on the splitting criteria and optimization of tree sizes. The dilemma between overfitting and achieving maximum accuracy is seldom resolved. A method to construct a decision tree based classifier is proposed that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity. The classifier consists of multiple trees constructed systematically by pseudorandomly selecting subsets of components of the feature vector, that is, trees constructed in randomly chosen subspaces. The subspace method is compared to single-tree classifiers and other forest construction methods by experiments on publicly available datasets, where the method's superiority is demonstrated. We also discuss independence between trees in a forest and relate that to the combined classification accuracy.",Tin Kam Ho,IEEE Transactions on Pattern Analysis and Machine Intelligence,1998,,,
377,A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems,"Recent online services rely heavily on automatic personalization to recommend relevant content to a large number of users. This requires systems to scale promptly to accommodate the stream of new users visiting the online services for the first time. In this work, we propose a content-based recommendation system to address both the recommendation quality and the system scalability. We propose to use a rich feature set to represent users, according to their web browsing history and search queries. We use a Deep Learning approach to map users and items to a latent space where the similarity between users and their preferred items is maximized. We extend the model to jointly learn from features of items from different domains and user features by introducing a multi-view Deep Learning model. We show how to make this rich-feature based user representation scalable by reducing the dimension of the inputs and the amount of training data. The rich user feature representation allows the model to learn relevant user behavior patterns and give useful recommendations for users who do not have any interaction with the service, given that they have adequate search and browsing history. The combination of different domains into a single model for learning helps improve the recommendation quality across all the domains, as well as having a more compact and a semantically richer user latent feature vector. We experiment with our approach on three real-world recommendation systems acquired from different sources of Microsoft products: Windows Apps recommendation, News recommendation, and Movie/TV recommendation. Results indicate that our approach is significantly better than the state-of-the-art algorithms (up to 49% enhancement on existing users and 115% enhancement on new users). In addition, experiments on a publicly open data set also indicate the superiority of our method in comparison with transitional generative topic models, for modeling cross-domain recommender systems. Scalability analysis show that our multi-view DNN model can easily scale to encompass millions of users and billions of item entries. Experimental results also confirm that combining features from all domains produces much better performance than building separate models for each domain.","Ali Elkahky,Yang Song,Yang Song,Yang Song,Xiaodong He,Xiaodong He",,2015,,,
378,An ensemble based data fusion approach for early diagnosis of Alzheimer's disease,"As the number of the elderly population affected by Alzheimer's disease (AD) rises rapidly, the need to find an accurate, inexpensive and non-intrusive diagnostic procedure that can be made available to community healthcare providers is becoming an increasingly urgent public health concern. Several recent studies have looked at analyzing electroencephalogram (EEG) signals through the use of wavelets and neural networks. While showing great promise, the final outcomes of these studies have been largely inconclusive. This is mostly due to inherent difficulty of the problem, but also - perhaps - due to inefficient use of the available information, as many of these studies have used a single EEG channel for the analysis. In this contribution, we describe an ensemble of classifiers based data fusion approach to combine information from two or more sources, believed to contain complementary information, for early diagnosis of Alzheimer's disease. Our emphasis is on sequentially generating an ensemble of classifiers that explicitly seek the most discriminating information from each data source. Specifically, we use the event related potentials recorded from the Pz, Cz, and Fz electrodes of the EEG, decomposed into different frequency bands using multiresolution wavelet analysis. The proposed data fusion approach includes generating multiple classifiers trained with strategically selected subsets of the training data from each source, which are then combined through a modified weighted majority voting procedure. The implementation details and the promising outcomes of this implementation are presented.","Robi Polikar,Apostolos Topalis,Devi Parikh,Deborah Green,Jennifer L. Frymiare,John Kounios,Christopher M. Clark",Information Fusion,2008,,,
379,Content-based Recommender Systems: State of the Art and Trends,"Recommender systems have the effect of guiding users in a personal- ized way to interesting objects in a large space of possible options. Content-based recommendation systems try to recommend items similar to those a given user has liked in the past. Indeed, the basic process performed by a content-based recom- mender consists in matching up the attributes of a user profile in which preferences and interests are stored, with the attributes of a content object (item), in order to recommend to the user new interesting items. This chapter provides an overview of content-based recommender systems, with the aim of imposing a degree of order on the diversity of the different aspects involved in their design and implementation. The first part of the chapter presents the basic concepts and terminology of content- based recommender systems, a high level architecture, and their main advantages and drawbacks. The second part of the chapter provides a review of the state of the art of systems adopted in several application domains, by thoroughly describ- ing both classical and advanced techniques for representing items and user profiles. The most widely adopted techniques for learning user profiles are also presented. The last part of the chapter discusses trends and future research which might lead towards the next generation of systems, by describing the role of User Generated Content as a way for taking into account evolving vocabularies, and the challenge of feeding users with serendipitous recommendations, that is to say surprisingly interesting items that they might not have otherwise discovered.","Pasquale Lops,Marco de Gemmis,Giovanni Semeraro",,2011,,,
380,Web path recommendations based on page ranking and Markov models,"Markov models have been widely used for modelling users' navigational behaviour in the Web graph, using the transitional probabilities between web pages, as recorded in the web logs. The recorded users' navigation is used to extract popular web paths and predict current users' next steps. Such purely usage-based probabilistic models, however, present certain shortcomings. Since the prediction of users' navigational behaviour is based solely on the usage data, structural properties of the Web graph are ignored. Thus important - in terms of pagerank authority score - paths may be underrated. In this paper we present a hybrid probabilistic predictive model extending the properties of Markov models by incorporating link analysis methods. More specifically, we propose the use of a PageRank-style algorithm for assigning prior probabilities to the web pages based on their importance in the web site's graph. We prove, through experimentation, that this approach results in more objective and representative predictions than the ones produced from the pure usage-based approaches.","Magdalini Eirinaki,Michalis Vazirgiannis,Dimitris Kapogiannis",,2005,,,
381,Unsupervised Learning of Video Representations using LSTMs,"We use Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences - patches of image pixels and high-level representations (""percepts"") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We further evaluate the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.","Nitish Srivastava,Elman Mansimov,Ruslan Salakhudinov",,2015,,,
382,Distance Metric Learning with Application to Clustering with Side-Information,"Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many ""plausible"" ways, and if a clustering algorithm such as K-means initially fails to find one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufficiently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider ""similar."" For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in ℝn, learns a distance metric over ℝn that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efficient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to significantly improve clustering performance.","Eric P. Xing,Michael I. Jordan,Stuart Russell,Stuart Russell,Stuart D. Russell,Andrew Y. Ng",,2002,,,
383,Profitable Momentum Trading Strategies for Individual Investors,"For nearly three decades, scientific studies have explored momentum investing strategies and observed stable excess returns in various financial markets. However, the trading strategies typically analyzed in such research are not accessible to individual investors due to short selling constraints, nor are they profitable due to high trading costs. Incorporating these constraints, we explore a simplified momentum trading strategy that only exploits excess returns from topside momentum for a small number of individual stocks. Building on US data from the New York Stock Exchange from July 1991 to December 2010, we analyze whether such a simplified momentum strategy outperforms the benchmark after factoring in realistic transaction costs and risks. We find that the strategy can indeed work for individual investors with initial investment amounts of at least $5,000. In further attempts to improve this practical trading strategy, we analyze an overlapping momentum trading strategy consisting of a more frequent trading of a smaller number of “winner” stocks. We find that increasing the trading frequency initially increases the risk-adjusted returns of these portfolios up to an optimal point, after which excessive transaction costs begin to dominate the scene. In a calibration study, we find that, depending on the initial investment amount of the portfolio, the optimal momentum trading frequency ranges from bi-yearly to monthly.","Bryan Foltice,Bryan Foltice,Thomas Langer",Financial Markets and Portfolio Management,2015,,,
384,ImageNet Large Scale Visual Recognition Challenge,"The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.","Olga Russakovsky,Jia Deng,Hao Su,Jonathan Krause,Sanjeev Satheesh,Sean Ma,Zhiheng Huang,Andrej Karpathy,Aditya Khosla,Michael S. Bernstein,Alexander C. Berg,Li Fei-Fei",International Journal of Computer Vision,2015,,,
385,Discovering golden nuggets: data mining in financial application,"With the increase of economic globalization and evolution of information technology, financial data are being generated and accumulated at an unprecedented pace. As a result, there has been a critical need for automated approaches to effective and efficient utilization of massive amount of financial data to support companies and individuals in strategic planning and investment decision-making. Data mining techniques have been used to uncover hidden patterns and predict future trends and behaviors in financial markets. The competitive advantages achieved by data mining include increased revenue, reduced cost, and much improved marketplace responsiveness and awareness. There has been a large body of research and practice focusing on exploring data mining techniques to solve financial problems. In this paper, we describe data mining in the context of financial application from both technical and application perspectives. In addition, we compare different data mining techniques and discuss important data mining issues involved in specific financial applications. Finally, we highlight a number of challenges and trends for future research in this area.","Dongsong Zhang,Lina Zhou",,2004,,,
386,Transfer learning for collaborative filtering via a rating-matrix generative model,"Cross-domain collaborative filtering solves the sparsity problem by transferring rating knowledge across multiple domains. In this paper, we propose a rating-matrix generative model (RMGM) for effective cross-domain collaborative filtering. We first show that the relatedness across multiple rating matrices can be established by finding a shared implicit cluster-level rating matrix, which is next extended to a cluster-level rating model. Consequently, a rating matrix of any related task can be viewed as drawing a set of users and items from a user-item joint mixture model as well as drawing the corresponding ratings from the cluster-level rating model. The combination of these two models gives the RMGM, which can be used to fill the missing ratings for both existing and new users. A major advantage of RMGM is that it can share the knowledge by pooling the rating data from multiple tasks even when the users and items of these tasks do not overlap. We evaluate the RMGM empirically on three real-world collaborative filtering data sets to show that RMGM can outperform the individual models trained separately.","Bin Li,Qiang Yang,Xiangyang Xue",,2009,,,
387,Personalized recommendation via cross-domain triadic factorization,"Collaborative filtering (CF) is a major technique in recommender systems to help users find their potentially desired items. Since the data sparsity problem is quite commonly encountered in real-world scenarios, Cross-Domain Collaborative Filtering (CDCF) hence is becoming an emerging research topic in recent years. However, due to the lack of sufficient dense explicit feedbacks and even no feedback available in users' uninvolved domains, current CDCF approaches may not perform satisfactorily in user preference prediction. In this paper, we propose a generalized Cross Domain Triadic Factorization (CDTF) model over the triadic relation user-item-domain, which can better capture the interactions between domain-specific user factors and item factors. In particular, we devise two CDTF algorithms to leverage user explicit and implicit feedbacks respectively, along with a genetic algorithm based weight parameters tuning algorithm to trade off influence among domains optimally. Finally, we conduct experiments to evaluate our models and compare with other state-of-the-art models by using two real world datasets. The results show the superiority of our models against other comparative models.","Liang Hu,Jian Cao,Guandong Xu,Longbing Cao,Zhiping Gu,Can Zhu",,2013,,,
388,An efficient sparse metric learning in high-dimensional space via l1-penalized log-determinant regularization,"This paper proposes an efficient sparse metric learning algorithm in high dimensional space via an l1-penalized log-determinant regularization. Compare to the most existing distance metric learning algorithms, the proposed algorithm exploits the sparsity nature underlying the intrinsic high dimensional feature space. This sparsity prior of learning distance metric serves to regularize the complexity of the distance model especially in the ""less example number p and high dimension d"" setting. Theoretically, by analogy to the covariance estimation problem, we find the proposed distance learning algorithm has a consistent result at rate O (√m2 log d)/n) to the target distance matrix with at most m nonzeros per row. Moreover, from the implementation perspective, this l1-penalized log-determinant formulation can be efficiently optimized in a block coordinate descent fashion which is much faster than the standard semi-definite programming which has been widely adopted in many other advanced distance learning algorithms. We compare this algorithm with other state-of-the-art ones on various datasets and competitive results are obtained.","Guo-Jun Qi,Jinhui Tang,Zheng-Jun Zha,Zheng-Jun Zha,Tat-Seng Chua,Hong-Jiang Zhang,Hong-Jiang Zhang",,2009,,,
389,Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning,"This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.",Ronald J. Williams,Machine Learning,1992,,,
390,Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics,"We present a policy search method that uses iteratively refitted local linear models to optimize trajectory distributions for large, continuous problems. These trajectory distributions can be used within the framework of guided policy search to learn policies with an arbitrary parameterization. Our method fits time-varying linear dynamics models to speed up learning, but does not rely on learning a global model, which can be difficult when the dynamics are complex and discontinuous. We show that this hybrid approach requires many fewer samples than model-free methods, and can handle complex, nonsmooth dynamics that can pose a challenge for model-based techniques. We present experiments showing that our method can be used to learn complex neural network policies that successfully execute simulated robotic manipulation tasks in partially observed environments with numerous contact discontinuities and underactuation.","Sergey Levine,Pieter Abbeel",,2014,,,
391,On the Separation Theorem of Stochastic Control,Optimal control and filtering problem for stochastic linear dynamic system reduced to independent equations,W.M. Wonham,Siam Journal on Control,1968,,,
392,Hierarchical Ensemble Clustering,"Ensemble clustering has emerged as an important elaboration of the classical clustering problems. Ensemble clustering refers to the situation in which a number of different (input) clusterings have been obtained for a particular dataset and it is desired to find a single (consensus) clustering which is a better fit in some sense than the existing clusterings. Many approaches have been developed to solve ensemble clustering problems over the last few years. However, most of these ensemble techniques are designed for partitional clustering methods. Few research efforts have been reported for ensemble hierarchical clustering methods. In this paper, we propose a hierarchical ensemble clustering framework which can naturally combine both partitional clustering and hierarchical clustering results. We notice the importance of ultra-metric distance for hierarchical clustering and propose a novel method for learning the ultra-metric distance from the aggregated distance matrices and generating final hierarchical clustering with enhanced cluster separation. Experimental results demonstrate the effectiveness of our proposed approaches.","Li Zheng,Tao Li,Chris Ding",,2010,,,
393,"A comparative survey of artificial intelligence applications in finance: artificial neural networks, expert system and hybrid intelligent systems","Nowadays, many current real financial applications have nonlinear and uncertain behaviors which change across the time. Therefore, the need to solve highly nonlinear, time variant problems has been growing rapidly. These problems along with other problems of traditional models caused growing interest in artificial intelligent techniques. In this paper, comparative research review of three famous artificial intelligence techniques, i.e., artificial neural networks, expert systems and hybrid intelligence systems, in financial market has been done. A financial market also has been categorized on three domains: credit evaluation, portfolio management and financial prediction and planning. For each technique, most famous and especially recent researches have been discussed in comparative aspect. Results show that accuracy of these artificial intelligent methods is superior to that of traditional statistical methods in dealing with financial problems, especially regarding nonlinear patterns. However, this outperformance is not absolute.",Arash Bahrammirzaee,Neural Computing and Applications,2010,,,
394,Identifying suspicious URLs: an application of large-scale online learning,"This paper explores online learning approaches for detecting malicious Web sites (those involved in criminal scams) using lexical and host-based features of the associated URLs. We show that this application is particularly appropriate for online algorithms as the size of the training data is larger than can be efficiently processed in batch and because the distribution of features that typify malicious URLs is changing continuously. Using a real-time system we developed for gathering URL features, combined with a real-time source of labeled URLs from a large Web mail provider, we demonstrate that recently-developed online algorithms can be as accurate as batch techniques, achieving classification accuracies up to 99% over a balanced data set.","Justin Ma,Lawrence K. Saul,Stefan Savage,Geoffrey M. Voelker",,2009,,,
395,Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding,"On large problems, reinforcement learning systems must use parameterized function approximators such as neural networks in order to generalize between similar situations and actions. In these cases there are no strong theoretical results on the accuracy of convergence, and computational results have been mixed. In particular, Boyan and Moore reported at last year's meeting a series of negative results in attempting to apply dynamic programming together with function approximation to simple control problems with continuous state spaces. In this paper, we present positive results for all the control tasks they attempted, and for one that is significantly larger. The most important differences are that we used sparse-coarse-coded function approximators (CMACs) whereas they used mostly global function approximators, and that we learned online whereas they learned offline. Boyan and Moore and others have suggested that the problems they encountered could be solved by using actual outcomes (""rollouts""), as in classical Monte Carlo methods, and as in the TD(λ) algorithm when λ = 1. However, in our experiments this always resulted in substantially poorer performance. We conclude that reinforcement learning can work robustly in conjunction with function approximators, and that there is little justification at present for avoiding the case of general λ.","Richard S. Sutton,Richard S. Sutton",,1995,,,
396,Managing Diversity in Regression Ensembles,"Ensembles are a widely used and effective technique in machine learning---their success is commonly attributed to the degree of disagreement, or 'diversity', within the ensemble. For ensembles where the individual estimators output crisp class labels, this 'diversity' is not well understood and remains an open research issue. For ensembles of regression estimators, the diversity can be exactly formulated in terms of the covariance between individual estimator outputs, and the optimum level is expressed in terms of a bias-variance-covariance trade-off. Despite this, most approaches to learning ensembles use heuristics to encourage the right degree of diversity. In this work we show how to explicitly control diversity through the error function. The first contribution of this paper is to show that by taking the combination mechanism for the ensemble into account we can derive an error function for each individual that balances ensemble diversity with individual accuracy. We show the relationship between this error function and an existing algorithm called negative correlation learning, which uses a heuristic penalty term added to the mean squared error function. It is demonstrated that these methods control the bias-variance-covariance trade-off systematically, and can be utilised with any estimator capable of minimising a quadratic error function, for example MLPs, or RBF networks. As a second contribution, we derive a strict upper bound on the coefficient of the penalty term, which holds for any estimator that can be cast in a generalised linear regression framework, with mild assumptions on the basis functions. Finally we present the results of an empirical study, showing significant improvements over simple ensemble learning, and finding that this technique is competitive with a variety of methods, including boosting, bagging, mixtures of experts, and Gaussian processes, on a number of tasks.","Gavin Brown,Jeremy L. Wyatt,Peter Tiňo",Journal of Machine Learning Research,2005,,,
397,Price formation in double auction markets,"Abstract   This paper reports 14 laboratory experiments that examine existing theories of the price formation process in the continuous double auction. The experiments feature random values and costs, and therefore a new price formation observation each period. We find that efficiency is high and rises with experience in this environment. We also find that trades with greater exchange surplus tend to occur earlier in the period, that increased trader experience reduces an anomalous intertemporal arbitrage opportunity observed previously, and that only when traders are very experienced does exchange surplus accrue disproportionately to the side of the market with a smaller number of traders.","Timothy N. Cason,Daniel Friedman",Journal of Economic Dynamics and Control,1996,,,
398,Returns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency,"This paper documents that strategies that buy stocks that have performed well in the past and sell stocks that hav e performed poorly in the past generate significant positive returns o ver three- to twelve-month holding periods. The authors find that the profitability of these strategies are not due to their systematic risk or to delay ed stock price reactions to common factors. However, part of the abnorm al returns generated in the first year after portfolio formation dissipates in the following two years. A similar pattern of returns around the earnings announcements of past winners and losers is also documented. Copyright 1993 by American Finance Association.","Narasimhan Jegadeesh,Sheridan Titman",Journal of Finance,1993,,,
399,Price Formation in Double Auctions,"We develop a model of information processing and strategy choice for participants in a double auction. Sellers in this model form beliefs that an offer will be accepted by some buyer. Similarly, buyers form beliefs that a bid will be accepted. These beliefs are formed on the basis of observed market data, including frequencies of asks, bids, accepted asks, and accepted bids. Then traders choose an action that maximizes their own expected surplus. The trading activity resulting from these beliefs and strategies is sufficient to achieve transaction prices at competitive equilibrium and complete market efficiency after several periods of trading.Journal of Economic LiteratureClassification Numbers: D41, D44, D8","Steven Gjerstad,John Dickhaut",Games and Economic Behavior,1998,,,
400,Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis.,"article i nfo For the last decade, it has been shown that neuroimaging can be a potential tool for the diagnosis of Alzheimer's Disease (AD) and its prodromalstage,Mild Cognitive Impairment (MCI), and also fusion of different modalities can further provide the complementary information to enhance diagnostic accuracy. Here, we focus on the problems of both feature representation and fusion of multimodal information from Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET). To our best knowledge, the previous methods in the literature mostly used hand-crafted features such as cortical thickness, gray matter densities from MRI, or voxel intensities from PET, and then combined these multimodal features by simply concatenating into a long vector or transforming into a higher-dimensional kernel space. In this paper, we propose a novel method for a high-level latent and shared feature representation from neuroimaging modalities via deep learning. Specifically, we use Deep Boltzmann Machine (DBM) 2 , a deep network with a restricted Boltzmann machine as a building block, to find a latent hierar- chical feature representation from a 3D patch, and then devise a systematic method for a joint feature representa- tion from the paired patches of MRI and PET with a multimodal DBM. To validate the effectiveness of the proposed method, we performed experiments on ADNI dataset and compared with the state-of-the-art methods. In three binary classification problems of AD vs. healthy Normal Control (NC), MCI vs. NC, and MCI converter vs. MCI non-converter, we obtained the maximal accuracies of 95.35%, 85.67%, and 74.58%, respectively, outperforming the competing methods. By visual inspection of the trained model, we observed that the proposed method could hierarchically discover the complex latent patterns inherent in both MRI and PET.","Heung-Il Suk,Seong-Whan Lee,Dinggang Shen",NeuroImage,2014,,,
401,Exploiting Similarities among Languages for Machine Translation,"Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.","Tomas Mikolov,Quoc V. Le,Ilya Sutskever",arXiv: Computation and Language,2013,,,
402,Cross-domain collaboration recommendation,"Interdisciplinary collaborations have generated huge impact to society. However, it is often hard for researchers to establish such cross-domain collaborations. What are the patterns of cross-domain collaborations? How do those collaborations form? Can we predict this type of collaborations?   Cross-domain collaborations exhibit very different patterns compared to traditional collaborations in the same domain: 1) sparse connection: cross-domain collaborations are rare; 2) complementary expertise: cross-domain collaborators often have different expertise and interest; 3) topic skewness: cross-domain collaboration topics are focused on a subset of topics. All these patterns violate fundamental assumptions of traditional recommendation systems.   In this paper, we analyze the cross-domain collaboration data from research publications and confirm the above patterns. We propose the Cross-domain Topic Learning (CTL) model to address these challenges. For handling sparse connections, CTL consolidates the existing cross-domain collaborations through topic layers instead of at author layers, which alleviates the sparseness issue. For handling complementary expertise, CTL models topic distributions from source and target domains separately, as well as the correlation across domains. For handling topic skewness, CTL only models relevant topics to the cross-domain collaboration.   We compare CTL with several baseline approaches on large publication datasets from different domains. CTL outperforms baselines significantly on multiple recommendation metrics. Beyond accurate recommendation performance, CTL is also insensitive to parameter tuning as confirmed in the sensitivity analysis.","Jie Tang,Sen Wu,Jimeng Sun,Hang Su",,2012,,,
403,"Neural Network Ensembles, Cross Validation, and Active Learning","Learning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity is defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among the networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble generalization error, and how this type of ensemble cross-validation can sometimes improve performance. It is shown how to estimate the optimal weights of the ensemble members using unlabeled data. By a generalization of query by committee, it is finally shown how the ambiguity can be used to select new training data to be labeled in an active learning scheme.","Anders Krogh,Jesper Vedelsby",,1994,,,
404,Principal component analysis,"Principal component analysis PCA is a multivariate technique that analyzes a data table in which observations are described by several inter-correlated quantitative dependent variables. Its goal is to extract the important information from the table, to represent it as a set of new orthogonal variables called principal components, and to display the pattern of similarity of the observations and of the variables as points in maps. The quality of the PCA model can be evaluated using cross-validation techniques such as the bootstrap and the jackknife. PCA can be generalized as correspondence analysis CA in order to handle qualitative variables and as multiple factor analysis MFA in order to handle heterogeneous sets of variables. Mathematically, PCA depends upon the eigen-decomposition of positive semi-definite matrices and upon the singular value decomposition SVD of rectangular matrices. Copyright © 2010 John Wiley & Sons, Inc.","Hervé Abdi,Lynne J. Williams",Wiley Interdisciplinary Reviews: Computational Statistics,2010,,,
405,Competitive algorithms for VWAP and limit order trading,We introduce new online models for two important aspectsof modern financial markets: Volume Weighted Average Pricetrading and limit order books. We provide an extensivestudy of competitive algorithms in these models and relatethem to earlier online algorithms for stock trading.,"Sham M. Kakade,Michael Kearns,Yishay Mansour,Luis E. Ortiz",,2004,,,
406,Dark Trading and Price Discovery,"Regulators globally are concerned that dark trading harms price discovery. We show that dark trades are less informed than lit trades. High levels of dark trading increase adverse selection risk on the lit exchange by increasing the concentration of informed traders. Using both high- and low-frequency measures of informational efficiency we find that low levels of non-block dark trading are benign or even beneficial for informational efficiency, but high levels are harmful. In contrast, we find no evidence that block trades in the dark impede price discovery.","Carole Comerton-Forde,Tālis J. Putniņš",Journal of Financial Economics,2015,,,
407,New Concepts in Technical Trading Systems,,J. Welles Wilder,,1978,,,
408,Context-aware recommender systems,"The importance of contextual information has been recognized by researchers and practitioners in many disciplines, including e-commerce personalization, information retrieval, ubiquitous and mobile computing, data mining, marketing, and management. While a substantial amount of research has already been performed in the area of recommender systems, most existing approaches focus on recommending the most relevant items to users without taking into account any additional contextual information, such as time, location, or the company of other people (e.g., for watching movies or dining out). In this chapter we argue that relevant contextual information does matter in recommender systems and that it is important to take this information into account when providing recommendations. We discuss the general notion of context and how it can be modeled in recommender systems. Furthermore, we introduce three different algorithmic paradigms – contextual prefiltering, post-filtering, and modeling – for incorporating contextual information into the recommendation process, discuss the possibilities of combining several contextaware recommendation techniques into a single unifying approach, and provide a case study of one such combined approach. Finally, we present additional capabilities for context-aware recommenders and discuss important and promising directions for future research.","Gediminas Adomavicius,Alexander Tuzhilin",,2008,,,
409,Practical Bayesian Optimization of Machine Learning Algorithms,"The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a ""black art"" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.","Jasper Snoek,Hugo Larochelle,Ryan P. Adams",,2012,,,
410,word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method.,"The word2vec software of Tomas Mikolov and colleagues (this https URL ) has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations. 
This note is an attempt to explain equation (4) (negative sampling) in ""Distributed Representations of Words and Phrases and their Compositionality"" by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.","Yoav Goldberg,Omer Levy",arXiv: Computation and Language,2014,,,
411,EFFICIENT CAPITAL MARKETS: A REVIEW OF THEORY AND EMPIRICAL WORK*,"Efficient Capital Markets: A Review of Theory and Empirical Work Author(s): Eugene F. Fama Source: The Journal of Finance, Vol. 25, No. 2, Papers and Proceedings of the Twenty-Eighth Annual Meeting of the American Finance Association New York, N.Y. December, 28-30, 1969 (May, 1970), pp. 383-417 Published by: Blackwell Publishing for the American Finance Association Stable URL: http://www.jstor.org/stable/2325486 Accessed: 30/03/2010 21:28",Eugene F. Fama,Journal of Finance,1970,,,
412,On the Sensitivity of Mean-Variance-Efficient Portfolios to Changes in Asset Means: Some Analytical and Computational Results,"This paper investigates the sensitivity of mean-variance(MV)-efficient portfolios to changes in the means of individual assets. When only a budget constraint is imposed on the investment problem, the analytical results indicate that an MV-efficient portfolio's weights, mean, and variance can be extremely sensitive to changes in asset means. When nonnegativity constraints are also imposed on the problem, the computational results confirm that a positively weighted MV-efficient portfolio's weights are extremely sensitive to changes in asset means, but the portfolio's returns are not. A surprisingly small increase in the mean of just one asset drives half the securities from the portfolio. Yet the portfolio's expected return and standard deviation are virtually unchanged. Article published by Oxford University Press on behalf of the Society for Financial Studies in its journal, The Review of Financial Studies.","Michael J. Best,Robert R. Grauer",Review of Financial Studies,1991,,,
413,An Electronic Market-Maker,"This paper presents an adaptive learning model for market-making under the reinforcement learning framework. Reinforcement learning is a learning technique in which agents aim to maximize the long-term accumulated rewards. No knowledge of the market environment, such as the order arrival or price process, is assumed. Instead, the agent learns from realtime market experience and develops explicit market-making strategies, achieving multiple objectives including the maximizing of profits and minimization of the bid-ask spread. The simulation results show initial success in bringing learning techniques to building marketmaking algorithms. This report describes research done within the Center for Biological and Computational Learning in the Department of Brain and Cognitive Sciences and in the Artificial Intelligence Laboratory at the Massachusetts Institute of Technology. This research was sponsored by grants from: Office of Naval Research under contract No. N00014-93-1-3085, Office of Naval Research (DARPA) under contract No. N00014-00-1-0907, National Science Foundation (ITR) under contract No. IIS-0085836, National Science Foundation (KDI) under contract No. DMS-9872936, and National Science Foundation under contract No. IIS-9800032 This research was partially funded by the Center for e-Business (MIT). Additional support was provided by: Central Research Institute of Electric Power Industry, Eastman Kodak Company, DaimlerChrysler AG, Compaq, Honda R&D Co., Ltd., Komatsu Ltd., Merrill-Lynch, NEC Fund, Nippon Telegraph & Telephone, Siemens Corporate Research, Inc., and The Whitaker Foundation.","Nicholas Tung Chan,Christian R. Shelton",,2001,,,
414,Constructing diverse classifier ensembles using artificial training examples,"Ensemble methods like bagging and boosting that combine the decisions of multiple hypotheses are some of the strongest existing machine learning methods. The diversity of the members of an ensemble is known to be an important factor in determining its generalization error. This paper presents a new method for generating ensembles that directly constructs diverse hypotheses using additional artificially-constructed training examples. The technique is a simple, general metalearner that can use any strong learner as a base classifier to build diverse committees. Experimental results using decision-tree induction as a base learner demonstrate that this approach consistently achieves higher predictive accuracy than both the base classifier and bagging (whereas boosting can occasionally decrease accuracy), and also obtains higher accuracy than boosting early in the learning curve when training data is limited.","Prem Melville,Raymond J. Mooney",,2003,,,
415,Adaptive power management using reinforcement learning,"System level power management must consider the uncertainty and variability that comes from the environment, the application and the hardware. A robust power management technique must be able to learn the optimal decision from past history and improve itself as the environment changes. This paper presents a novel online power management technique based on model-free constrained reinforcement learning (RL). It learns the best power management policy that gives the minimum power consumption for a given performance constraint without any prior information of workload. Compared with existing machine learning based power management techniques, the RL based learning is capable of exploring the trade-off in the power-performance design space and converging to a better power management policy. Experimental results show that the proposed RL based power management achieves 24% and 3% reduction in power and latency respectively comparing to the existing expert based power management.","Ying Tan,Wei Liu,Qinru Qiu",,2009,,,
416,Neural network ensembles,Several means for improving the performance and training of neural networks for classification are proposed. Crossvalidation is used as a tool for optimizing network parameters and architecture. It is shown that the remaining residual generalization error can be reduced by invoking ensembles of similar networks. >,"Lars Kai Hansen,Peter Salamon",IEEE Transactions on Pattern Analysis and Machine Intelligence,1990,,,
417,Software agents and market (in) efficiency: a human trader experiment,"This paper studies how software agents influence the market behavior of human traders. Software agents with a passive arbitrage-seeking strategy are introduced in a double auction market experiment with human subjects in the laboratory. As a treatment variable, the influence of information on the existence of software agents is investigated. We found that common knowledge about the presence of software agents triggers more efficient market prices when the programmed strategy was employed, whereas an effect of the information condition on behavioral variables could not be observed. When controlling for information on software agents' participation, the introduction of software agents results in lower market efficiency","Jens Grossklags,Carsten Schmidt",,2006,,,
418,Semi-Supervised Learning Literature Survey,,Xiaojin Zhu,,2005,,,
419,An MDP-Based Recommender System,"Typical recommender systems adopt a static view of the recommendation process and treat it as a prediction problem. We argue that it is more appropriate to view the problem of generating recommendations as a sequential optimization problem and, consequently, that Markov decision processes (MDPs) provide a more appropriate model for recommender systems. MDPs introduce two benefits: they take into account the long-term effects of each recommendation and the expected value of each recommendation. To succeed in practice, an MDP-based recommender system must employ a strong initial model, must be solvable quickly, and should not consume too much memory. In this paper, we describe our particular MDP model, its initialization using a predictive model, the solution and update algorithm, and its actual performance on a commercial site. We also describe the particular predictive model we used which outperforms previous models. Our system is one of a small number of commercially deployed recommender systems. As far as we know, it is the first to report experimental analysis conducted on a real commercial site. These results validate the commercial value of recommender systems, and in particular, of our MDP-based approach.","Guy Shani,David Heckerman,Ronen I. Brafman",Journal of Machine Learning Research,2005,,,
420,Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval,"Relevance feedback schemes based on support vector machines (SVM) have been widely used in content-based image retrieval (CBIR). However, the performance of SVM-based relevance feedback is often poor when the number of labeled positive feedback samples is small. This is mainly due to three reasons: 1) an SVM classifier is unstable on a small-sized training set, 2) SVM's optimal hyperplane may be biased when the positive feedback samples are much less than the negative feedback samples, and 3) overfitting happens because the number of feature dimensions is much higher than the size of the training set. In this paper, we develop a mechanism to overcome these problems. To address the first two problems, we propose an asymmetric bagging-based SVM (AB-SVM). For the third problem, we combine the random subspace method and SVM for relevance feedback, which is named random subspace SVM (RS-SVM). Finally, by integrating AB-SVM and RS-SVM, an asymmetric bagging and random subspace SVM (ABRS-SVM) is built to solve these three problems and further improve the relevance feedback performance","Dacheng Tao,Xiaoou Tang,Xuelong Li,Xindong Wu",IEEE Transactions on Pattern Analysis and Machine Intelligence,2006,,,
421,Semi-supervised ensemble classification in subspaces,"Graph-based semi-supervised classification depends on a well-structured graph. However, it is difficult to construct a graph that faithfully reflects the underlying structure of data distribution, especially for data with a high dimensional representation. In this paper, we focus on graph construction and propose a novel method called semi-supervised ensemble classification in subspaces, SSEC in short. Unlike traditional methods that execute graph-based semi-supervised classification in the original space, SSEC performs semi-supervised linear classification in subspaces. More specifically, SSEC first divides the original feature space into several disjoint feature subspaces. Then, it constructs a neighborhood graph in each subspace, and trains a semi-supervised linear classifier on this graph, which will serve as the base classifier in an ensemble. Finally, SSEC combines the obtained base classifiers into an ensemble classifier using the majority-voting rule. Experimental results on facial images classification show that SSEC not only has higher classification accuracy than the competitive methods, but also can be effective in a wide range of values of input parameters.","Guoxian Yu,Guoji Zhang,Zhiwen Yu,Carlotta Domeniconi,Jane You,Guoqiang Han",Applied Soft Computing,2012,,,
422,An Improved Moving Average Technical Trading Rule,"This paper proposes a modified version of the widely used price and moving average cross-over trading strategies. The suggested approach (presented in its ‘long only’ version) is a combination of cross-over ‘buy’ signals and a dynamic threshold value which acts as a dynamic trailing stop. The trading behaviour and performance from this modified strategy are different from the standard approach with results showing that, on average, the proposed modification increases the cumulative return and the Sharpe ratio of the investor while exhibiting smaller maximum drawdown and smaller drawdown duration than the standard strategy.","Fotis Papailias,Dimitrios D. Thomakos",Physica A-statistical Mechanics and Its Applications,2015,,,
423,The Impact of Diversity on Online Ensemble Learning in the Presence of Concept Drift,"Online learning algorithms often have to operate in the presence of concept drift (i.e., the concepts to be learned can change with time). This paper presents a new categorization for concept drift, separating drifts according to different criteria into mutually exclusive and nonheterogeneous categories. Moreover, although ensembles of learning machines have been used to learn in the presence of concept drift, there has been no deep study of why they can be helpful for that and which of their features can contribute or not for that. As diversity is one of these features, we present a diversity analysis in the presence of different types of drifts. We show that, before the drift, ensembles with less diversity obtain lower test errors. On the other hand, it is a good strategy to maintain highly diverse ensembles to obtain lower test errors shortly after the drift independent on the type of drift, even though high diversity is more important for more severe drifts. Longer after the drift, high diversity becomes less important. Diversity by itself can help to reduce the initial increase in error caused by a drift, but does not provide the faster recovery from drifts in long-term.","Leandro L. Minku,A.P. White,Allan P. White,A.P. White,Xin Yao",IEEE Transactions on Knowledge and Data Engineering,2010,,,
424,Deep Convex Net: A Scalable Architecture for Speech Pattern Classification,"We recently developed context-dependent DNN-HMM (DeepNeural-Net/Hidden-Markov-Model) for large-vocabulary speech recognition. While achieving impressive recognition error rate reduction, we face the insurmountable problem of scalability in dealing with virtually unlimited amount of training data available nowadays. To overcome the scalability challenge, we have designed the deep convex network (DCN) architecture. The learning problem in DCN is convex within each module. Additional structure-exploited fine tuning further improves the quality of DCN. The full learning in DCN is batch-mode based instead of stochastic, naturally lending it amenable to parallel training that can be distributed over many machines. Experimental results on both MNIST and TIMIT tasks evaluated thus far demonstrate superior performance of DCN over the DBN (Deep Belief Network) counterpart that forms the basis of the DNN. The superiority is reflected not only in training scalability and CPU-only computation, but more importantly in classification accuracy in both tasks.","Dong Yu,Li Deng,Dong Yu,Li Deng,Dong Yu,Dong Yu,Dong Yu,Li Deng",,2011,,,
425,PILCO: A Model-Based and Data-Efficient Approach to Policy Search,"In this paper, we introduce PILCO, a practical, data-efficient model-based policy search method. PILCO reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, PILCO can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks.","Marc Peter Deisenroth,Carl Edward Rasmussen",,2011,,,
426,Web Page Personalization Based on Weighted Association Rules,"Web personalization is the process of customizing a web site to the needs of each specific user or set of users, taking advantage of the knowledge acquired through the analysis of the user’s navigational behavior. Personalized recommendation by predicting user-browsing behavior using association-mining technology has gained much attention in web personalization research area. However, the resulting association patterns did not perform well in prediction of future browsing patterns due to the low matching rate of the resulting rules and users’ browsing behavior. In this paper, we extend the traditional association rule problem by allowing a weight to be associated with each item in a transaction to reflect the interest/intensity of each item within the transaction. In turn, this provides us with an opportunity to associate a weight parameter with each item in a resulting association rule. We assign a significant weight to each page based on the time spent by user on each page and visiting frequency of each page, taking in to account the degree of interest instead of binary weighting. We present new personalized recommendation method base on the proposed weighted association-mining technique. We show, through experimentation on real data set that this approach results in more objective and representative predictions and shows a significant improvement in the recommendation effectiveness in comparison to the traditional association rule approaches.","Rana Forsati,Mohammad Reza Meybodi,A. Ghari Neiat",,2009,,,
427,BPR: Bayesian personalized ranking from implicit feedback,"Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive k-nearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.","Steffen Rendle,Christoph Freudenthaler,Zeno Gantner,Lars Schmidt-Thieme",,2009,,,
428,Multi-column deep neural networks for image classification,"Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.","Dan Ciregan,Dan Ciresan,Ueli Meier,Jürgen Schmidhuber",,2012,,,
429,Limits on the majority vote accuracy in classifier fusion,"We derive upper and lower limits on the majority vote accuracy with respect to individual accuracy p, the number of classifiers in the pool (L), and the pairwise dependence between classifiers, measured by Yule’s Q statistic. Independence between individual classifiers is typically viewed as an asset in classifier fusion. We show that the majority vote with dependent classifiers can potentially offer a dramatic improvement both over independent classifiers and over an individual classifier with accuracy p. A functional relationship between the limits and the pairwise dependence Q is derived. Two patterns of the joint distribution for classifier outputs (correct/incorrect) are identified to derive the limits: the pattern of success and the pattern of failure. The results support the intuition that negative pairwise dependence is beneficial although not straightforwardly related to the accuracy. The pattern of success showed that for the highest improvement over p, all pairs of classifiers in the pool should have the same negative dependence.","Ludmila I. Kuncheva,Christopher J. Whitaker,Catherine A. Shipp,Robert P. W. Duin",Pattern Analysis and Applications,2003,,,
430,RL-DOT: A Reinforcement Learning NPC Team for Playing Domination Games,"In this paper, we describe the design of reinforcement-learning-based domination team (RL-DOT), a nonplayer character (NPC) team for playing Unreal Tournament (UT) Domination games. In RL-DOT, there is a commander NPC and several soldier NPCs. The running process of RL-DOT consists of several decision cycles. In each decision cycle, the commander NPC makes a decision of troop distribution and, according to that decision, sends action orders to other soldier NPCs. Each soldier NPC tries to accomplish its task in a goal-directed way, i.e., decomposing the final ultimate task (attacking or defending a domination point) into basic actions (such as running and shooting) that are directly supported by UT application programming interfaces (APIs). We use a Q-learning-style algorithm to learn the optimal decision-making policy. We carefully choose some opponent policies for our illustrative experiments. In these experiments, RL-DOT shows a distinct learning characteristic, which illustrates its efficiency in playing UT Domination games.","Hao Wang,Hao Wang,Hao Wang,Yang Gao,Yang Gao,Xingguo Chen",IEEE Transactions on Computational Intelligence and AI in Games,2010,,,
431,Mean-Variance Portfolio Selection with Random Parameters in a Complete Market,"This paper concerns the continuous-time, mean-variance portfolio selection problem in a complete market with random interest rate, appreciation rates, and volatility coef.cients. The problem is tackled using the results of stochastic linear-quadratic (LQ) optimal control and backward stochastic differential equations (BSDEs), two theories that have been extensively studied and developed in recent years. Specifically, the mean-variance problem is formulated as a linearly constrained stochastic LQ control problem. Solvability of this LQ problem is reduced, in turn, to proving global solvability of a stochastic Riccati equation. The proof of existence and uniqueness of this Riccati equation, which is a fully nonlinear and singular BSDE with random coefficients, is interesting in its own right and relies heavily on the structural properties of the equation. Efficient investment strategies as well as the mean-variance efficient frontier are then analytically derived in terms of the solution of this equation. In particular, it is demonstrated that the efficient frontier in the mean-standard deviation diagram is still a straight line or, equivalently, risk-free investment is still possible, even when the interest rate is random. Finally, a version of the Mutual Fund Theorem is presented.","Andrew Lim,Andrew E. B. Lim,Xun Yu Zhou",Mathematics of Operations Research,2002,,,
432,ZIP60: an enhanced variant of the ZIP trading algorithm,"The ""ZIP"" adaptive automated trading algorithm has been shown to outperform human traders in experimental studies of continuous double auction (CDA) markets populated by mixtures of human and ""software robot"" traders. This paper introduces a more sophisticated version of the ZIP algorithm, called ZIP60 because it requires the values of 60 parameters to be set correctly. ZIP60 is shown here to produce significantly improved results in comparison to the original ZIP algorithm (called ""ZIP8"" hereafter). A genetic algorithm is used to search the 60-dimensional parameter space, and it finds parameter values that yield ZIP60 traders with mean scores that are significantly better than those of ZIP8s. Principal component analysis of the best evolved ZIP60 parameter-sets establishes that no ZIP8 solutions are embedded in the 60-d space. Moreover, the results presented here cast doubt on earlier ZIP8 results concerning the evolution of new hybrid auction mechanisms that appeared to be improvements on the CDA.","D. Cliff,Dave Cliff",,2006,,,
433,An ensemble approach for incremental learning in nonstationary environments,"We describe an ensemble of classifiers based algorithm for incremental learning in nonstationary environments. In this formulation, we assume that the learner is presented with a series of training datasets, each of which is drawn from a different snapshot of a distribution that is drifting at an unknown rate. Furthermore, we assume that the algorithm must learn the new environment in an incremental manner, that is, without having access to previously available data. Instead of a time window over incoming instances, or an aged based forgetting - as used by most ensemble based nonstationary learning algorithms - a strategic weighting mechanism is employed that tracks the classifiers' performances over drifting environments to determine appropriate voting weights. Specifically, the proposed approach generates a single classifier for each dataset that becomes available, and then combines them through a dynamically modified weighted majority voting, where the voting weights themselves are computed as weighted averages of classifiers' individual performances over all environments. We describe the implementation details of this approach, as well as its initial results on simulated non-stationary environments.","M.D. Muhlbaier,Robi Polikar",,2007,,,
434,Collaborative filtering recommender systems,"One of the potent personalization technologies powering the adaptive web is collaborative filtering. Collaborative filtering (CF) is the process of filtering or evaluating items through the opinions of other people. CF technology brings together the opinions of large interconnected communities on the web, supporting filtering of substantial quantities of data. In this chapter we introduce the core concepts of collaborative filtering, its primary uses for users of the adaptive web, the theory and practice of CF algorithms, and design decisions regarding rating systems and acquisition of ratings. We also discuss how to evaluate CF systems, and the evolution of rich interaction interfaces. We close the chapter with discussions of the challenges of privacy particular to a CF recommendation service and important open research questions in the field.","J. Ben Schafer,Dan Frankowski,Jon Herlocker,Shilad Sen",,2007,,,
435,Reinforcement Learning with Factored States and Actions,A novel approximation method is presented for approximating the value function and selecting good actions for Markov decision processes with large state and action spaces. The method approximates state-action values as negative free energies in an undirected graphical model called a product of experts. The model parameters can be learned efficiently because values and derivatives can be efficiently computed for a product of experts. Actions can be found even in large factored action spaces by the use of Markov chain Monte Carlo sampling. Simulation results show that the product of experts approximation can be used to solve large problems. In one simulation it is used to find actions in action spaces of size 240.,"Brian Sallans,Geoffrey E. Hinton",Journal of Machine Learning Research,2004,,,
436,Multi-label classification: An overview,"Multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization, and semantic scene classification. This article introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative experimental results of certain multilabel classification methods. It also contributes the definition of concepts for the quantification of the multi-label nature of a data set.","Grigorios Tsoumakas,Ioannis Katakis",International Journal of Data Warehousing and Mining,2007,,,
437,Multiagent Systems: A Survey from a Machine Learning Perspective,"Distributed Artificial Intelligence (DAI) has existed as a subfield of AI for less than two decades. DAI is concerned with systems that consist of multiple independent entities that interact in a domain. Traditionally, DAI has been divided into two sub-disciplines: Distributed Problem Solving (DPS) focuses on the information management aspects of systems with several components working together towards a common goals Multiagent Systems (MAS) deals with behavior management in collections of several independent entities, or agents. This survey of MAS is intended to serve as an introduction to the field and as an organizational framework. A series of general multiagent scenarios are presented. For each scenario, the issues that arise are described along with a sampling of the techniques that exist to deal with them. The presented techniques are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. When options exist, the techniques presented are biased towards machine learning approaches. Additional opportunities for applying machine learning to MAS are highlighted and robotic soccer is presented as an appropriate test bed for MAS. This survey does not focus exclusively on robotic systems. However, we believe that much of the prior research in non-robotic MAS is relevant to robotic MAS, and we explicitly discuss several robotic MAS, including all of those presented in this issue.","Peter Stone,Manuela Veloso",Autonomous Robots,2000,,,
438,A survey of consensus problems in multi-agent coordination,"As a distributed solution to multi-agent coordination, consensus or agreement problems have been studied extensively in the literature. This paper provides a survey of consensus problems in multi-agent cooperative control with the goal of promoting research in this area. Theoretical results regarding consensus seeking under both time-invariant and dynamically changing information exchange topologies are summarized. Applications of consensus protocols to multiagent coordination are investigated. Future research directions and open problems are also proposed.","Wei Ren,Randal W. Beard,Ella M. Atkins",,2005,,,
439,SMOTE: synthetic minority over-sampling technique,"An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of ""normal"" examples with only a small percentage of ""abnormal"" or ""interesting"" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of oversampling the minority (abnormal)cla ss and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space)tha n only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space)t han varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC)and the ROC convex hull strategy.","Nitesh V. Chawla,Kevin W. Bowyer,Lawrence O. Hall,W. Philip Kegelmeyer",Journal of Artificial Intelligence Research,2002,,,
440,Deep Learning: Methods and Applications,"This monograph provides an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria in mind: (1) expertise or knowledge of the authors; (2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and (3) the application areas that have the potential to be impacted significantly by deep learning and that have been experiencing research growth, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.","Li Deng,Dong Yu",,2014,,,
441,AI-based world behaviour for emergent narratives,"Research in Interactive Narrative has developed new approaches to the behaviour of virtual actors, but has dedicated little attention to the physical behaviour of the environment in which the action takes place. In this paper, we describe a method supporting the AI-based simulation of object behaviour, so that interactive narrative can feature the physical environment inhabited by the player character as an ""actor"". The prototype we describe has been developed on top of the Unreal Tournament game engine and relies on a ""causal engine"", which essentially bypasses the native Physics engine to generate alternative consequences to player interventions. It operates using a small depth-bound planning system which determines the most appropriate object behaviours following player interaction. The prototype is illustrated through a test application called ""Death Kitchen"", freely inspired from various thriller and horror films, in which the kitchen is plotting against the player character to generate domestic accidents.","Jean-Luc Lugrin,Marc Cavazza",,2006,,,
442,Two Novel On-policy Reinforcement Learning Algorithms based on TD(λ)-methods,"This paper describes two novel on-policy reinforcement learning algorithms, named QV(λ)-learning and the actor critic learning automaton (ACLA). Both algorithms learn a state value-function using TD(λ)-methods. The difference between the algorithms is that QV-learning uses the learned value function and a form of Q-learning to learn Q-values, whereas ACLA uses the value function and a learning automaton-like update rule to update the actor. We describe several possible advantages of these methods compared to other value-function-based reinforcement learning algorithms such as Q-learning, Sarsa, and conventional actor-critic methods. Experiments are performed on (1) small, (2) large, (3) partially observable, and (4) dynamic maze problems with tabular and neural network value-function representations, and on the mountain car problem. The overall results show that the two novel algorithms can outperform previously known reinforcement learning algorithms","Marco Wiering,H. van Hasselt",,2007,,,
443,Distributed Representations of Words and Phrases and their Compositionality,"The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.

An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of ""Canada"" and ""Air"" cannot be easily combined to obtain ""Air Canada"". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.","Tomas Mikolov,Ilya Sutskever,Kai Chen,Kai Chen,Greg S. Corrado,Jeffrey Dean",,2013,,,
444,Strategic bidding in continuous double auctions,"In this paper, we describe a novel bidding strategy that autonomous trading agents can use to participate in Continuous Double Auctions (CDAs). Our strategy is based on both short and long-term learning that allows such agents to adapt their bidding behaviour to be efficient in a wide variety of environments. For the short-term learning, the agent updates the aggressiveness of its bidding behaviour (more aggressive means it will trade off profit to improve its chance of transacting, less aggressive that it targets more profitable transactions and is willing to trade off its chance of transacting to achieve them) based on market information observed after any bid or ask appears in the market. The long-term learning then determines how this aggressiveness factor influences an agent's choice of which bids or asks to submit in the market, and is based on market information observed after every transaction (successfully matched bid and ask). The principal motivation for the short-term learning is to enable the agent to immediately respond to market fluctuations, while for the long-term learning it is to adapt to broader trends in the way in which the market demand and supply changes over time. We benchmark our strategy against the current state of the art (ZIP and GDX) and show that it outperforms these benchmarks in both static and dynamic environments. This is true both when the population is homogeneous (where the increase in efficiency is up to 5.2%) and heterogeneous (in which case there is a 0.85 probability of our strategy being adopted in a two-population evolutionary game theoretic analysis).","Perukrishnen Vytelingum,Dave Cliff,Nicholas R. Jennings",Artificial Intelligence,2008,,,
445,Multimodal freight transportation planning : a literature review,"Multimodal transportation offers an advanced platform for more efficient, reliable, flexible, and sustainable freight transportation. Planning such a complicated system provides interesting areas in Operations Research. This paper presents a structured overview of the multimodal transportation literature from 2005 onward. We focus on the traditional strategic, tactical, and operational levels of planning, where we present the relevant models and their developed solution techniques. We conclude our review paper with an outlook to future research directions.","M Maryam SteadieSeifi,Nico Nico Dellaert,Nico Dellaert,Wpm Wim Nuijten,van T Tom Woensel,R Rasa Raoufi",European Journal of Operational Research,2014,,,
446,Policy Gradient Methods for Reinforcement Learning with Function Approximation,"Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.","Richard S. Sutton,Richard S. Sutton,David McAllester,Satinder Singh,Yishay Mansour",,1999,,,
447,Review: zero intelligence in economics and finance,"This paper reviews the Zero Intelligence (ZI) methodology for investigating markets. This approach models individual traders, operating within a market mechanism, who behave without strategy, in order to determine the impact of the market mechanism and consequently the effect of trader behaviour. The paper considers the major contributions and models within this area from both the economics and finance communities before examining the strengths and weaknesses of this methodology.",Daniel Ladley,Knowledge Engineering Review,2012,,,
448,Ensemble-based classifiers,"The idea of ensemble methodology is to build a predictive model by integrating multiple models. It is well-known that ensemble methods can be used for improving prediction performance. Researchers from various disciplines such as statistics and AI considered the use of ensemble methodology. This paper, review existing ensemble techniques and can be served as a tutorial for practitioners who are interested in building ensemble based systems.",Lior Rokach,Artificial Intelligence Review,2010,,,
449,Importance of input data normalization for the application of neural networks to complex industrial problems,"Recent advances in artificial intelligence have allowed the application of such technologies in real industrial problems. We have studied the application of backpropagation neural networks to several problems of estimation and identification in nuclear power plants. These problems often have been reported to be very time-consuming in the training phase. Among the different approaches suggested to ease the backpropagation training process, input data pretreatment has been pointed out, although no specific procedure has been proposed. We have found that input data normalization with certain criteria, prior to a training process, is crucial to obtain good results as well as to fasten significantly the calculations. This paper shows how data normalization affects the performance error of parameter estimators trained to predict the value of several variables of a PWR nuclear power plant. The criteria needed to accomplish such data normalization are also described.","J. Sola,J. Sevilla",IEEE Transactions on Nuclear Science,1997,,,
450,Neural-network-based fuzzy logic control and decision system,"A general neural-network (connectionist) model for fuzzy logic control and decision systems is proposed. This connectionist model, in the form of feedforward multilayer net, combines the idea of fuzzy logic controller and neural-network structure and learning abilities into an integrated neural-network-based fuzzy logic control and decision system. A fuzzy logic control decision network is constructed automatically by learning the training examples itself. By combining both unsupervised (self-organized) and supervised learning schemes, the learning speed converges much faster than the original backpropagation learning algorithm. The connectionist structure avoids the rule-matching time of the inference engine in the traditional fuzzy logic system. Two examples are presented to illustrate the performance and applicability of the proposed model. >","Chin-Teng Lin,C.S.G. Lee,Chiang Lee",IEEE Transactions on Computers,1991,,,
451,Actor-critic algorithms,"Many complex decision making problems like scheduling in manufacturing systems, portfolio management in finance, admission control in communication networks etc., with clear and precise objectives, can be formulated as stochastic dynamic programming problems in which the objective of decision making is to maximize a single “overall” reward. In these formulations, finding an optimal decision policy involves computing a certain “value function” which assigns to each state the optimal reward one would obtain if the system was started from that state. This function then naturally prescribes the optimal policy, which is to take decisions that drive the system to states with maximum value. 
For many practical problems, the computation of the exact value function is intractable, analytically and numerically, due to the enormous size of the state space. Therefore one has to resort to one of the following approximation methods to find a good sub-optimal policy: (1)Approximate the value function. (2)Restrict the search for a good policy to a smaller family of policies. 
In this thesis, we propose and study actor-critic algorithms which combine the above two approaches with simulation to find the best policy among a parameterized class of policies. Actor-critic algorithms have two learning units: an actor and a critic. An actor is a decision maker with a tunable parameter. A critic is a function approximator. The critic tries to approximate the value function of the policy used by the actor, and the actor in turn tries to improve its policy based on the current approximation provided by the critic. Furthermore, the critic evolves on a faster time-scale than the actor. 
We propose several variants of actor-critic algorithms. In all the variants, the critic uses Temporal Difference (TD) learning with linear function approximation. Some of the variants are inspired by a new geometric interpretation of the formula for the gradient of the overall reward with respect to the actor parameters. This interpretation suggests a natural set of basis functions for the critic, determined by the family of policies parameterized by the actor's parameters. We concentrate on the average expected reward criterion but we also show how the algorithms can be modified for other objective criteria. We prove convergence of the algorithms for problems with general (finite, countable, or continuous) state and decision spaces. 
To compute the rate of convergence (ROC) of our algorithms, we develop a general theory of the ROC of two-time-scale algorithms and we apply it to study our algorithms. In the process, we study the ROC of TD learning and compare it with related methods such as Least Squares TD (LSTD). We study the effect of the basis functions used for linear function approximation on the ROC of TD. We also show that the ROC of actor-critic algorithms does not depend on the actual basis functions used in the critic but depends only on the subspace spanned by them and study this dependence. 
Finally, we compare the performance of our algorithms with other algorithms that optimize over a parameterized family of policies. We show that when only the “natural” basis functions are used for the critic, the rate of convergence of the actor critic algorithms is the same as that of certain stochastic gradient descent algorithms. However, with appropriate additional basis functions for the critic, we show that our algorithms outperform the existing ones in terms of ROC. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)","Vijay R. Konda,John N. Tsitsiklis",,2002,,,
452,NONPARAMETRIC KERNEL-BASED SEQUENTIAL INVESTMENT STRATEGIES,"The purpose of this paper is to introduce sequential investment strategies that guarantee an optimal rate of growth of the capital, under minimal assumptions on the behavior of the market. The new strategies are analyzed both theoretically and empirically. The theoretical results show that the asymptotic rate of growth matches the optimal one that one could achieve with a full knowledge of the statistical properties of the underlying process generating the market, under the only assumption that the market is stationary and ergodic. The empirical results show that the performance of the proposed investment strategies measured on past nyse and currency exchange data is solid, and sometimes even spectacular.","László Györfi,Gábor Lugosi,Frederic Udina",Mathematical Finance,2006,,,
453,"Comparative study of stock trend prediction using time delay, recurrent and probabilistic neural networks","Three networks are compared for low false alarm stock trend predictions. Short-term trends, particularly attractive for neural network analysis, can be used profitably in scenarios such as option trading, but only with significant risk. Therefore, we focus on limiting false alarms, which improves the risk/reward ratio by preventing losses. To predict stock trends, we exploit time delay, recurrent, and probabilistic neural networks (TDNN, RNN, and PNN, respectively), utilizing conjugate gradient and multistream extended Kalman filter training for TDNN and RNN. We also discuss different predictability analysis techniques and perform an analysis of predictability based on a history of daily closing price. Our results indicate that all the networks are feasible, the primary preference being one of convenience.","E.W. Saad,Danil V. Prokhorov,Donald C. Wunsch",IEEE Transactions on Neural Networks,1998,,,
454,MuJoCo: A physics engine for model-based control,"We describe a new physics engine tailored to model-based control. Multi-joint dynamics are represented in generalized coordinates and computed via recursive algorithms. Contact responses are computed via efficient new algorithms we have developed, based on the modern velocity-stepping approach which avoids the difficulties with spring-dampers. Models are specified using either a high-level C++ API or an intuitive XML file format. A built-in compiler transforms the user model into an optimized data structure used for runtime computation. The engine can compute both forward and inverse dynamics. The latter are well-defined even in the presence of contacts and equality constraints. The model can include tendon wrapping as well as actuator activation states (e.g. pneumatic cylinders or muscles). To facilitate optimal control applications and in particular sampling and finite differencing, the dynamics can be evaluated for different states and controls in parallel. Around 400,000 dynamics evaluations per second are possible on a 12-core machine, for a 3D homanoid with 18 dofs and 6 active contacts. We have already used the engine in a number of control applications. It will soon be made publicly available.","Emanuel Todorov,Tom Erez,Yuval Tassa",,2012,,,
455,Amazon.com recommendations: item-to-item collaborative filtering,"Recommendation algorithms are best known for their use on e-commerce Web sites, where they use input about a customer's interests to generate a list of recommended items. Many applications use only the items that customers purchase and explicitly rate to represent their interests, but they can also use other attributes, including items viewed, demographic data, subject interests, and favorite artists. At Amazon.com, we use recommendation algorithms to personalize the online store for each customer. The store radically changes based on customer interests, showing programming titles to a software engineer and baby toys to a new mother. There are three common approaches to solving the recommendation problem: traditional collaborative filtering, cluster models, and search-based methods. Here, we compare these methods with our algorithm, which we call item-to-item collaborative filtering. Unlike traditional collaborative filtering, our algorithm's online computation scales independently of the number of customers and number of items in the product catalog. Our algorithm produces recommendations in real-time, scales to massive data sets, and generates high quality recommendations.","Greg Linden,Greg Linden,Brent R. Smith,Jeremy C. York,J. York",IEEE Internet Computing,2003,,,
456,Building Personalized Recommendation System in E-commerce Using Association Rule-based Mining and Classification,"Due to the convenience of Internet, people can search for whatever information they need and buy whatever they want on the web. In the age of E- Commerce, it is difficult to provide support for customers to find the most valuable products that match their heterogeneous needs. Traditional approaches to this so-called personalization problem adopt pre-defined formats to describe the customer requirements. This always leads to distortion in eliciting requirement information and thus inaccurate recommendations. In this paper,we propose a personalized recommendation system using association rule mining and classification in e-commerce. Customer requirements are extracted from text documents and transformed into a set of significant phrases. Allowing the transformed transaction records, a set of association rule are mined from database using Apriori algorithm. CBA-CB algorithm is applied to produce the best rules out of the whole set of rules. The best classifiers are then generated after the test and validation of those rules, aimed to predict the item labels for new customer requirements and thus assigns the corresponding class labels to the customer. The system analysis and design of the proposed recommendation system as well as the implementation of prototype are also presented.",Zhang Xi-zheng,,2007,,,
457,Reinforcement learning for optimized trade execution,"We present the first large-scale empirical application of reinforcement learning to the important problem of optimized trade execution in modern financial markets. Our experiments are based on 1.5 years of millisecond time-scale limit order data from NASDAQ, and demonstrate the promise of reinforcement learning methods to market microstructure problems. Our learning algorithm introduces and exploits a natural ""low-impact"" factorization of the state space.","Yuriy Nevmyvaka,Yi Feng,Michael Kearns",,2006,,,
458,Passive Investment Strategies and Efficient Markets,"This paper presents the case for and the evidence in favour of passive investment strategies and examines the major criticisms of the technique. I conclude that the evidence strongly supports passive investment management in all markets—small–capitalisation stocks as well as large–capitalisation equities, US markets as well as international markets, and bonds as well as stocks. Recent attacks on the efficient market hypothesis do not weaken the case for indexing.",Burton G. Malkiel,European Financial Management,2003,,,
459,Learn $^{++}$ .NC: Combining Ensemble of Classifiers With Dynamically Weighted Consult-and-Vote for Efficient Incremental Learning of New Classes,"We have previously introduced an incremental learning algorithm Learn++, which learns novel information from consecutive data sets by generating an ensemble of classifiers with each data set, and combining them by weighted majority voting. However, Learn++ suffers from an inherent ldquooutvotingrdquo problem when asked to learn a new class omeganew introduced by a subsequent data set, as earlier classifiers not trained on this class are guaranteed to misclassify omeganew instances. The collective votes of earlier classifiers, for an inevitably incorrect decision, then outweigh the votes of the new classifiers' correct decision on omeganew instances-until there are enough new classifiers to counteract the unfair outvoting. This forces Learn++ to generate an unnecessarily large number of classifiers. This paper describes Learn++ .NC, specifically designed for efficient incremental learning of multiple new classes using significantly fewer classifiers. To do so, Learn ++.NC introduces dynamically weighted consult and vote (DW-CAV) , a novel voting mechanism for combining classifiers: individual classifiers consult with each other to determine which ones are most qualified to classify a given instance, and decide how much weight, if any, each classifier's decision should carry. Experiments on real-world problems indicate that the new algorithm performs remarkably well with substantially fewer classifiers, not only as compared to its predecessor Learn++, but also as compared to several other algorithms recently proposed for similar problems.","M.D. Muhlbaier,Apostolos Topalis,Robi Polikar",IEEE Transactions on Neural Networks,2009,,,
460,Recommendations for the long tail by term-query graph,"We define a new approach to the query recommendation problem. In particular, our main goal is to design a model enabling the generation of query suggestions also for rare and previously unseen queries. In other words we are targeting queries in the long tail. The model is based on a graph having two sets of nodes: Term nodes, and Query nodes. The graph induces a Markov chain on which a generic random walker starts from a subset of Term nodes, moves along Query nodes, and restarts (with a given probability) only from the same initial subset of Term nodes. Computing the stationary distribution of such a Markov chain is equivalent to extracting the so-called Center-piece Subgraph from the graph associated with the Markov chain itself. Given a query, we extract its terms and we set the restart subset to this term set. Therefore, we do not require a query to have been previously observed for the recommending model to be able to generate suggestions.","Francesco Bonchi,Raffaele Perego,Fabrizio Silvestri,Hossein Vahabi,Rossano Venturini",,2011,,,
461,Fault-Tolerant Adaptive Control of High-Speed Trains Under Traction/Braking Failures: A Virtual Parameter-Based Approach,"Advanced control is a key technology for enhancing safe and reliable operation of high-speed trains. This paper presents an automated train control scheme for high-speed trains with combined longitudinal aerodynamics and tracking/braking dynamics, with special emphasis on reliable position and velocity tracking in the face of traction/braking failures. The controller is synthesized using a so-called virtual-parameter-based backstepping adaptive control method, which exhibits several salient features: 1) The inherent coupling effects are taken into account as a result of combining both longitudinal and traction/braking dynamics; 2) fully parameter independent rather than partially parameter independent control algorithms are derived; and 3) closed-loop tracking stability of the overall system is ensured under unnoticeable time-varying traction/braking failures. The effectiveness of the developed control scheme is authenticated via a formative mathematical analysis based on Lyapunov stability theory and validated via numerical simulations.","Yongduan Song,Qi Song,Wenchuan Cai",IEEE Transactions on Intelligent Transportation Systems,2014,,,
462,High frequency market microstructure.,"Markets are different now, transformed by technology and high frequency trading. In this paper, I investigate the implications of these changes for high frequency market microstructure (HFT). I describe the new high frequency world, with a particular focus on how HFT affects the strategies of traders and markets. I discuss some of the gaps that arise when thinking about microstructure research issues in the high frequency world. I suggest that, like everything else in the markets, research must also change to reflect the new realities of the high frequency world. I propose some topics for this new research agenda in high frequency market microstructure.",Maureen O'Hara,Journal of Financial Economics,2015,,,
463,What are the main drivers of the Bitcoin price? Evidence from wavelet coherence analysis,"The Bitcoin has emerged as a fascinating phenomenon in the Financial markets. Without any central authority issuing the currency, the Bitcoin has been associated with controversy ever since its popularity, accompanied by increased public interest, reached high levels. Here, we contribute to the discussion by examining the potential drivers of Bitcoin prices, ranging from fundamental sources to speculative and technical ones, and we further study the potential influence of the Chinese market. The evolution of relationships is examined in both time and frequency domains utilizing the continuous wavelets framework, so that we not only comment on the development of the interconnections in time but also distinguish between short-term and long-term connections. We find that the Bitcoin forms a unique asset possessing properties of both a standard financial asset and a speculative one.",Ladislav Kristoufek,PLOS ONE,2015,,,
464,An Ensemble-Based Incremental Learning Approach to Data Fusion,"This paper introduces Learn++, an ensemble of classifiers based algorithm originally developed for incremental learning, and now adapted for information/data fusion applications. Recognizing the conceptual similarity between incremental learning and data fusion, Learn++ follows an alternative approach to data fusion, i.e., sequentially generating an ensemble of classifiers that specifically seek the most discriminating information from each data set. It was observed that Learn++ based data fusion consistently outperforms a similarly configured ensemble classifier trained on any of the individual data sources across several applications. Furthermore, even if the classifiers trained on individual data sources are fine tuned for the given problem, Learn++ can still achieve a statistically significant improvement by combining them, if the additional data sets carry complementary information. The algorithm can also identify-albeit indirectly-those data sets that do not carry such additional information. Finally, it was shown that the algorithm can consecutively learn both the supplementary novel information coming from additional data of the same source, and the complementary information coming from new data sources without requiring access to any of the previously seen data","Devi Parikh,Robi Polikar",,2007,,,
465,Collaborative filtering meets next check-in location prediction,"With the increasing popularity of Location-based Social Networks, a vast amount of location check-ins have been accumulated. Though location prediction in terms of check-ins has been recently studied, the phenomena that users often check in novel locations has not been addressed. To this end, in this paper, we leveraged collaborative filtering techniques for check-in location prediction and proposed a short- and long-term preference model. We extensively evaluated it on two large-scale check-in datasets from Gowalla and Dianping with 6M and 1M check-ins, respectively, and showed that the proposed model can outperform the competing baselines.","Defu Lian,Vincent W. Zheng,Xing Xie",,2013,,,
466,Energy-Based Models in Document Recognition and Computer Vision,"The machine learning and pattern recognition communities are facing two challenges: solving the normalization problem, and solving the deep learning problem. The normalization problem is related to the difficulty of training probabilistic models over large spaces while keeping them properly normalized. In recent years, the ML and natural language communities have devoted considerable efforts to circumventing this problem by developing ""un-normalized"" learning models for tasks in which the output is highly structured (e.g. English sentences). This class of models was in fact originally developed during the 90's in the handwriting recognition community, and includes graph transformer networks, conditional random fields, hidden Markov SVMs, and maximum margin Markov networks. We describe these models within the unifying framework of ""energy-based models"" (EBM). The deep learning problem is related to the issue of training all the levels of a recognition system (e.g. segmentation, feature extraction, recognition, etc) in an integrated fashion. We first consider "" traditional"" methods for deep learning, such as convolutional networks and back-propagation, and show that, although they produce very low error rates for handwriting and object recognition, they require many training samples. We show that using unsupervised learning to initialize the layers of a deep network dramatically reduces the required number of training samples, particularly for such tasks as the recognition of everyday objects at the category level.","Yann LeCun,Sumit Chopra,Marc'Aurelio Ranzato,Fu-Jie Huang",,2007,,,
467,"Information Theory, Inference and Learning Algorithms",Fun and exciting textbook on the mathematics underpinning the most dynamic areas of modern science and engineering.,David J. C. MacKay,,2003,,,
468,"A case study on bagging, boosting and basic ensembles of neural networks for OCR","We study the effectiveness of three neural network ensembles in improving OCR performance: basic, bagging, and boosting. Three random character degradation models are introduced for training individual networks in order to reduce error correlation between individual network and to improve the generalization ability of neural networks. We compare the recognition accuracies of these three ensembles at various rejection rates. It is shown that although the boosting ensemble is slightly more accurate than the basic and bagging ensembles at zero rejection rate, the advantage of the boosting training over the basic and bagging ensembles quickly disappears as more patterns are rejected. Eventually the basic and bagging ensembles outperform the boosting ensemble at high rejection rates. Explanation of such a phenomenon is provided. We also apply the optimal linear combiner to each of the three ensembles to capture different error correlation characteristics of the three ensembles.",Jianchang Mao,,1998,,,
469,3D Virtual worlds and the metaverse: Current status and future possibilities,"Moving from a set of independent virtual worlds to an integrated network of 3D virtual worlds or Metaverse rests on progress in four areas: immersive realism, ubiquity of access and identity, interoperability, and scalability. For each area, the current status and needed developments in order to achieve a functional Metaverse are described. Factors that support the formation of a viable Metaverse, such as institutional and popular interest and ongoing improvements in hardware performance, and factors that constrain the achievement of this goal, including limits in computational methods and unrealized collaboration among virtual world stakeholders and developers, are also considered.","John David N. Dionisio,William G. Burns,Richard L. Gilbert",ACM Computing Surveys,2013,,,
470,Diversity creation methods: a survey and categorisation,"Ensemble approaches to classication and regression have attracted a great deal of interest in recent years. These methods can be shown both theoretically and empirically to outperform single predictors on a wide range of tasks. One of the elements required for accurate prediction when using an ensemble is recognised to be error \diversity"". However, the exact meaning of this concept is not clear from the literature, particularly for classication tasks. In this paper we rst review the varied attempts to provide a formal explanation of error diversity, including several heuristic and qualitative explanations in the literature. For completeness of discussion we include not only the classication literature but also some excerpts of the rather more mature regression literature, which we believe can still provide some insights. We proceed to survey the various techniques used for creating diverse ensembles, and categorise them, forming a preliminary taxonomy of diversity creation methods. As part of this taxonomy we introduce the idea of implicit and explicit diversity creation methods, and three dimensions along which these may be applied. Finally we propose some new directions that may prove fruitful in understanding classication error diversity.","Gavin Brown,Jeremy L. Wyatt,Rachel Harris,Xin Yao",Information Fusion,2004,,,
471,RotBoost: A technique for combining Rotation Forest and AdaBoost,"This paper presents a novel ensemble classifier generation technique RotBoost, which is constructed by combining Rotation Forest and AdaBoost. The experiments conducted with 36 real-world data sets available from the UCI repository, among which a classification tree is adopted as the base learning algorithm, demonstrate that RotBoost can generate ensemble classifiers with significantly lower prediction error than either Rotation Forest or AdaBoost more often than the reverse. Meanwhile, RotBoost is found to perform much better than Bagging and MultiBoost. Through employing the bias and variance decompositions of error to gain more insight of the considered classification methods, RotBoost is seen to simultaneously reduce the bias and variance terms of a single tree and the decrement achieved by it is much greater than that done by the other ensemble methods, which leads RotBoost to perform best among the considered classification procedures. Furthermore, RotBoost has a potential advantage over AdaBoost of suiting parallel execution.","Chun-Xia Zhang,Jiang-She Zhang",Pattern Recognition Letters,2008,,,
472,Efficient Hybrid Web Recommendations Based on Markov Clickstream Models and Implicit Search,"In this paper, we present novel methods that combine (1) Markov models and (2) Web page content search techniques to generate Web navigation recommendations. For click-stream modeling, both first-order and second-order Markov models were studied and a compact storage format for Markov transition matrices was used. For content-based search, a search engine was used to obtain similar-content pages for recommendation to compensate for the sparsity of the Markov model and thus improve coverage. Experiments were conducted on real Web clickstream logs, and confirmed the efficiency of the proposed methods.","Zhiyong Zhang,Olfa Nasraoui",,2007,,,
473,Multi-Class Deep Boosting,"We present new ensemble learning algorithms for multi-class classification. Our algorithms can use as a base classifier set a family of deep decision trees or other rich or complex families and yet benefit from strong generalization guarantees. We give new data-dependent learning bounds for convex ensembles in the multi-class classification setting expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. These bounds are finer than existing ones both thanks to an improved dependency on the number of classes and, more crucially, by virtue of a more favorable complexity term expressed as an average of the Rademacher complexities based on the ensemble's mixture weights. We introduce and discuss several new multi-class ensemble algorithms benefiting from these guarantees, prove positive results for the H-consistency of several of them, and report the results of experiments showing that their performance compares favorably with that of multi-class versions of AdaBoost and Logistic Regression and their L1-regularized counterparts.","Vitaly Kuznetsov,Mehryar Mohri,Umar Syed",,2014,,,
474,An adaptive neuro-fuzzy inference system for predicting unconfined compressive strength and Young’s modulus: a study on Main Range granite,"Engineering properties of rocks such as unconfined compressive strength (UCS) and Young’s modulus (E) are among the essential parameters for the design of tunnel excavations. Many attempts have been made to develop indirect methods of estimating UCS and E. This is generally attributed to the difficulty of preparing and conducting the aforementioned tests in a laboratory. In essence, this study aims to present two predictive models of UCS and E for granite using an adaptive neuro-fuzzy inference system (ANFIS). The required rock samples for model development (45 granite sample sets) were obtained from site investigation work at the Pahang-Selangor raw water transfer tunnel, which was excavated across the Main Range of Peninsular Malaysia. In developing the predictive models, dry density, ultrasonic velocity, quartz content and plagioclase were set as model inputs. These parameters were selected based on simple and multiple regression analyses presented in the article. However, for the sake of comparison, the prediction performances of the ANFIS models were checked against multiple regression analysis (MRA) and artificial neural network (ANN) predictive models of UCS and E. The capacity performances of the predictive models were assessed based on the value account for (VAF), root mean squared error (RMSE) and coefficient of determination (R2). It was found that the ANFIS predictive model of UCS, with R2, RMSE and VAF equal to 0.985, 6.224 and 98.455 %, respectively, outperforms the MRA and ANN models. A similar conclusion was drawn for the ANFIS predictive model of E where the values of R2, RMSE and VAF were 0.990, 3.503 and 98.968 %, respectively.","Danial Jahed Armaghani,Edy Tonnizam Mohamad,Ehsan Momeni,Mogana Sundaram Narayanasamy,Mohd For Mohd Amin",Bulletin of Engineering Geology and the Environment,2015,,,
475,The exponentially weighted moving average,"The Shewhart and CUSUM control chart techniques have found wide application in the manufacturing industries. However, workpiece quality has also been greatly enhanced by rapid and precise individual item measurements and by improvements in automatic dynamic machine control. One consequence is a growing similarity in the control problems faced by the workpiece quality control engineer and his compatriot in the continuous process industries. The purpose of this paper is to exposit a control chart technique that may be of value to both manufacturing and continuous process quality control engineers: the exponentially weighted moving average (EWMA) control chart. The EWMA has its origins in the early work of econometricians, and although its use in quality control has been recognized, it remains a largely neglected tool. The EWMA chart is easy to plot, easy to interpret, and its control limits are easy to obtain. Further, the EWMA leads naturally to an empirical dynamic control equation.",J. Stuart Hunter,Journal of Quality Technology,1986,,,
476,Applying associative retrieval techniques to alleviate the sparsity problem in collaborative filtering,"Recommender systems are being widely applied in many application settings to suggest products, services, and information items to potential consumers. Collaborative filtering, the most successful recommendation approach, makes recommendations based on past transactions and feedback from consumers sharing similar interests. A major problem limiting the usefulness of collaborative filtering is the sparsity problem, which refers to a situation in which transactional or feedback data is sparse and insufficient to identify similarities in consumer interests. In this article, we propose to deal with this sparsity problem by applying an associative retrieval framework and related spreading activation algorithms to explore transitive associations among consumers through their past transactions and feedback. Such transitive associations are a valuable source of information to help infer consumer interests and can be explored to deal with the sparsity problem. To evaluate the effectiveness of our approach, we have conducted an experimental study using a data set from an online bookstore. We experimented with three spreading activation algorithms including a constrained Leaky Capacitor algorithm, a branch-and-bound serial symbolic search algorithm, and a Hopfield net parallel relaxation search algorithm. These algorithms were compared with several collaborative filtering approaches that do not consider the transitive associations: a simple graph search approach, two variations of the user-based approach, and an item-based approach. Our experimental results indicate that spreading activation-based approaches significantly outperformed the other collaborative filtering methods as measured by recommendation precision, recall, the F-measure, and the rank score. We also observed the over-activation effect of the spreading activation approach, that is, incorporating transitive associations with past transactional data that is not sparse may ""dilute"" the data used to infer user preferences and lead to degradation in recommendation performance.","Zan Huang,Hsinchun Chen,Daniel Zeng",ACM Transactions on Information Systems,2004,,,
477,Pruning an ensemble of classifiers via reinforcement learning,"This paper studies the problem of pruning an ensemble of classifiers from a reinforcement learning perspective. It contributes a new pruning approach that uses the Q-learning algorithm in order to approximate an optimal policy of choosing whether to include or exclude each classifier from the ensemble. Extensive experimental comparisons of the proposed approach against state-of-the-art pruning and combination methods show very promising results. Additionally, we present an extension that allows the improvement of the solutions returned by the proposed approach over time, which is very useful in certain performance-critical domains.","Ioannis Partalas,Grigorios Tsoumakas,Ioannis Vlahavas",Neurocomputing,2009,,,
478,Developing a Web Recommendation System Based on Closed Sequential Patterns,"The proposed system is mainly based on mining closed sequential web access patterns. Initially, the PrefixSpan algorithm is employed on the preprocessed web server log data for mining sequential web access patterns. Subsequently, with the aid of post-pruning strategy, the closed sequential web access patterns are discovered from the complete set of sequential web access patterns. Then, a pattern tree, a compact representation of closed sequential patterns, is constructed from the discovered closed sequential web access patterns. The Patricia trie based data structure is used in the construction of the pattern tree. For a given user’s web access sequence, the proposed system provides recommendations on the basis of the constructed pattern tree. The experimentation of the proposed system is performed using synthetic dataset and the performance of the proposed recommendation system is evaluated with precision, applicability and hit ratio","Utpala Niranjan,R. B. V. Subramanyam,V. Khanaa,V. Khanaa",,2010,,,
479,Factorizing personalized Markov chains for next-basket recommendation,"Recommender systems are an important component of many websites. Two of the most popular approaches are based on matrix factorization (MF) and Markov chains (MC). MF methods learn the general taste of a user by factorizing the matrix over observed user-item preferences. On the other hand, MC methods model sequential behavior by learning a transition graph over items that is used to predict the next action based on the recent actions of a user. In this paper, we present a method bringing both approaches together. Our method is based on personalized transition graphs over underlying Markov chains. That means for each user an own transition matrix is learned - thus in total the method uses a transition cube. As the observations for estimating the transitions are usually very limited, our method factorizes the transition cube with a pairwise interaction model which is a special case of the Tucker Decomposition. We show that our factorized personalized MC (FPMC) model subsumes both a common Markov chain and the normal matrix factorization model. For learning the model parameters, we introduce an adaption of the Bayesian Personalized Ranking (BPR) framework for sequential basket data. Empirically, we show that our FPMC model outperforms both the common matrix factorization and the unpersonalized MC model both learned with and without factorization.","Steffen Rendle,Christoph Freudenthaler,Lars Schmidt-Thieme",,2010,,,
480,Twitter mood predicts the stock market.,"Behavioral economics tells us that emotions can profoundly affect individual behavior and decision-making. Does this also apply to societies at large, i.e. can societies experience mood states that affect their collective decision making? By extension is the public mood correlated or even predictive of economic indicators? Here we investigate whether measurements of collective mood states derived from large-scale Twitter feeds are correlated to the value of the Dow Jones Industrial Average (DJIA) over time. We analyze the text content of daily Twitter feeds by two mood tracking tools, namely OpinionFinder that measures positive vs. negative mood and Google-Profile of Mood States (GPOMS) that measures mood in terms of 6 dimensions (Calm, Alert, Sure, Vital, Kind, and Happy). We cross-validate the resulting mood time series by comparing their ability to detect the public's response to the presidential election and Thanksgiving day in 2008. A Granger causality analysis and a Self-Organizing Fuzzy Neural Network are then used to investigate the hypothesis that public mood states, as measured by the OpinionFinder and GPOMS mood time series, are predictive of changes in DJIA closing values. Our results indicate that the accuracy of DJIA predictions can be significantly improved by the inclusion of specific public mood dimensions but not others. We find an accuracy of 87.6% in predicting the daily up and down changes in the closing values of the DJIA and a reduction of the Mean Average Percentage Error by more than 6%. Index Terms—stock market prediction — twitter — mood analysis.","Johan Bollen,Huina Mao,Xiao-Jun Zeng",Journal of Computational Science,2011,,,
481,Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions,"This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches. This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multicriteria ratings, and a provision of more flexible and less intrusive types of recommendations.","Gediminas Adomavicius,Alexander Tuzhilin",IEEE Transactions on Knowledge and Data Engineering,2005,,,
482,On the Properties of Neural Machine Translation: Encoder-Decoder Approaches,"Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.","Kyunghyun Cho,Bart van Merriënboer,Dzmitry Bahdanau,Yoshua Bengio",arXiv: Computation and Language,2014,,,
483,Does Academic Research Destroy Stock Return Predictability,"We study the out-of-sample and post-publication return predictability of 97 variables shown to predict cross-sectional stock returns. Portfolio returns are 26% lower out-of-sample and 58% lower post-publication. The out-of-sample decline is an upper bound estimate of data mining effects. We estimate a 32% (58%–26%) lower return from publication-informed trading. Post-publication declines are greater for predictors with higher in-sample returns, and returns are higher for portfolios concentrated in stocks with high idiosyncratic risk and low liquidity. Predictor portfolios exhibit post-publication increases in correlations with other published-predictor portfolios. Our findings suggest that investors learn about mispricing from academic publications.","R. David McLean,Jeffrey Pontiff",Journal of Finance,2016,,,
484,Forecasting of Stock Prices Using Multi Layer Perceptron,"Prediction of stock market has been a challenging task and of great interest for researchers as the very fact that stock market is a highly volatile in its behavior. For predicting stock price of Bombay Stock Exchange (BSE), Multilayer Networks with dynamic back propagation has been used. The stock prices are determined and compared with two different architectures NN1 (3-16-1) and NN2 (3-6-1). Neural Network based forecasting of stock prices of selected sectors under Bombay Stock Exchange show that neural networks have the power to predict prices albeit the volatility in the markets. The paper is organized as follows. In Section one the volatile nature of stock market is discussed. Section two reviews the literature on the applications of ANNs in predicting the stock prices. Section three gives an overview of forecasting methods. In Section four the concept of Artificial Neural Network presented. Section five presents the methodology adopted in forecasting the stock price. In the final section results, future direction of the study and conclusion are derived.","A. Victor Devadoss,A. Victor Devadoss,A. Victor Devadoss,T. Antony Alphonnse Ligori",,2013,,,
485,Multimodal Deep Learning,"Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning.","Jiquan Ngiam,Aditya Khosla,Mingyu Kim,Juhan Nam,Honglak Lee,Andrew Y. Ng",,2011,,,
486,Visualizing Data using t-SNE,"We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.","Laurens van der Maaten,Geoffrey E. Hinton",Journal of Machine Learning Research,2008,,,
487,Taking the Human Out of the Loop: A Review of Bayesian Optimization,"Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.","Bobak Shahriari,Kevin Swersky,Ziyu Wang,Ryan P. Adams,Nando de Freitas",,2016,,,
488,Deep Residual Learning for Image Recognition,"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.","Kaiming He,Xiangyu Zhang,Shaoqing Ren,Jian Sun",,2016,,,
489,Coupled interdependent attribute analysis on mixed data,"In the real-world applications, heterogeneous interdependent attributes that consist of both discrete and numerical variables can be observed ubiquitously. The usual representation of these data sets is an information table, assuming the independence of attributes. However, very often, they are actually interdependent on one another, either explicitly or implicitly. Limited research has been conducted in analyzing such attribute interactions, which causes the analysis results to be more local than global. This paper proposes the coupled heterogeneous attribute analysis to capture the interdependence among mixed data by addressing coupling context and coupling weights in unsupervised learning. Such global couplings integrate the interactions within discrete attributes, within numerical attributes and across them to form the coupled representation for mixed-type objects based on dimension conversion and feature selection. This work makes one step forward towards explicitly modeling the interdependence of heterogeneous attributes among mixed data, verified by the applications in data structure analysis, data clustering evaluation, and density comparison. Substantial experiments on 12 UCI data sets show that our approach can effectively capture the global couplings of heterogeneous attributes and outperforms the state-of-the-art methods, supported by statistical analysis.","Can Wang,Chi-Hung Chi,Wei Zhou,Raymond K. Wong",,2015,,,
490,Prioritized Experience Replay,"Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.","Tom Schaul,John Quan,Ioannis Antonoglou,David Silver",arXiv: Learning,2015,,,
491,Personalized ranking metric embedding for next new POI recommendation,"The rapidly growing of Location-based Social Networks (LBSNs) provides a vast amount of check-in data, which enables many services, e.g., point-of-interest (POI) recommendation. In this paper, we study the next new POI recommendation problem in which new POIs with respect to users' current location are to be recommended. The challenge lies in the difficulty in precisely learning users' sequential information and personalizing the recommendation model. To this end, we resort to the Metric Embedding method for the recommendation, which avoids drawbacks of the Matrix Factorization technique. We propose a personalized ranking metric embedding method (PRME) to model personalized check-in sequences. We further develop a PRME-G model, which integrates sequential information, individual preference, and geographical influence, to improve the recommendation performance. Experiments on two real-world LBSN datasets demonstrate that our new algorithm outperforms the state-of-the-art next POI recommendation methods.","Shanshan Feng,Shanshan Feng,Xutao Li,Yifeng Zeng,Gao Cong,Yeow Meng Chee,Quan Yuan,Quan Yuan",,2015,,,
492,Stock market forecasting by using a hybrid model of exponential fuzzy time series,"The initial aim of this study is to propose a hybrid method based on exponential fuzzy time series and learning automata based optimization for stock market forecasting. For doing so, a two-phase approach is introduced. In the first phase, the optimal lengths of intervals are obtained by applying a conventional fuzzy time series together with learning automata swarm intelligence algorithm to tune the length of intervals properly. Subsequently, the obtained optimal lengths are applied to generate a new fuzzy time series, proposed in this study, named exponential fuzzy time series. In this final phase, due to the nature of exponential fuzzy time series, another round of optimization is required to estimate certain method parameters. Finally, this model is used for future forecasts. In order to validate the proposed hybrid method, forty-six case studies from five stock index databases are employed and the findings are compared with well-known fuzzy time series models and classic methods for time series. The proposed model has outperformed its counterparts in terms of accuracy. In this study a two phase approach is proposed based on exponential fuzzy time series and learning automata.In the first phase, the optimal lengths of intervals are estimated by applying LA based EAs in training set.Second phase aim is to estimate certain adjusting parameters for minimizing errors in training set.The conventional FTS in the first phase is applied and in the second phase EFTS is employed.Forty six case studies from five stock index databases are employed in extensive experiments.","Fatemeh Mirzaei Talarposhti,Hossein Javedani Sadaei,Rasul Enayatifar,Frederico G. Guimarães,Maqsood Mahmud,Tayyebeh Eslami",International Journal of Approximate Reasoning,2016,,,
493,Reinforcement Learning in Large Discrete Action Spaces.,"Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Recommender systems, industrial plants and language models are only some of the many real-world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. An ability to generalize over the set of actions as well as sub-linear complexity relative to the size of the set are both necessary to handle such tasks. Current approaches are not able to provide both of these, which motivates the work in this paper. Our proposed approach leverages prior information about the actions to embed them in a continuous space upon which it can generalize. Additionally, approximate nearest-neighbor methods allow for logarithmic-time lookup complexity relative to the number of actions, which is necessary for time-wise tractable training. This combined approach allows reinforcement learning methods to be applied to large-scale learning problems previously intractable with current methods. We demonstrate our algorithm’s abilities on a series of tasks having up to one million actions.","Gabriel Dulac-Arnold,Richard Evans,Richard J. Evans,Peter Sunehag,Ben Coppin,Ben Coppin",arXiv: Artificial Intelligence,2015,,,
494,From Word Embeddings to Item Recommendation.,"Social network platforms can use the data produced by their users to serve them better. One of the services these platforms provide is recommendation service. Recommendation systems can predict the future preferences of users using their past preferences. In the recommendation systems literature there are various techniques, such as neighborhood based methods, machine-learning based methods and matrix-factorization based methods. In this work, a set of well known methods from natural language processing domain, namely Word2Vec, is applied to recommendation systems domain. Unlike previous works that use Word2Vec for recommendation, this work uses non-textual features, the check-ins, and it recommends venues to visit/check-in to the target users. For the experiments, a Foursquare check-in dataset is used. The results show that use of continuous vector space representations of items modeled by techniques of Word2Vec is promising for making recommendations.",Makbule Gulcin Ozsoy,arXiv: Learning,2016,,,
495,Predicting drug side effects by multi-label learning and ensemble learning.,"Predicting drug side effects is an important topic in the drug discovery. Although several machine learning methods have been proposed to predict side effects, there is still space for improvements. Firstly, the side effect prediction is a multi-label learning task, and we can adopt the multi-label learning techniques for it. Secondly, drug-related features are associated with side effects, and feature dimensions have specific biological meanings. Recognizing critical dimensions and reducing irrelevant dimensions may help to reveal the causes of side effects. In this paper, we propose a novel method ‘feature selection-based multi-label k-nearest neighbor method’ (FS-MLKNN), which can simultaneously determine critical feature dimensions and construct high-accuracy multi-label prediction models. Computational experiments demonstrate that FS-MLKNN leads to good performances as well as explainable results. To achieve better performances, we further develop the ensemble learning model by integrating individual feature-based FS-MLKNN models. When compared with other state-of-the-art methods, the ensemble method produces better performances on benchmark datasets. In conclusion, FS-MLKNN and the ensemble method are promising tools for the side effect prediction. The source code and datasets are available in the Additional file 1.","Wen Zhang,Wen Zhang,Wen Zhang,Feng Liu,Longqiang Luo,Jingxia Zhang",BMC Bioinformatics,2015,,,
496,"Ensemble Classification and Regression-Recent Developments, Applications and Future Directions [Review Article]","Ensemble methods use multiple models to get better performance. Ensemble methods have been used in multiple research fields such as computational intelligence, statistics and machine learning. This paper reviews traditional as well as state-of-the-art ensemble methods and thus can serve as an extensive summary for practitioners and beginners. The ensemble methods are categorized into conventional ensemble methods such as bagging, boosting and random forest, decomposition methods, negative correlation learning methods, multi-objective optimization based ensemble methods, fuzzy ensemble methods, multiple kernel learning ensemble methods and deep learning based ensemble methods. Variations, improvements and typical applications are discussed. Finally this paper gives some recommendations for future research directions.","Ye Ren,Le Zhang,Ponnuthurai Nagaratnam Suganthan",IEEE Computational Intelligence Magazine,2016,,,
497,Glove: Global Vectors for Word Representation,"Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.","Jeffrey Pennington,Richard Socher,Christopher D. Manning",,2014,,,
498,Long Short-Term Memory,"As discussed in the previous chapter, an important benefit of recurrent neural networks is their ability to use contextual information when mapping between input and output sequences. Unfortunately, for standard RNN architectures, the range of context that can be in practice accessed is quite limited. The problem is that the influence of a given input on the hidden layer, and therefore on the network output, either decays or blows up exponentially as it cycles around the network’s recurrent connections. This effect is often referred to in the literature as the vanishing gradient problem (Hochreiter, 1991; Hochreiter et al., 2001a; Bengio et al., 1994). The vanishing gradient problem is illustrated schematically in Figure 4.1",Alex Graves,,2012,,,
499,Commodity Channel Index: Evaluation of Trading Rule of Agricultural Commodities,"This paper is focused on evaluating the trading rule of indicator commodity channel index (CCI), using selected agricultural commodities. The reason of testing is that this indicator is calculated with respect to fluctuation of commodity market - volatility. The recent issue of commodity markets examines trading under risk. The concept is in analyzing of predictive power of CCI. The main core of this paper is if the trading strategy under evaluating using technical analysis, respectively CCI, reaches positive profit. The returns of trading rule are measured using signals to buying or selling and comparison each of them. Authors of this paper created trading rule based on CCI and tested it on commodity markets. The results are positive in term of % CCI. Findings of the strategy are positive due to measurement volatility involved in indicator. The commodity markets are volatile, time series are fluctuating due to actual announcements or news.","Mansoor Maitah,Petr Procházka,Michal Cermak,Karel Šrédl",International Journal of Economics and Financial Issues,2016,,,
500,Exercise Cycles Injure Kids' Hands,,David Araujo,The Physician and Sportsmedicine,1998,,,
501,Asynchronous Methods for Deep Reinforcement Learning,"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.","Volodymyr Mnih,Adrià Puigdomènech Badia,Mehdi Mirza,Alex Graves,Timothy Lillicrap,Tim Harley,David Silver,Koray Kavukcuoglu",arXiv: Learning,2016,,,
502,Neuro-fuzzy technique to predict air-overpressure induced by blasting,"In addition to all benefits of blasting in mining and civil engineering applications, blasting has some undesirable impacts on surrounding areas. Blast-induced air-overpressure (AOp) is one of the most important environmental impacts of blasting operation which may cause severe damage to nearby residents and structures. Hence, it is a major concern to predict and subsequently control the AOp due to blasting. This paper presents an adaptive neuro-fuzzy inference system (ANFIS) model for prediction of blast-induced AOp in quarry blasting sites. For this purpose, 128 blasting operations were monitored in three quarry sites, Malaysia. Several models were constructed to obtain the optimum model in which each model involved five inputs and one output. Values of maximum charge per delay, powder factor, burden to spacing ratio, stemming length, and distance between monitoring station and blast face were set as input parameters to predict AOp. For comparison purposes, considering the same data, AOp values were predicted through the pre-developed artificial neural network (ANN) model and multiple regression (MR) technique. The results demonstrated the superiority of the ANFIS model to predict AOp compared to other methods. Moreover, results of sensitivity analysis indicated that the maximum charge per delay and powder factor and distance from the blast face are the most influential parameters on AOp.","Danial Jahed Armaghani,Mohsen Hajihassani,Houman Sohaei,Edy Tonnizam Mohamad,Aminaton Marto,Hossein Motaghedi,Mohammad Reza R. Moghaddam",Arabian Journal of Geosciences,2015,,,
503,Session-based Recommendations with Recurrent Neural Networks,"We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.","Balázs Hidasi,Alexandros Karatzoglou,Linas Baltrunas,Domonkos Tikk",arXiv: Learning,2015,,,
504,An intelligent pattern recognition model for supporting investment decisions in stock market,"Abstract   For many years, how to make stock market predictions has been a prevalent research topic. To carry out accurate forecasting, stock analysts and academic researchers have tried various analysis techniques, algorithms, and models. For example, ""technical analysis” is a popular approach used by common stock investors to analyze market trend, and Artificial Intelligence (AI) algorithms such as genetic algorithms (GAs), neural network (NN), and fuzzy time-series (FTS), were proposed by researchers to forecast the future stock index. Although the daily forecasts are very useful for professional investors who implement intraday trading, we argue that forecasting a bullish turning point is a more interesting issue than the future stock index for common investor because an accurate forecast will bring a huge amount of stock return. Therefore, this paper proposes an intelligent pattern recognition model, based on two new stock pattern recognition methods, “PIP bull-flag pattern matching” and the “floating-weighted bull-flag template,” to recognize a bull-flag stock pattern. The bull-flag pattern is a stock's turning point with proper timing, which can enable a stock investor to profit. To promote recognition accuracy, the proposed model employs chart patterns and technical indicators, simultaneously, as pattern recognition factors. In the model verification, we evaluate the proposed model with stock returns by forecasting two stock databases (TAIEX and NASDAQ), and comparing the returns with other advanced algorithms. The experimental results indicate that the proposed model outperforms the published algorithms, such as rough set theory (RST), genetic algorithms (GAs) and their hybrid model, and gives a high-level of profitability. Additionally, the trading strategies, provided by the proposed model, also help investors to make beneficial investment decisions in the stock market.","Tai-Liang Chen,Feng-yu Chen",Information Sciences,2016,,,
505,A deep learning approach to unsupervised ensemble learning,"We show how deep learning methods can be applied in the context of crowdsourcing and unsupervised ensemble learning. First, we prove that the popular model of Dawid and Skene, which assumes that all classifiers are conditionally independent, is equivalent to a Restricted Boltzmann Machine (RBM) with a single hidden node. Hence, under this model, the posterior probabilities of the true labels can be instead estimated via a trained RBM. Next, to address the more general case, where classifiers may strongly violate the conditional independence assumption, we propose to apply RBM-based Deep Neural Net (DNN). Experimental results on various simulated and real-world datasets demonstrate that our proposed DNN approach outperforms other state-of-the-art methods, in particular when the data violates the conditional independence assumption.","Uri Shaham,Xiuyuan Cheng,Omer Dror,Ariel Jaffe,Boaz Nadler,Joseph T. Chang,Yuval Kluger",,2016,,,
506,Using computational intelligence to forecast carbon prices,"A novel neuro-fuzzy controller to forecast carbon emission prices was built.A closed loop mechanism to enhance the forecasts was used.Forecasting performance in terms of errors end directional prediction was examined.Trading simulation results with the Buy and Hold strategy was compared.The neuro-fuzzy controller with the closed loop outperformed the other models. European Union has introduced the European Trading System (ETS) as a tool for developing and implementing international treaties related to climate changes and to identify the most cost-effective methods for reducing greenhouse gas emissions, in particular carbon dioxide (CO2), which is the most substantial. Companies producing carbon emissions must effectively manage associated costs by buying or selling carbon emission futures. Viewed from this perspective, this paper provides a model for managing the risk by buying and selling carbon emission futures by implementing techniques that leverage computational intelligence. Three computational intelligence techniques are proposed to provide accurate and timely forecasts for changes in the price of carbon: a novel hybrid neuro-fuzzy controller that forms a closed-loop feedback mechanism called PATSOS; an artificial neural network (ANN) based system; an adaptive neuro-fuzzy inference system (ANFIS). Results are based on 1074 daily carbon price observations collected to comprise a useful time-series dataset and for evaluation of the proposed techniques. The extra-sample performance of the proposed techniques is calculated. Analysis results are compared with those produced by other models. Comparison studies reveal that PATSOS is the most accurate and promising methodology for predicting the price of carbon. It is stated that this paper registers a first attempt to apply a hybrid neuro-fuzzy controller to forecasting carbon prices.",George S. Atsalakis,Applied Soft Computing,2016,,,
507,Deep Reinforcement Learning from Self-Play in Imperfect-Information Games,"Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on significant domain expertise.","Johannes Heinrich,David Silver",arXiv: Learning,2016,,,
508,Algorithmic and High-Frequency Trading,Preface How to read this book Part I. Microstructure and Empirical Facts: 1. Electronic markets and the limit order book 2. A primer on the microstructure of financial markets 3. Empirical and statistical evidence - prices and returns 4. Empirical and statistical evidence - activity and market quality Part II. Mathematical Tools: 5. Stochastic optimal control and stopping Part III. Algorithmic and High-Frequency Trading: 6. Optimal execution with continuous trading I 7. Optimal execution with continuous trading II 8. Optimal execution with limit and market orders 9. Targeting volume 10. Market making 11. Pairs trading and statistical arbitrage strategies 12. Order imbalance Appendix A. Stochastic calculus for finance Bibliography Glossary Subject index.,"Álvaro Cartea,Sebastian Jaimungal,José Penalva",,2015,,,
509,Multi-agent reinforcement learning as a rehearsal for decentralized planning,"Decentralized partially observable Markov decision processes (Dec-POMDPs) are a powerful tool for modeling multi-agent planning and decision-making under uncertainty. Prevalent Dec-POMDP solution techniques require centralized computation given full knowledge of the underlying model. Multi-agent reinforcement learning (MARL) based approaches have been recently proposed for distributed solution of Dec-POMDPs without full prior knowledge of the model, but these methods assume that conditions during learning and policy execution are identical. In some practical scenarios this may not be the case. We propose a novel MARL approach in which agents are allowed to rehearse with information that will not be available during policy execution. The key is for the agents to learn policies that do not explicitly rely on these rehearsal features. We also establish a weak convergence result for our algorithm, RLaR, demonstrating that RLaR converges in probability when certain conditions are met. We show experimentally that incorporating rehearsal features can enhance the learning rate compared to non-rehearsal-based learners, and demonstrate fast, (near) optimal performance on many existing benchmark Dec-POMDP problems. We also compare RLaR against an existing approximate Dec-POMDP solver which, like RLaR, does not assume a priori knowledge of the model. While RLaR's policy representation is not as scalable, we show that RLaR produces higher quality policies for most problems and horizons studied.","Landon Kraemer,Bikramjit Banerjee",Neurocomputing,2016,,,
510,30Music listening and playlists dataset,"We introduce the 30Music dataset 1 , a collection of listening and playlists data retrieved from Internet radio stations through Last.fm API. In this paper we describe the creation process, its content, and its possible uses. Attractive features of the 30Music dataset that dierentiate it from existing public datasets include, among the others, (i) the user listening sessions complete of contextual time information, (ii) the user playlists, and (iii) the positive user ratings, key information to experiment with the task of modeling user taste and recommending playlists.","Roberto Turrin,Massimo Quadrana,Andrea Condorelli,Roberto Pagano,Paolo Cremonesi",,2015,,,
511,Next Basket Recommendation with Neural Networks.,"One crucial task in recommendation is to predict what a user will buy next given her shopping history. In this paper, we propose a novel neural network to complete this task. The model consists of an embedding layer, a hidden layer and an output layer. Firstly, the distributed representations of the user and the items bought before are obtained and used to form a feature vector by the embedding layer. Then the hidden layer transforms the feature vector to another space by a non-linear operator. Finally, the softmax operator is adopted to output the probabilities of next items. We can see that the model elegantly involves both the user's general interest and the sequential dependencies between items for prediction. Experimental results on two real datasets prove the effectiveness of our model.","Shengxian Wan,Yanyan Lan,Pengfei Wang,Jiafeng Guo,Jun Xu,Xueqi Cheng",,2015,,,
512,Using Implicit Preference Relations to Improve Recommender Systems,"Our work is generally focused on making recommendations for small or medium-sized e-commerce portals, where we are facing scarcity of explicit feedback, low user loyalty, short visit durations or a low number of visited objects. In this paper, we present a novel approach to use a specific user behavior pattern as implicit feedback, forming binary relations between objects. Our hypothesis is that if a user selects a specific object from the list of displayed objects, it is an expression of his/her binary preference between the selected object and others that are visible, but ignored. We expand this relation with content-based similarity of objects. We define implicit preference relation (IPR) a partial ordering of objects based on similarity expansion of ignored-selected preference relation. We propose a merging algorithm utilizing the synergic effect of two approaches this IPR partial ordering and a list of recommended objects based on any/another algorithm. We report on a series of offline experiments with various recommending algorithms on two real-world e-commerce datasets. The merging algorithm could improve the ranked list of most of the evaluated algorithms in terms of nDCG. Furthermore, we also provide access to the relevant datasets and source codes for further research.","Ladislav Peska,Peter Vojtáš",Journal on Data Semantics,2017,,,
513,"Deep or shallow, NLP is breaking out",Neural net advances improve computers' language ability in many fields.,Gregory Goth,Communications of The ACM,2016,,,
514,XGBoost: A Scalable Tree Boosting System,"Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.","Tianqi Chen,Carlos Guestrin",,2016,,,
515,Deep Boosting,"We present a new ensemble learning algorithm, DeepBoost, which can use as base classifiers a hypothesis set containing deep decision trees, or members of other rich or complex families, and succeed in achieving high accuracy without over-fitting the data. The key to the success of the algorithm is a capacity-conscious criterion for the selection of the hypotheses. We give new data-dependent learning bounds for convex ensembles expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. Our algorithm directly benefits from these guarantees since it seeks to minimize the corresponding learning bound. We give a full description of our algorithm, including the details of its derivation, and report the results of several experiments showing that its performance compares favorably to that of AdaBoost and Logistic Regression and their L1-regularized variants.","Corinna Cortes,Mehryar Mohri,Umar Syed",,2014,,,
516,Deep learning for event-driven stock prediction,"We propose a deep learning method for event-driven stock market prediction. First, events are extracted from news text, and represented as dense vectors, trained using a novel neural tensor network. Second, a deep convolutional neural network is used to model both short-term and long-term influences of events on stock price movements. Experimental results show that our model can achieve nearly 6% improvements on S&P 500 index prediction and individual stock prediction, respectively, compared to state-of-the-art baseline methods. In addition, market simulation results show that our system is more capable of making profits than previously reported systems trained on S&P 500 stock historical data.","Xiao Ding,Lei Zhang,Yue Zhang,Ting Liu,Junwen Duan",,2015,,,
517,Algorithmic Trading,"In electronic financial markets, algorithmic trading refers to the use of computer programs to automate one or more stages of the trading process: pretrade analysis (data analysis), trading signal generation (buy and sell recommendations), and trade execution. Trade execution is further divided into agency/broker execution (when a system optimizes the execution of a trade on behalf of a client) and principal/proprietary trading (where an institution trades on its own account). Each stage of this trading process can be conducted by humans, by humans and algorithms, or fully by algorithms.","G. Nuti,G. Nuti,Giuseppe Nuti,M. Mirghaemi,P. Treleaven,Chaiyakorn Yingsaeree,C. Yingsaeree",IEEE Computer,2011,,,
518,Stock market sentiment lexicon acquisition using microblogging data and statistical measures,"Lexicon acquisition is a key issue for sentiment analysis. This paper presents a novel and fast approach for creating stock market lexicons. The approach is based on statistical measures applied over a vast set of labeled messages from StockTwits, which is a specialized stock market microblog. We compare three adaptations of statistical measures, such as Pointwise Mutual Information (PMI), two new complementary statistics and the use of sentiment scores for affirmative and negated contexts. Using StockTwits, we show that the new lexicons are competitive for measuring investor sentiment when compared with six popular lexicons. We also applied a lexicon to easily produce Twitter investor sentiment indicators and analyzed their correlation with survey sentiment indexes. The new microblogging indicators have a moderate correlation with popular Investors Intelligence (II) and American Association of Individual Investors (AAII) indicators. Thus, the new microblogging approach can be used alternatively to traditional survey indicators with advantages (e.g., cheaper creation, higher frequencies). Proposal of an automatic procedure for the creation of stock market lexicons.The procedure uses diverse statistical measures on StockTwits labeled messages.The new lexicons obtain better investor sentiment indicators than general lexicons.The new Twitter sentiment indicators correlate with survey sentiment indicators.","Nuno Oliveira,Paulo Cortez,Nelson Areal",,2016,,,
519,The variance of discounted Markov decision processes,Formulae are presented for the variance and higher moments of the present value of single-stage rewards in a finite Markov decision process. Similar formulae are exhibited for a semi-Markov decision process. There is a short discussion of the obstacles to using the variance formula in algorithms to maximize the mean minus a multiple of the standard deviation.,Matthew J. Sobel,Journal of Applied Probability,1982,,,
520,Dynamic Factor model with infinite dimensional factor space: forecasting,"Abstract. The paper compares the pseudo real-time forecasting performance of threeDynamic Factor Models: (i) The standard principal-component model, Stock and Watson(2002a), (ii) The model based on generalized principal components, Forni et al. (2005),(iii) The model recently proposed in Forni et al. (2015) and Forni et al. (2016). We employa large monthly dataset of macroeconomic and financial time series for the US economy,which includes the Great Moderation, the Great Recession and the subsequent recovery.Using a rolling window for estimation and prediction, we find that (iii) neatly outperforms(i) and (ii) in the Great Moderation period for both Industrial Production and Inflation,and for Inflation over the full sample. However, (iii) is outperfomed by (i) and (ii) over thefull sample for Industrial Production.","Mario Forni,Alessandro Giovannelli,Marco Lippi,Marco Lippi,Stefano Soccorsi",Journal of Applied Econometrics,2018,,,
521,Mean-Semi-Entropy Models of Fuzzy Portfolio Selection,"In this paper, a concept of fuzzy semientropy is proposed to quantify the downside uncertainty. Several properties of fuzzy semientropy are identified and interpreted. By quantifying the downside risk with the use of semientropy, two mean-semi-entropy portfolio selection models are formulated, and a fuzzy simulation-based genetic algorithm is designed to solve the models to optimality. We carry out comparative analyses among the fuzzy mean-entropy models and the fuzzy mean-semi-entropy models and demonstrate that the mean-semi-entropy models can significantly improve the dispersion of investment. Several illustrative examples using stock dataset from the real-world financial market (China Shanghai Stock Exchange) also show the effectiveness of the models.","Jiandong Zhou,Jiandong Zhou,Xiang Li,Xiang Li,Witold Pedrycz",IEEE Transactions on Fuzzy Systems,2016,,,
522,A Review On Evaluation Metrics For Data Classification Evaluations,"Evaluation metric plays a critical role in achieving the optimal classifier during the classification training.
Thus, a selection of suitable evaluation metric is an important key for discriminating and obtaining the
optimal classifier. This paper systematically reviewed the related evaluation metrics that are specifically
designed as a discriminator for optimizing generative classifier. Generally, many generative classifiers
employ accuracy as a measure to discriminate the optimal solution during the classification training.
However, the accuracy has several weaknesses which are less distinctiveness, less discriminability, less
informativeness and bias to majority class data. This paper also briefly discusses other metrics that are
specifically designed for discriminating the optimal solution. The shortcomings of these alternative metrics
are also discussed. Finally, this paper suggests five important aspects that must be taken into consideration
in constructing a new discriminator metric.","M. Hossin,Sulaiman M.N,Md. Nasir Sulaiman",International Journal of Data Mining & Knowledge Management Process,2015,,,
523,Deep Networks with Stochastic Depth,"Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91 % on CIFAR-10).","Gao Huang,Yu Sun,Zhuang Liu,Daniel Sedra,Kilian Q. Weinberger",,2016,,,
524,Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,"Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.","Tejas D. Kulkarni,Karthik Narasimhan,Ardavan Saeedi,Joshua B. Tenenbaum",arXiv: Learning,2016,,,
525,Efficient Market Hypothesis,"A capital market is said to be efficient if it fully and correctly reflects all relevant information in determining security prices. Formally, the market is said to be efficient with respect to some information set, o, if security prices would be unaffected by revealing that information to all participants. Moreover, efficiency with respect to an information set, o, implies that it is impossible to make economic profits by trading on the basis of o.",Burton G. Malkiel,,1991,,,
526,Fast Matrix Factorization for Online Recommendation with Implicit Feedback,"This paper contributes improvements on both the effectiveness and efficiency of Matrix Factorization (MF) methods for implicit feedback. We highlight two critical issues of existing works. First, due to the large space of unobserved feedback, most existing works resort to assign a uniform weight to the missing data to reduce computational complexity. However, such a uniform assumption is invalid in real-world settings. Second, most methods are also designed in an offline setting and fail to keep up with the dynamic nature of online data. We address the above two issues in learning MF models from implicit feedback. We first propose to weight the missing data based on item popularity, which is more effective and flexible than the uniform-weight assumption. However, such a non-uniform weighting poses efficiency challenge in learning the model. To address this, we specifically design a new learning algorithm based on the element-wise Alternating Least Squares (eALS) technique, for efficiently optimizing a MF model with variably-weighted missing data. We exploit this efficiency to then seamlessly devise an incremental update strategy that instantly refreshes a MF model given new feedback. Through comprehensive experiments on two public datasets in both offline and online protocols, we show that our implemented, open-source (https://github.com/hexiangnan/sigir16-eals) eALS consistently outperforms state-of-the-art implicit MF methods.","Xiangnan He,Hanwang Zhang,Min-Yen Kan,Tat-Seng Chua",,2016,,,
527,Content-Based Recommender Systems,"The collaborative systems discussed in the previous chapters use the correlations in the ratings patterns across users to make recommendations. On the other hand, these methods do not use item attributes for computing predictions. This would seem rather wasteful; after all, if John likes the futuristic science fiction movie Terminator, then there is a very good chance that he might like a movie from a similar genre, such as Aliens. In such cases, the ratings of other users may not be required to make meaningful recommendations.",Charu C. Aggarwal,,2016,,,
528,ieRSPOP: A novel incremental rough set-based pseudo outer-product with ensemble learning,"Abstract   The  rough set-based pseudo outer-product fuzzy neural network  (RSPOP FNN) is a member of the  pseudo outer-product  (POP) FNN family known for high accuracy and interpretability. The POP algorithm utilizes a one-pass rule identification and generation process and rough set theory to perform attribute and rule reduction, hence, producing highly interpretable if-then fuzzy rules while maintaining a high level of accuracy. However, non-incremental systems are heavily dependant on the quality and quantity of the training set, an issue especially prominent in time series data. The robustness of RSPOP FNN is improved using an adapted form of  discrete incremental clustering  (DIC), an incremental learning algorithm. This renders the system immune to deficiencies in the training set. Issues with the incremental rough set attribute reduction are also addressed using an adapted form of  Learn ++  Non-Stationary Environments  (Learn++.NSE), a form of ensemble learning strong in datasets with the  concept drift  phenomenon. This is often found in time series data. The proposed system has been extensively benchmarked in traffic flow prediction, real life stock price and volatility predictions. The results show the strength of the online systems against offline systems. The promising results demonstrated the benefit of incremental learning in the accuracy and adaptability of its time series prediction ability.","Ronald Tor Das,Kai Keng Ang,Chai Quek",Applied Soft Computing,2016,,,
529,Predicting the direction of stock market prices using random forest,"Predicting trends in stock market prices has been an area of interest for researchers for many years due to its complex and dynamic nature. Intrinsic volatility in stock market across the globe makes the task of prediction challenging. Forecasting and diffusion modeling, although effective can't be the panacea to the diverse range of problems encountered in prediction, short-term or otherwise. Market risk, strongly correlated with forecasting errors, needs to be minimized to ensure minimal risk in investment. The authors propose to minimize forecasting error by treating the forecasting problem as a classification problem, a popular suite of algorithms in Machine learning. In this paper, we propose a novel way to minimize the risk of investment in stock market by predicting the returns of a stock using a class of powerful machine learning algorithms known as ensemble learning. Some of the technical indicators such as Relative Strength Index (RSI), stochastic oscillator etc are used as inputs to train our model. The learning model used is an ensemble of multiple decision trees. The algorithm is shown to outperform existing algo- rithms found in the literature. Out of Bag (OOB) error estimates have been found to be encouraging. Key Words: Random Forest Classifier, stock price forecasting, Exponential smoothing, feature extraction, OOB error and convergence.","Luckyson Khaidem,Snehanshu Saha,Sudeepa Roy Dey",arXiv: Learning,2016,,,
530,Infinite Ensemble for Image Clustering,"Image clustering has been a critical preprocessing step for vision tasks, e.g., visual concept discovery, content-based image retrieval. Conventional image clustering methods use handcraft visual descriptors as basic features via K-means, or build the graph within spectral clustering. Recently, representation learning with deep structure shows appealing performance in unsupervised feature pre-treatment. However, few studies have discussed how to deploy deep representation learning to image clustering problems, especially the unified framework which integrates both representation learning and ensemble clustering for efficient image clustering still remains void. In addition, even though it is widely recognized that with the increasing number of basic partitions, ensemble clustering gets better performance and lower variances, the best number of basic partitions for a given data set is a pending problem. In light of this, we propose the Infinite Ensemble Clustering (IEC), which incorporates the power of deep representation and ensemble clustering in a one-step framework to fuse infinite basic partitions. Generally speaking, a set of basic partitions is firstly generated from the image data, then by converting the basic partitions to the 1-of-K codings, we link the marginalized auto-encoder to the infinite ensemble clustering with i.i.d. basic partitions, which can be approached by the closed-form solutions, finally we follow the layer-wise training procedure and feed the concatenated deep features to K-means for final clustering. Extensive experiments on diverse vision data sets with different levels of visual descriptors demonstrate both the time efficiency and superior performance of IEC compared to the state-of-the-art ensemble clustering and deep clustering methods.","Hongfu Liu,Ming Shao,Sheng Li,Yun Fu",,2016,,,
531,Ensemble deep learning for speech recognition.,"Deep learning systems have dramatically improved the accuracy of speech recognition, and various deep architectures and learning methods have been developed with distinct strengths and weaknesses in recent years. How can ensemble learning be applied to these varying deep learning systems to achieve greater recognition accuracy is the focus of this paper. We develop and report linear and log-linear stacking methods for ensemble learning with applications specifically to speechclass posterior probabilities as computed by the convolutional, recurrent, and fully-connected deep neural networks. Convex optimization problems are formulated and solved, with analytical formulas derived for training the ensemble-learning parameters. Experimental results demonstrate a significant increase in phone recognition accuracy after stacking the deep learning subsystems that use different mechanisms for computing high-level, hierarchical features from the raw acoustic signals in speech.","Li Deng,John Platt",,2014,,,
532,Swapout: Learning an ensemble of deep architectures,"We describe Swapout, a new stochastic training method, that outperforms ResNets of identical network structure yielding impressive results on CIFAR-10 and CIFAR-100. Swapout samples from a rich set of architectures including dropout, stochastic depth and residual architectures as special cases. When viewed as a regularization method swapout not only inhibits co-adaptation of units in a layer, similar to dropout, but also across network layers. We conjecture that swapout achieves strong regularization by implicitly tying the parameters across layers. When viewed as an ensemble training method, it samples a much richer set of architectures than existing methods such as dropout or stochastic depth. We propose a parameterization that reveals connections to exiting architectures and suggests a much richer set of architectures to be explored. We show that our formulation suggests an efficient training method and validate our conclusions on CIFAR-10 and CIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider model performs similar to a 1001 layer ResNet model.","Saurabh Singh,Derek Hoiem,David Forsyth",arXiv: Computer Vision and Pattern Recognition,2016,,,
533,Metasynthetic Computing and Engineering of Complex Systems,,Longbing Cao,,2015,,,
534,A framework for parameter estimation and model selection in kernel deep stacking networks,"HighlightsKernel deep stacking networks (KDSNs) are a novel method in biomedical research.KDSNs belong to the class of supervised deep learning.They are computationally faster to train than artificial neural networks.KDSNs require the specification of a large number of tuning parameters.We propose a new data-driven framework for model selection in KDSNs.The proposed methodology includes model-based optimization and hill climbing.No pre-specification of any of the KDSN tuning parameters is required.Application of the proposed methodology results in a fast tuning procedure.KDSNs are competitive with other techniques in the field of deep learning. Background and objectivesKernel deep stacking networks (KDSNs) are a novel method for supervised learning in biomedical research. Belonging to the class of deep learning techniques, KDSNs are based on artificial neural network architectures that involve multiple nonlinear transformations of the input data. Unlike traditional artificial neural networks, KDSNs do not rely on backpropagation algorithms but on an efficient fitting procedure that is based on a series of kernel ridge regression models with closed-form solutions. Although being computationally advantageous, KDSN modeling remains a challenging task, as it requires the specification of a large number of tuning parameters. Methods and materialWe propose a new data-driven framework for parameter estimation, hyperparameter tuning, and model selection in KDSNs. The proposed methodology is based on a combination of model-based optimization and hill climbing approaches that do not require the pre-specification of any of the KDSN tuning parameters. We demonstrate the performance of KDSNs by analyzing three medical data sets on hospital readmission of diabetes patients, coronary artery disease, and hospital costs. ResultsOur numerical studies show that the run-time of the proposed KDSN methodology is significantly shorter than the respective run-time of grid search strategies for hyperparameter tuning. They also show that KDSN modeling is competitive in terms of prediction accuracy with other state-of-the-art techniques for statistical learning. ConclusionsKDSNs are a computationally efficient approximation of backpropagation-based artificial neural network techniques. Application of the proposed methodology results in a fast tuning procedure that generates KDSN fits having a similar prediction accuracy as other techniques in the field of deep learning.","Thomas Welchowski,Matthias Schmid",Artificial Intelligence in Medicine,2016,,,
535,Learning Music Embedding with Metadata for Context Aware Recommendation,"Contextual factors can benefit music recommendation and retrieval tasks remarkably. However, how to acquire and utilize the contextual information still need to be studied. In this paper, we propose a context aware music recommendation approach, which can recommend music appropriate for users' contextual preference for music. In analogy to matrix factorization methods for collaborative filtering, the proposed approach does not require songs to be described by features beforehand, but it learns music pieces' embeddings (vectors in low-dimensional continuous space) from music playing records and corresponding metadata and infer users' general and contextual preference for music from their playing records with the learned embedding. Then, our approach can recommend appropriate music pieces. Experimental evaluations on a real world dataset show that the proposed approach outperforms baseline methods.","Dongjing Wang,Shuiguang Deng,Shuiguang Deng,Xin Zhang,Guandong Xu",,2016,,,
536,Learning Vector-space Representations of Items for Recommendations Using Word Embedding Models,We present a method of generating item recommendations by learning item feature vector embeddings. Our work is analogous to approaches like Word2Vec or Glove used to generate a good vector representation of words in a natural language corpus. We treat the items that a user interacted with as analogous to words and the string of items interacted with in a session as sentences. Our embedding generates semantically related clusters and the item vectors generated can be used to compute item similarity which can be used to drive product recommendations. Our method also allows us to use the feature vectors in other machine learning systems. We validate our method on the MovieLens dataset.,"Balaji Krishnamurthy,Nikaash Puri,Raghavender Goel",,2016,,,
537,Propensity score prediction for electronic healthcare databases using Super Learner and High-dimensional Propensity Score Methods,ABSTRACTThe optimal learner for prediction modeling varies depending on the underlying data-generating distribution. Super Learner (SL) is a generic ensemble learning algorithm that uses cross-vali...,"Cheng Ju,Mary Combs,Samuel D. Lendle,Jessica M. Franklin,Richard Wyss,Sebastian Schneeweiss,Mark J. van der Laan,Mark J. van der Laan,Mark J. van der Laan",Journal of Applied Statistics,2019,,,
538,On the Value of Reminders within E-Commerce Recommendations,"Most research in recommender systems is focused on the problem of identifying and ranking items that are relevant for the individual users but unknown to them. The potential value of such systems is to help users discover new items, e.g., in e-commerce settings. Many real-world systems however also utilize recommendation lists for a different goal, namely to remind users of items that they have viewed or consumed in the past. In this work, we aim to quantify the value of such reminders in recommendation lists (""recominders""), which has to our knowledge not been done in the past. We first report the results of a live experiment in which we applied a naive reminding strategy on an online platform and compare them with results obtained through different offline analyses. We then propose more elaborate reminding techniques, which aim to avoid reminders of too obvious or of already outdated items. Overall, our results show that although reminders do not lead to new item discoveries, they can be valuable both for users and service providers.","Lukas Lerche,Dietmar Jannach,Malte Ludewig",,2016,,,
539,Human Protein Subcellular Localization with Integrated Source and Multi-label Ensemble Classifier.,"Predicting protein subcellular location is necessary for understanding cell function. Several machine learning methods have been developed for computational prediction of primary protein sequences because wet experiments are costly and time consuming. However, two problems still exist in state-of-the-art methods. First, several proteins appear in different subcellular structures simultaneously, whereas current methods only predict one protein sequence in one subcellular structure. Second, most software tools are trained with obsolete data and the latest new databases are missed. We proposed a novel multi-label classification algorithm to solve the first problem and integrated several latest databases to improve prediction performance. Experiments proved the effectiveness of the proposed method. The present study would facilitate research on cellular proteomics.","Xiaotong Guo,Fulin Liu,Ying Ju,Ying Ju,Ying Ju,Zhen Wang,Chunyu Wang",Scientific Reports,2016,,,
540,Improved Recurrent Neural Networks for Session-based Recommendations,"Recurrent neural networks (RNNs) were recently proposed for the session-based recommendation task. The models showed promising improvements over traditional recommendation approaches. In this work, we further study RNN-based models for session-based recommendations. We propose the application of two techniques to improve model performance, namely, data augmentation, and a method to account for shifts in the input data distribution. We also empirically study the use of generalised distillation, and a novel alternative model that directly predicts item embeddings. Experiments on the RecSys Challenge 2015 dataset demonstrate relative improvements of 12.8% and 14.8% over previously reported results on the Recall@20 and Mean Reciprocal Rank@20 metrics respectively.","Yong Kiam Tan,Xinxing Xu,Yong Liu,Yong Liu,Yong Liu",,2016,,,
541,Stock trend prediction using news sentiment analysis,"Efficient Market Hypothesis is the popular theory about stock prediction. With its failure much research has been carried in the area of prediction of stocks. This project is about taking non quantifiable data such as financial news articles about a company and predicting its future stock trend with news sentiment classification. Assuming that news articles have impact on stock market, this is an attempt to study relationship between news and stock trend. To show this, we created three different classification models which depict polarity of news articles being positive or negative. Observations show that RF and SVM perform well in all types of testing. Na\""ive Bayes gives good result but not compared to the other two. Experiments are conducted to evaluate various aspects of the proposed model and encouraging results are obtained in all of the experiments. The accuracy of the prediction model is more than 80% and in comparison with news random labeling with 50% of accuracy; the model has increased the accuracy by 30%.","Joshi Kalyani,H. N. Bharathi,Rao Jyothi",arXiv: Computation and Language,2016,,,
542,Meta-learning with memory-augmented neural networks,"Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of ""one-shot learning."" Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.","Adam Santoro,Sergey Bartunov,Matthew Botvinick,Daan Wierstra,Timothy Lillicrap",,2016,,,
543,Multi-Rate Deep Learning for Temporal Recommendation,"Modeling temporal behavior in recommendation systems is an important and challenging problem. Its challenges come from the fact that temporal modeling increases the cost of parameter estimation and inference, while requiring large amount of data to reliably learn the model with the additional time dimensions. Therefore, it is often difficult to model temporal behavior in large-scale real-world recommendation systems. In this work, we propose a novel deep neural network based architecture that models the combination of long-term static and short-term temporal user preferences to improve the recommendation performance. To train the model efficiently for large-scale applications, we propose a novel pre-train method to reduce the number of free parameters significantly. The resulted model is applied to a real-world data set from a commercial News recommendation system. We compare to a set of established baselines and the experimental results show that our method outperforms the state-of-the-art significantly.","Yang Song,Yang Song,Yang Song,Ali Elkahky,Xiaodong He,Xiaodong He",,2016,,,
544,A Dynamic Recurrent Model for Next Basket Recommendation,"Next basket recommendation becomes an increasing concern. Most conventional models explore either sequential transaction features or general interests of users. Further, some works treat users' general interests and sequential behaviors as two totally divided matters, and then combine them in some way for next basket recommendation. Moreover, the state-of-the-art models are based on the assumption of Markov Chains (MC), which only capture local sequential features between two adjacent baskets. In this work, we propose a novel model, Dynamic REcurrent bAsket Model (DREAM), based on Recurrent Neural Network (RNN). DREAM not only learns a dynamic representation of a user but also captures global sequential features among baskets. The dynamic representation of a specific user can reveal user's dynamic interests at different time, and the global sequential features reflect interactions of all baskets of the user over time. Experiment results on two public datasets indicate that DREAM is more effective than the state-of-the-art models for next basket recommendation.","Feng Yu,Qiang Liu,Qiang Liu,Shu Wu,Liang Wang,Liang Wang,Liang Wang,Tieniu Tan",,2016,,,
545,Wide & Deep Learning for Recommender Systems,"Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.","Heng-Tze Cheng,Levent Koc,Jeremiah Harmsen,Tal Shaked,Tushar Deepak Chandra,Hrishi Aradhye,Glen Anderson,Greg S. Corrado,Wei Chai,Mustafa Ispir,Rohan Anil,Zakaria Haque,Lichan Hong,Vihan Jain,Xiaobing Liu,Hemal Shah",,2016,,,
546,Recognizing the Gradual Changes in sEMG Characteristics Based on Incremental Learning of Wavelet Neural Network Ensemble,"Most myoelectric prosthetic hands use a fixed pattern recognition model to identify the user's hand motion commands. Since surface electromyogram (sEMG) characteristics vary with time, it is difficult to employ the fixed pattern recognition model in identifying hand motion commands stably for a long period of time. In order to adapt to the gradual changes in sEMG characteristics, we utilized incremental learning based on the wavelet neural network (WNN) ensemble, and used negative correlation learning (NCL) to train it. To verify the effect of the proposed method, a group of subjects executed six hand motions in a continual experiment for more than 2 h. Compared with the fixed pattern recognition model, the classification accuracy rate of incremental learning with nonintegration becomes substantially improved. In addition, the results of the WNN ensemble with the fixed-size mode are more stable than those of the WNN ensemble with the growth mode. The experimental results demonstrate that our method can recognize the gradual changes in sEMG characteristics stably. Using the proposed method, the average accuracy rate is found to be 92.17%, even after a long period of time. Moreover, since the update time is short, the proposed method can be successfully applied in myoelectric prosthetic hands.","Feng Duan,Lili Dai,Lili Dai",IEEE Transactions on Industrial Electronics,2017,,,
547,Non-IID Recommender Systems: A Review and Framework of Recommendation Paradigm Shifting,"ABSTRACT   While recommendation plays an increasingly critical role in our living, study, work, and entertainment, the recommendations we receive are often for irrelevant, duplicate, or uninteresting products and services. A critical reason for such bad recommendations lies in the intrinsic assumption that recommended users and items are independent and identically distributed (IID) in existing theories and systems. Another phenomenon is that, while tremendous efforts have been made to model specific aspects of users or items, the overall user and item characteristics and their non-IIDness have been overlooked. In this paper, the non-IID nature and characteristics of recommendation are discussed, followed by the non-IID theoretical framework in order to build a deep and comprehensive understanding of the intrinsic nature of recommendation problems, from the perspective of both couplings and heterogeneity. This non-IID recommendation research triggers the paradigm shift from IID to non-IID recommendation research and can hopefully deliver informed, relevant, personalized, and actionable recommendations. It creates exciting new directions and fundamental solutions to address various complexities including cold-start, sparse data-based, cross-domain, group-based, and shilling attack-related issues.",Longbing Cao,Engineering,2016,,,
548,Sortino Ratio Based Portfolio Optimization Considering EVs and Renewable Energy in Microgrid Power Market,"Portfolio optimization in finance is the optimal allocation of financial assets in different stocks, mutual funds, bonds, etc. to maximize the returns with risk tolerance. Sortino ratio is a measure for calculating risk adjusted return of investment portfolios. Here, it is adapted for power portfolio optimization in microgrid where total load demand (including losses) is optimally distributed to different microsources so that profit per unit risk of aggregator is maximized. The diminishment in profit (from energy and reserve markets) with reference to a target profit, for different levels of uncertainties in renewable energy and electric vehicles (EVs), is consolidated to find an estimate of risk. The profit relating to deterministic forecasted data of renewable energy and pre-dispatch information from the EV parking lots is considered as the risk free target profit. The reserve market is balanced using demand response, grid power purchase, EV discharging, and other dispatchable energy sources to compensate possible discrepancy between scheduled and actual dispatch. Stochastic weight tradeoff particle swarm optimization (SWT-PSO) is used to maximize the Sortino ratio subjected to constraints of a modified backward--forward sweep (BFS) power flow problem. The results are found to be better in terms of reduced financial risk and increased robustness to uncertainties.","Vivek Mohan,Jai Govind Singh,Weerakorn Ongsakul,Weerakorn Ongsakul,Weerakorn Ongsakul",IEEE Transactions on Sustainable Energy,2017,,,
549,Random classification noise defeats all convex potential boosters,"A broad class of boosting algorithms can be interpreted as performing coordinate-wise gradient descent to minimize some potential function of the margins of a data set. This class includes AdaBoost, LogitBoost, and other widely used and well-studied boosters. In this paper we show that for a broad class of convex potential functions, any such boosting algorithm is highly susceptible to random classification noise. We do this by showing that for any such booster and any nonzero random classification noise rate ?, there is a simple data set of examples which is efficiently learnable by such a booster if there is no noise, but which cannot be learned to accuracy better than 1/2 if there is random classification noise at rate ?. This holds even if the booster regularizes using early stopping or a bound on the L 1 norm of the voting weights. This negative result is in contrast with known branching program based boosters which do not fall into the convex potential function framework and which can provably learn to high accuracy in the presence of random classification noise.","Philip M. Long,Rocco A. Servedio",Machine Learning,2010,,,
550,On the risk prediction and analysis of soft information in finance reports,"We attempt in this paper to utilize soft information in financial reports to analyze financial risk among companies. Specifically, on the basis of the text information in financial reports, which is the so-called soft information, we apply analytical techniques to study relations between texts and financial risk. Furthermore, we conduct a study on financial sentiment analysis by using a finance-specific sentiment lexicon to examine the relations between financial sentiment words and financial risk. A large collection of financial reports published annually by publicly-traded companies is employed to conduct our experiments; moreover, two analytical techniques – regression and ranking methods – are applied to conduct these analyses. The experimental results show that, based on a bag-of-words model, using only financial sentiment words results in performance comparable to using the whole texts; this confirms the importance of financial sentiment words with respect to risk prediction. In addition to this performance comparison, via the learned models, we draw attention to some strong and interesting correlations between texts and financial risk. These valuable findings yield greater insight and understanding into the usefulness of soft information in financial reports and can be applied to a broad range of financial and accounting applications.","Ming-Feng Tsai,Chuan-Ju Wang",European Journal of Operational Research,2017,,,
551,Meta-Prod2Vec: Product Embeddings Using Side-Information for Recommendation,"We propose Meta-Prod2vec, a novel method to compute item similarities for recommendation that leverages existing item metadata. Such scenarios are frequently encountered in applications such as content recommendation, ad targeting and web search. Our method leverages past user interactions with items and their attributes to compute low-dimensional embeddings of items. Specifically, the item metadata is injected into the model as side information to regularize the item embeddings. We show that the new item representations lead to better performance on recommendation tasks on an open music dataset.","Flavian Vasile,Flavian Vasile,Elena Smirnova,Alexis Conneau",,2016,,,
552,Modeling Sequential Preferences with Dynamic User and Context Factors,"Users express their preferences for items in diverse forms, through their liking for items, as well as through the sequence in which they consume items. The latter, referred to as “sequential preference”, manifests itself in scenarios such as song or video playlists, topics one reads or writes about in social media, etc. The current approach to modeling sequential preferences relies primarily on the sequence information, i.e., which item follows another item. However, there are other important factors, due to either the user or the context, which may dynamically affect the way a sequence unfolds. In this work, we develop generative modeling of sequences, incorporating dynamic user-biased emission and context-biased transition for sequential preference. Experiments on publicly-available real-life datasets as well as synthetic data show significant improvements in accuracy at predicting the next item in a sequence.","Duc-Trong Le,Yuan Fang,Hady W. Lauw",,2016,,,
553,Factorization Meets the Item Embedding: Regularizing Matrix Factorization with Item Co-occurrence,"Matrix factorization (MF) models and their extensions are standard in modern recommender systems. MF models decompose the observed user-item interaction matrix into user and item latent factors. In this paper, we propose a co-factorization model, CoFactor, which jointly decomposes the user-item interaction matrix and the item-item co-occurrence matrix with shared item latent factors. For each pair of items, the co-occurrence matrix encodes the number of users that have consumed both items. CoFactor is inspired by the recent success of word embedding models (e.g., word2vec) which can be interpreted as factorizing the word co-occurrence matrix. We show that this model significantly improves the performance over MF models on several datasets with little additional computational overhead. We provide qualitative results that explain how CoFactor improves the quality of the inferred factors and characterize the circumstances where it provides the most significant improvements.","Dawen Liang,Jaan Altosaar,Laurent Charlin,David M. Blei",,2016,,,
554,Deep learning for stock prediction using numerical and textual information,"This paper proposes a novel application of deep learning models, Paragraph Vector, and Long Short-Term Memory (LSTM), to financial time series forecasting. Investors make decisions according to various factors, including consumer price index, price-earnings ratio, and miscellaneous events reported in newspapers. In order to assist their decisions in a timely manner, many automatic ways to analyze those information have been proposed in the last decade. However, many of them used either numerical or textual information, but not both for a single company. In this paper, we propose an approach that converts newspaper articles into their distributed representations via Paragraph Vector and models the temporal effects of past events on opening prices about multiple companies with LSTM. The performance of the proposed approach is demonstrated on real-world data of fifty companies listed on Tokyo Stock Exchange.","Ryo Akita,Akira Yoshihara,Takashi Matsubara,Kuniaki Uehara",,2016,,,
555,Parallel Recurrent Neural Network Architectures for Feature-rich Session-based Recommendations,"Real-life recommender systems often face the daunting task of providing recommendations based only on the clicks of a user session. Methods that rely on user profiles -- such as matrix factorization -- perform very poorly in this setting, thus item-to-item recommendations are used most of the time. However the items typically have rich feature representations such as pictures and text descriptions that can be used to model the sessions. Here we investigate how these features can be exploited in Recurrent Neural Network based session models using deep learning. We show that obvious approaches do not leverage these data sources. We thus introduce a number of parallel RNN (p-RNN) architectures to model sessions based on the clicks and the features (images and text) of the clicked items. We also propose alternative training strategies for p-RNNs that suit them better than standard training. We show that p-RNN architectures with proper training have significant performance improvements over feature-less session models while all session-based models outperform the item-to-item type baseline.","Balázs Hidasi,Massimo Quadrana,Alexandros Karatzoglou,Domonkos Tikk",,2016,,,
556,Modelling Contextual Information in Session-Aware Recommender Systems with Neural Networks,"Preparing recommendations for unknown users or such that correctly respond to the short-term needs of a particular user is one of the fundamental problems for e-commerce. Most of the common Recommender Systems assume that user identification must be explicit. In this paper a Session-Aware Recommender System approach is presented where no straightforward user information is required. The recommendation process is based only on user activity within a single session, defined as a sequence of events. This information is incorporated in the recommendation process by explicit context modeling with factorization methods and a novel approach with Recurrent Neural Network (RNN). Compared to the session modeling approach, RNN directly models the dependency of user observed sequential behavior throughout its recurrent structure. The evaluation discusses the results based on sessions from real-life system with ephemeral items (identified only by the set of their attributes) for the task of top-n best recommendations.",Bartłomiej Twardowski,,2016,,,
557,RecSys'16 Workshop on Deep Learning for Recommender Systems (DLRS),"We believe that Deep Learning is one of the next big things in Recommendation Systems technology. The past few years have seen the tremendous success of deep neural networks in a number of complex tasks such as computer vision, natural language processing and speech recognition. Despite this, only little work has been published on Deep Learning methods for Recommender Systems. Notable recent application areas are music recommendation, news recommendation, and session-based recommendation. The aim of the workshop is to encourage the application of Deep Learning techniques in Recommender Systems, to promote research in deep learning methods for Recommender Systems, and to bring together researchers from the Recommender Systems and Deep Learning communities.","Alexandros Karatzoglou,Balázs Hidasi,Domonkos Tikk,Oren Sar-Shalom,Haggai Roitman,Bracha Shapira,Lior Rokach",,2016,,,
558,The Contextual Turn: from Context-Aware to Context-Driven Recommender Systems,"A critical change has occurred in the status of context in recommender systems. In the past, context has been considered 'additional evidence'. This past picture is at odds with many present application domains, where user and item information is scarce. Such domains face continuous cold start conditions and must exploit session rather than user information. In this paper, we describe the `Contextual Turn?: the move towards context-driven recommendation algorithms for which context is critical, rather than additional. We cover application domains, algorithms that promise to address the challenges of context-driven recommendation, and the steps that the community has taken to tackle context-driven problems. Our goal is to point out the commonalities of context-driven problems, and urge the community to address the overarching challenges that context-driven recommendation poses.","Roberto Pagano,Paolo Cremonesi,Martha Larson,Balázs Hidasi,Domonkos Tikk,Alexandros Karatzoglou,Massimo Quadrana",,2016,,,
559,Addressing Cold Start for Next-song Recommendation,"The cold start problem arises in various recommendation applications. In this paper, we propose a tensor factorization-based algorithm that exploits content features extracted from music audio to deal with the cold start problem for the emerging application next-song recommendation. Specifically, the new algorithm learns sequential behavior to predict the next song that a user would be interested in based on the last song the user just listened to. A unique characteristic of the algorithm is that it learns and updates the mapping between the audio feature space and the item latent space each time during the iterations of the factorization process. This way, the content features can be better exploited in forming the latent features for both users and items, leading to more effective solutions for cold-start recommendation. Evaluation on a large-scale music recommendation dataset shows that the recommendation result of the proposed algorithm exhibits not only higher accuracy but also better novelty and diversity, suggesting its applicability in helping a user explore new items in next-item recommendation. Our implementation is available at https://github.com/fearofchou/ALMM.","Szu-Yu Chou,Yi-Hsuan Yang,Jyh-Shing Roger Jang,Jyh-Shing Roger Jang,Jyh-Shing Roger Jang,Yu-Ching Lin",,2016,,,
560,An extension of fuzzy TOPSIS for a group decision making with an application to tehran stock exchange,"Display Omitted We propose three versions of fuzzy TOPSIS for solving group MADM problems.We apply fuzzy set theory to handle the imprecise information in the real-world problems.We take advantage of fuzzy-valued distance and fuzzy ranking method to provide a more rational decision-making process.We apply the proposed methods in the Tehran stock exchange. In financial markets, investors attempt to maximize their profits within a constructed portfolio with the aim of optimizing the tradeoffs between risk and return across the many stocks. This requires proper handling of conflicting factors, which can benefit from the domain of multiple criteria decision making (MCDM). However, the indexes and factors representing the stock performance are often imprecise or vague and this should be represented by linguistic terms characterized by fuzzy numbers. The aim of this research is to first develop three group MCDM methods, then use them for selecting undervalued stocks by dint of financial ratios and subjective judgments of experts. This study proposes three versions of fuzzy TOPSIS (Technique for Order Preference by Similarity to Ideal Solution): conventional TOPSIS (C-TOPSIS), adjusted TOPSIS (A-TOPSIS) and modified TOPSIS (M-TOPSIS) where a new fuzzy distance measure, derived from the confidence level of the experts and fuzzy performance ratings have been included in the proposed methods. The practical aspects of the proposed methods are demonstrated through a case study in the Tehran stock exchange (TSE), which is timely given the need for investors to select undervalued stocks in untapped markets in the anticipation of easing economic sanctions from a change in recent government leadership.","Adel Hatami-Marbini,Fatemeh Kangi",Applied Soft Computing,2017,,,
561,Multi-level fusion of audio and visual features for speaker identification,"This paper explores the fusion of audio and visual evidences through a multi-level hybrid fusion architecture based on dynamic Bayesian network (DBN), which combines model level and decision level fusion to achieve higher performance. In model level fusion, a new audio-visual correlative model (AVCM) based on DBN is proposed, which describes both the inter-correlations and loose timing synchronicity between the audio and video streams. The experiments on the CMU database and our own homegrown database both demonstrate that the methods can improve the accuracies of audio-visual bimodal speaker identification at all levels of acoustic signal-to-noise-ratios (SNR) from 0dB to 30dB with varying acoustic conditions.","Zhiyong Wu,Lianhong Cai,Helen Meng",Lecture Notes in Computer Science,2006,,,
562,Learning to Count with CNN Boosting,"In this paper, we address the task of object counting in images. We follow modern learning approaches in which a density map is estimated directly from the input image. We employ CNNs and incorporate two significant improvements to the state of the art methods: layered boosting and selective sampling. As a result, we manage both to increase the counting accuracy and to reduce processing time. Moreover, we show that the proposed method is effective, even in the presence of labeling errors. Extensive experiments on five different datasets demonstrate the efficacy and robustness of our approach. Mean Absolute error was reduced by 20 % to 35 %. At the same time, the training time of each CNN has been reduced by 50 %.","Elad Walach,Lior Wolf",,2016,,,
563,The Option-Critic Architecture,"Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup & Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.","Pierre-Luc Bacon,Jean Harb,Doina Precup,Doina Precup,Doina Precup",arXiv: Artificial Intelligence,2016,,,
564,Fusing Similarity Models with Markov Chains for Sparse Sequential Recommendation,"Predicting personalized sequential behavior is a key task for recommender systems. In order to predict user actions such as the next product to purchase, movie to watch, or place to visit, it is essential to take into account both long-term user preferences and sequential patterns (i.e., short-term dynamics). Matrix Factorization and Markov Chain methods have emerged as two separate but powerful paradigms for modeling the two respectively. Combining these ideas has led to unified methods that accommodate long- and short-term dynamics simultaneously by modeling pairwise user-item and item-item interactions. 
In spite of the success of such methods for tackling dense data, they are challenged by sparsity issues, which are prevalent in real-world datasets. In recent years, similarity-based methods have been proposed for (sequentially-unaware) item recommendation with promising results on sparse datasets. In this paper, we propose to fuse such methods with Markov Chains to make personalized sequential recommendations. We evaluate our method, Fossil, on a variety of large, real-world datasets. We show quantitatively that Fossil outperforms alternative algorithms, especially on sparse datasets, and qualitatively that it captures personalized dynamics and is able to make meaningful recommendations.","Ruining He,Julian McAuley",arXiv: Information Retrieval,2016,,,
565,Learning Distributed Representations for Recommender Systems with a Network Embedding Approach,"In this paper, we present a novel perspective to address recommendation tasks by utilizing the network representation learning techniques. Our idea is based on the observation that the input of typical recommendation tasks can be formulated as graphs. Thus, we propose to use the k-partite adoption graph to characterize various kinds of information in recommendation tasks. Once the historical adoption records have been transformed into a graph, we can apply the network embedding approach to learn vertex embeddings on the k-partite adoption network. Embeddings for different kinds of information are projected into the same latent space, where we can easily measure the relatedness between multiple vertices on the graph using some similarity measurements. In this way, the recommendation task has been casted into a similarity evaluation process using embedding vectors. The proposed approach is both general and scalable. To evaluate the effectiveness of the proposed approach, we construct extensive experiments on two different recommendation tasks using real-world datasets. The experimental results have shown the superiority of our approach. To the best of our knowledge, it is the first time that a network representation learning approach has been applied to recommendation tasks.","Wayne Xin Zhao,Jin Huang,Ji-Rong Wen,Ji-Rong Wen",,2016,,,
566,Temporal ensembling for semi-supervised learning,"In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.","Samuli Laine,Timo Aila",,2017,,,
567,A survey of neural network ensembles,"A neural network ensemble combines a finite number of neural networks or other types of predictors, which are trained simultaneously for a common classification task. Compared with a single neural network, the ensemble is able to efficiently improve the generalization ability of the classifier. The objective of this paper is to introduce existing research work on the neural network ensembles, including effective analysis, general implement steps of ensembles, and traditional technologies for training component neural networks, and also description the applications of it","Ying Zhao,Jun Gao,Xuezhi Yang",,2005,,,
568,MRI segmentation fusion for brain tumor detection,"Abstract   The process of manually generating precise segmentations of brain tumors from magnetic resonance images (MRI) is time-consuming and error-prone. We present a new algorithm, Potential Field Segmentation (PFS), and propose the use of ensemble approaches that combine the results generated by PFS and other methods to achieve a fused segmentation. For the PFS method, we build on our recently proposed clustering algorithm, Potential Field Clustering, which is based on an analogy with the concept of potential field in Physics. We view the intensity of a pixel in an MRI as a “mass” that creates a potential field. Specifically, for each pixel in the MRI, the potential field is computed and, if smaller than an adaptive potential threshold, the pixel is associated with the tumor region. This “small potential” segmentation criterion is intuitively valid because tumor pixels have larger “mass” and thus the potential of surrounding regions is also much larger than in other regions of smaller or no “mass”. We evaluate the performance of the different methods, including the ensemble approaches, on the publicly available Brain Tumor Image Segmentation (BRATS) MRI benchmark database.","I. Cabria,I. Cabria,I. Cabria,Iker Gondra",Information Fusion,2017,,,
569,Particle swarm optimization,"Particle swarm optimization (PSO) has undergone many changes since its introduction in 1995. As researchers have learned about the technique, they have derived new versions, developed new applications, and published theoretical studies of the effects of the various parameters and aspects of the algorithm. This paper comprises a snapshot of particle swarming from the authors’ perspective, including variations in the algorithm, current and ongoing research, applications and open problems.","Riccardo Poli,Russell C. Eberhart,James Kennedy,Tim Blackwell",Swarm Intelligence,2007,,,
570,Opinion mining and sentiment analysis,"In the recent years, micro blogging platforms like Twitter have become instrumental in gauging public mood. So it makes it a feasible predictive strategy to speculate the rise and fall of stock prices. This paper aims to undertake a stepwise methodology to determine the effects of an average person's tweets over fluctuation of stock prices of a multinational firm called Samsung Electronics Ltd. It involves extracting tweets from twitter, data cleansing and application of a suitable algorithm in order to get the adequate sentiment analysis. The vast impact created by twitter data feed has been greatly studied in this paper. Attempts have been made to design an algorithm which works well analysing the positive, negative and neutral tweets.","Rushlene Kaur Bakshi,Navneet Kaur,Ravneet Kaur,Gurpreet Kaur,Gurpreet Kaur,Gurpreet Kaur",,2016,,,
571,Sentiment analysis in financial texts,"The growth of financial texts in the wake of big data has challenged most organizations and brought escalating demands for analysis tools. In general, text streams are more challenging to handle than numeric data streams. Text streams are unstructured by nature, but they represent collective expressions that are of value in any financial decision. It can be both daunting and necessary to make sense of unstructured textual data. In this study, we address key questions related to the explosion of interest in how to extract insight from unstructured data and how to determine if such insight provides any hints concerning the trends of financial markets. A sentiment analysis engine (SAE) is proposed which takes advantage of linguistic analyses based on grammars. This engine extends sentiment analysis not only at the word token level, but also at the phrase level within each sentence. An assessment heuristic is applied to extract the collective expressions shown in the texts. Also, three evaluations are presented to assess the performance of the engine. First, several standard parsing evaluation metrics are applied on two treebanks. Second, a benchmark evaluation using a dataset of English movie review is conducted. Results show our SAE outperforms the traditional bag of words approach. Third, a financial text stream with twelve million words that aligns with a stock market index is examined. The evaluation results and their statistical significance provide strong evidence of a long persistence in the mood time series generated by the engine. In addition, our approach establishes grounds for belief that the sentiments expressed through text streams are helpful for analyzing the trends in a stock market index, although such sentiments and market indices are normally considered to be completely uncorrelated. To explain a classifier-based sentiment parser for financial textsTo demonstrate how to assign the polarity of phrases using an assessment heuristicTo provide statistical tests using twelve million words to attest its significance","Samuel W. K. Chan,Mickey W. C. Chong",,2017,,,
572,Training deep neural networks on imbalanced data sets,"Deep learning has become increasingly popular in both academic and industrial areas in the past years. Various domains including pattern recognition, computer vision, and natural language processing have witnessed the great power of deep networks. However, current studies on deep learning mainly focus on data sets with balanced class labels, while its performance on imbalanced data is not well examined. Imbalanced data sets exist widely in real world and they have been providing great challenges for classification tasks. In this paper, we focus on the problem of classification using deep network on imbalanced data sets. Specifically, a novel loss function called mean false error together with its improved version mean squared false error are proposed for the training of deep networks on imbalanced data sets. The proposed method can effectively capture classification errors from both majority class and minority class equally. Experiments and comparisons demonstrate the superiority of the proposed approach compared with conventional methods in classifying imbalanced data sets on deep neural networks.","Shoujin Wang,Wei Liu,Wei Liu,Wei Liu,Jia Wu,Longbing Cao,Qinxue Meng,Paul J. Kennedy",,2016,,,
573,Incremental Boosting Convolutional Neural Network for Facial Action Unit Recognition,"Recognizing facial action units (AUs) from spontaneous facial expressions is still a challenging problem. Most recently, CNNs have shown promise on facial AU recognition. However, the learned CNNs are often overfitted and do not generalize well to unseen subjects due to limited AU-coded training images. We proposed a novel Incremental Boosting CNN (IB-CNN) to integrate boosting into the CNN via an incremental boosting layer that selects discriminative neurons from the lower layer and is incrementally updated on successive mini-batches. In addition, a novel loss function that accounts for errors from both the incremental boosted classifier and individual weak classifiers was proposed to fine-tune the IB-CNN. Experimental results on four benchmark AU databases have demonstrated that the IB-CNN yields significant improvement over the traditional CNN and the boosting CNN without incremental learning, as well as outperforming the state-of-the-art CNN-based methods in AU recognition. The improvement is more impressive for the AUs that have the lowest frequencies in the databases.","Shizhong Han,Zibo Meng,Ahmed Shehab Khan,Yan Tong",arXiv: Computer Vision and Pattern Recognition,2017,,,
574,Ensemble feature selection: Homogeneous and heterogeneous approaches,"Abstract   In the last decade, ensemble learning has become a prolific discipline in pattern recognition, based on the assumption that the combination of the output of several models obtains better results than the output of any individual model. On the basis that the same principle can be applied to feature selection, we describe two approaches: (i) homogeneous, i.e., using the same feature selection method with different training data and distributing the dataset over several nodes; and (ii) heterogeneous, i.e., using different feature selection methods with the same training data. Both approaches are based on combining rankings of features that contain all the ordered features. The results of the base selectors are combined using different combination methods, also called aggregators, and a practical subset is selected according to several different threshold values (traditional values based on fixed percentages, and more novel automatic methods based on data complexity measures). In testing using a Support Vector Machine as a classifier, ensemble results for seven datasets demonstrate performance that is at least comparable and often better than the performance of individual feature selection methods.","Borja Seijo-Pardo,Iago Porto-Díaz,Verónica Bolón-Canedo,Amparo Alonso-Betanzos",Knowledge Based Systems,2017,,,
575,Sample Efficient Actor-Critic with Experience Replay,"This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.","Ziyu Wang,Victor Bapst,Nicolas Heess,Volodymyr Mnih,Rémi Munos,Koray Kavukcuoglu,Nando de Freitas",arXiv: Learning,2016,,,
576,GuessWhat?! Visual Object Discovery through Multi-modal Dialogue,"We introduce GuessWhat?!, a two-player guessing game as a testbed for research on the interplay of computer vision and dialogue systems. The goal of the game is to locate an unknown object in a rich image scene by asking a sequence of questions. Higher-level image understanding, like spatial reasoning and language grounding, is required to solve the proposed task. Our key contribution is the collection of a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images. We explain our design decisions in collecting the dataset and introduce the oracle and questioner tasks that are associated with the two players of the game. We prototyped deep learning models to establish initial baselines of the introduced tasks.","Harm de Vries,Florian Strub,Sarath Chandar,Olivier Pietquin,Hugo Larochelle,Aaron Courville",,2017,,,
577,Multi-agent reinforcement learning,Reinforcement learning deals with the problem of how to map situations (states) to actions so as to maximize a numerical reward while interacting with dynamical and uncertain environment. Within the framework of Markov Decision Processes (MDPs) these methods are typically based on approximate dynamic programming using appropriate calculation/approximation of the value function. In this work we propose new algorithms for multi-agent distributed iterative value function approximation where the agents are allowed to have different behavior policies while evaluating the response to a single target policy. The algorithms assume linear parametrization of the value function and are based on consensus-based distributed stochastic approximation. Under appropriate assumptions on the time-varying network topology and the overall state-visiting distributions of the agents we prove weak convergence of the parameter estimates to the globally optimal point. It is demonstrated that the agents are able to together reach this solution even when the individual agents cannot.,Milos S. Stankovic,,2016,,,
578,Recognition of emotions using multimodal physiological signals and an ensemble deep learning model,"An ensemble of deep classifiers is built for recognizing emotions using multimodal physiological signals.The higher-level abstractions of physiological features of each modality are separately extracted by deep hidden neurons in member stacked-autoencoders.The minimal structure of the deep model is identified according to a structural loss function for local geometrical information preservation.The physiological feature abstractions are merged via an adjacent-graph based fusion network with hierarchical layers. Background and ObjectiveUsing deep-learning methodologies to analyze multimodal physiological signals becomes increasingly attractive for recognizing human emotions. However, the conventional deep emotion classifiers may suffer from the drawback of the lack of the expertise for determining model structure and the oversimplification of combining multimodal feature abstractions. MethodsIn this study, a multiple-fusion-layer based ensemble classifier of stacked autoencoder (MESAE) is proposed for recognizing emotions, in which the deep structure is identified based on a physiological-data-driven approach. Each SAE consists of three hidden layers to filter the unwanted noise in the physiological features and derives the stable feature representations. An additional deep model is used to achieve the SAE ensembles. The physiological features are split into several subsets according to different feature extraction approaches with each subset separately encoded by a SAE. The derived SAE abstractions are combined according to the physiological modality to create six sets of encodings, which are then fed to a three-layer, adjacent-graph-based network for feature fusion. The fused features are used to recognize binary arousal or valence states. ResultsDEAP multimodal database was employed to validate the performance of the MESAE. By comparing with the best existing emotion classifier, the mean of classification rate and F-score improves by 5.26%. ConclusionsThe superiority of the MESAE against the state-of-the-art shallow and deep emotion classifiers has been demonstrated under different sizes of the available physiological instances.","Zhong Yin,Mengyuan Zhao,Yongxiong Wang,Jingdong Yang,Jianhua Zhang",Computer Methods and Programs in Biomedicine,2017,,,
579,TD algorithm for the variance of return and mean-variance reinforcement learning,,"Makoto Sato,Hajime Kimura,Shibenobu Kobayashi",Transactions of The Japanese Society for Artificial Intelligence,2001,,,
580,The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process,"Many events occur in the world. Some event types are stochastically excited or inhibited---in the sense of having their probabilities elevated or decreased---by patterns in the sequence of previous events. Discovering such patterns can help us predict which type of event will happen next and when. We model streams of discrete events in continuous time, by constructing a neurally self-modulating multivariate point process in which the intensities of multiple event types evolve according to a novel continuous-time LSTM. This generative model allows past events to influence the future in complex and realistic ways, by conditioning future event intensities on the hidden state of a recurrent neural network that has consumed the stream of past events. Our model has desirable qualitative properties. It achieves competitive likelihood and predictive accuracy on real and synthetic datasets, including under missing-data conditions.","Hongyuan Mei,Jason Eisner",arXiv: Learning,2016,,,
581,Robot gains social intelligence through multimodal deep reinforcement learning,"For robots to coexist with humans in a social world like ours, it is crucial that they possess human-like social interaction skills. Programming a robot to possess such skills is a challenging task. In this paper, we propose a Multimodal Deep Q-Network (MDQN) to enable a robot to learn human-like interaction skills through a trial and error method. This paper aims to develop a robot that gathers data during its interaction with a human, and learns human interaction behavior from the high dimensional sensory information using end-to-end reinforcement learning. This paper demonstrates that the robot was able to learn basic interaction skills successfully, after 14 days of interacting with people.","Ahmed Hussain Qureshi,Yutaka Nakamura,Yoshikawa Yuichiro,Yuichiro Yoshikawa,Hiroshi Ishiguro",,2016,,,
582,Stock market prediction and Portfolio selection models: a survey,"Abstract Stock data is known to be chaotic in nature and it is a challenging task to predict the non-linear patterns of such data. Forming an optimal portfolio of stocks is yet another challenging task and limitations do exist in every portfolio model in some form or the other. In order to resolve such problems, many artificial intelligence models have appeared in literature which are also known as intelligent models. Prediction of stocks as well as investing in appropriate stocks has remained in focus among investors, industrialists as well as among academicians. This paper surveys important published articles in the related area available in literature. This survey highlights traditional mathematical models available in articles which have appeared decades back till artificial intelligence based models available in recent articles.","Akhter Mohiuddin Rather,V. N. Sastry,Arun Agarwal",Opsearch,2017,,,
583,Modelling Session Activity with Neural Embedding.,,"Oren Barkan,Yael Brumer,Noam Koenigstein",,2016,,,
584,Empirical Mode Decomposition based ensemble deep learning for load demand time series forecasting,"Graphical abstractDisplay Omitted HighlightsAn ensemble deep learning method has been proposed for load demand forecasting.The hybrid method composes of Empirical Mode Decomposition and Deep Belief Network.Empirical Mode Decomposition based methods outperform the single structure models.Deep learning shows more advantages when the forecasting horizon increases. Load demand forecasting is a critical process in the planning of electric utilities. An ensemble method composed of Empirical Mode Decomposition (EMD) algorithm and deep learning approach is presented in this work. For this purpose, the load demand series were first decomposed into several intrinsic mode functions (IMFs). Then a Deep Belief Network (DBN) including two restricted Boltzmann machines (RBMs) was used to model each of the extracted IMFs, so that the tendencies of these IMFs can be accurately predicted. Finally, the prediction results of all IMFs can be combined by either unbiased or weighted summation to obtain an aggregated output for load demand. The electricity load demand data sets from Australian Energy Market Operator (AEMO) are used to test the effectiveness of the proposed EMD-based DBN approach. Simulation results demonstrated attractiveness of the proposed method compared with nine forecasting methods.","Xueheng Qiu,Ye Ren,Ponnuthurai Nagaratnam Suganthan,Gehan A. J. Amaratunga",Applied Soft Computing,2017,,,
585,Knowledge-Driven Event Embedding for Stock Prediction,"Representing structured events as vectors in continuous space offers a new way for defining dense features for natural language processing (NLP) applications. Prior work has proposed effective methods to learn event representations that can capture syntactic and semantic information over text corpus, demonstrating their effectiveness for downstream tasks such as event-driven stock prediction. On the other hand, events extracted from raw texts do not contain background knowledge on entities and relations that they are mentioned. To address this issue, this paper proposes to leverage extra information from knowledge graph, which provides ground truth such as attributes and properties of entities and encodes valuable relations between entities. Specifically, we propose a joint model to combine knowledge graph information into the objective function of an event embedding learning model. Experiments on event similarity and stock market prediction show that our model is more capable of obtaining better event embeddings and making more accurate prediction on stock market volatilities.","Xiao Ding,Lei Zhang,Yue Zhang,Ting Liu,Junwen Duan",,2016,,,
586,Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates,"Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.","Shixiang Gu,Ethan Holly,Timothy Lillicrap,Sergey Levine",,2017,,,
587,Outlier detection in complex categorical data by modelling the feature value couplings,"This paper introduces a novel unsupervised outlier detection method, namely Coupled Biased Random Walks (CBRW), for identifying outliers in categorical data with diversified frequency distributions and many noisy features. Existing pattern-based outlier detection methods are ineffective in handling such complex scenarios, as they misfit such data. CBRW estimates outlier scores of feature values by modelling feature value level couplings, which carry intrinsic data characteristics, via biased random walks to handle this complex data. The outlier scores of feature values can either measure the outlierness of an object or facilitate the existing methods as a feature weighting and selection indicator. Substantial experiments show that CBRW can not only detect outliers in complex data significantly better than the state-of-the-art methods, but also greatly improve the performance of existing methods on data sets with many noisy features.","Guansong Pang,Longbing Cao,Ling Chen",,2016,,,
588,Deep Reinforcement Learning: An Overview,"We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions. 
Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.","Yuxi Li,Yuxi Li",arXiv: Learning,2017,,,
589,Recurrent Recommender Networks,"Recommender systems traditionally assume that user profiles and movie attributes are static. Temporal dynamics are purely reactive, that is, they are inferred after they are observed, e.g. after a user's taste has changed or based on hand-engineered temporal bias corrections for movies. We propose Recurrent Recommender Networks (RRN) that are able to predict future behavioral trajectories. This is achieved by endowing both users and movies with a Long Short-Term Memory (LSTM) autoregressive model that captures dynamics, in addition to a more traditional low-rank factorization. On multiple real-world datasets, our model offers excellent prediction accuracy and it is very compact, since we need not learn latent state but rather just the state transition function.","Chao-Yuan Wu,Amr Ahmed,Alex Beutel,Alex Beutel,Alex Beutel,Alexander J. Smola,How Jing,How Jing",,2017,,,
590,Neural Survival Recommender,"The ability to predict future user activity is invaluable when it comes to content recommendation and personalization. For instance, knowing when users will return to an online music service and what they will listen to increases user satisfaction and therefore user retention.   We present a model based on Long-Short Term Memory to estimate when a user will return to a site and what their future listening behavior will be. In doing so, we aim to solve the problem of Just-In-Time recommendation, that is, to recommend the right items at the right time. We use tools from survival analysis for return time prediction and exponential families for future activity analysis. We show that the resulting multitask problem can be solved accurately, when applied to two real-world datasets.","How Jing,How Jing,Alexander J. Smola,Alex Smola,Alexander J. Smola",,2017,,,
591,Pattern graph tracking-based stock price prediction using big data,"Stock price forecasting is the most difficult field owing to irregularities. However, because stock prices sometimes show similar patterns and are determined by a variety of factors, we propose determining similar patterns in historical stock data to achieve daily stock prices with high prediction accuracy and potential rules for selecting the main factors that significantly affect the price, while simultaneously considering all factors. This study is intended at suggesting a new complex methodology that finds the optimal historical dataset with similar patterns according to various algorithms for each stock item and provides a more accurate prediction of daily stock price. First, we use a Dynamic Time Warping algorithm to find patterns with the most similar situation adjacent to a current pattern. Second, we select the determinants most affected by the stock price using feature selection based on Stepwise Regression Analysis. Moreover, we generate an artificial neural network model with selected features as training data for predicting the best stock price. Finally, we use JaroWinkler distance with Symbolic Aggregate approXimation (SAX) as a prediction accuracy measure to verify the accuracy of our model. We propose a new methodology combined Dynamic Time Warping, Stepwise Regression Analysis, and Artificial Neural Network.We use JaroWinkler distance with Symbolic Aggregate approXimation (SAX) as prediction accuracy measure to verify our model.We construct a big data processing framework to handle the overall processes using big data open sources.","Seungwoo Jeon,Bonghee Hong,Victor Chang",Future Generation Computer Systems,2018,,,
592,An automated information system to ensure quality in higher education institutions,"Despite the great efforts to assure quality in higher education institutions, the ambiguity of its related concepts and requirements constitute a big challenge when trying to implement it as an automated information system. The present work introduces a framework for an automated information system that manages the quality assurance in higher educations institutions. The aim of designing such a system is to provide an automation tool that avoids unnecessary and redundant tasks associated to quality in higher education institutions. In addition, the proposed system helps all higher education stockholders to handle and monitor their tasks. Moreover, it aims to help the quality assurance center in a higher education institution to apply its qualitys standards, and to make sure that they are being maintained and enhanced. This information system contains a core module and 17 sub-modules, which are described in this paper.","Amit Singh,Mohamed Elhoseny,Noura Metawa,Aboul Ella Hassanien",,2016,,,
593,Session-Based Recommendations Using Item Embedding,"Recent methods for learning vector space representations of words, word embedding, such as GloVe and Word2Vec have succeeded in capturing fine-grained semantic and syntactic regularities. We analyzed the effectiveness of these methods for e-commerce recommender systems by transferring the sequence of items generated by users' browsing journey in an e-commerce website into a sentence of words. We examined the prediction of fine-grained item similarity (such as item most similar to iPhone 6 64GB smart phone) and item analogy (such as iPhone 5 is to iPhone 6 as Samsung S5 is to Samsung S6) using real life users' browsing history of an online European department store. Our results reveal that such methods outperform related models such as singular value decomposition (SVD) with respect to item similarity and analogy tasks across different product categories. Furthermore, these methods produce a highly condensed item vector space representation, item embedding, with behavioral meaning sub-structure. These vectors can be used as features in a variety of recommender system applications. In particular, we used these vectors as features in a neural network based models for anonymous user recommendation based on session's first few clicks. It is found that recurrent neural network that preserves the order of user's clicks outperforms standard neural network, item-to-item similarity and SVD (recall@10 value of 42% based on first three clicks) for this task.","Asnat Greenstein-Messica,Lior Rokach,Michael Friedman",,2017,,,
594,Stabilising experience replay for deep multi-agent reinforcement learning,"Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on single-agent RL to the multi-agent setting. A major stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep Q-learning relies. This paper proposes two methods that address this problem: 1) using a multi-agent variant of importance sampling to naturally decay obsolete data and 2) conditioning each agent's value function on a fingerprint that disambiguates the age of the data sampled from the replay memory. Results on a challenging decentralised variant of StarCraft unit micro-management confirm that these methods enable the successful combination of experience replay with multi-agent RL.","Jakob Foerster,Nantas Nardelli,Gregory Farquhar,Triantafyllos Afouras,Philip H. S. Torr,Pushmeet Kohli,Shimon Whiteson",,2017,,,
595,Information Bodies: Computational Anxiety in Neal Stephenson's Snow Crash,"Cognitive science-based enactive theories of perception afford surprising insight onto a less examined component of perceptual experience: perceptual entrainment through the embodied encounter with tools. My analysis of the body/tool/perception nexus in Neal Stephenson’s  Snow Crash  (1992) introduces the concept of perceptual entrainment in two steps: first I explain Alva Noe’s claim that perception is virtual—that it takes place as an active process of environmental investigation rather than through computer-processing-like brain activity. I then take Noe’s approach to perception a step further by examining how tool use directs and amplifies our perceptual focus. I argue that in  Snow Crash , tools contain within them ideologies of repression that emerge precisely at the “technological interface,” moments when characters in the novel engage with tools that interpellate them, enable them to wield power, or blur the boundaries between control and abjection. This article analyzes the novel’s  computational anxiety —the fear of loss of self identity via an informational/mathematical determinism mapped by the tool that entrains our perception.",Judy Joshua,Interdisciplinary Literary Studies,2017,,,
596,Constrained Portfolio Optimization by Hybridized Bat Algorithm,"This paper presents the application of the hybridized bat algorithm to the constrained portfolio optimization problem which is a hard optimization problem suitable for stochastic optimization metaheuristics. Bat algorithm is a recent member of the group of nature-inspired algorithms. Hybridization between bat and artificial bee colony metaheuristics was adapted for solving portfolio problem with constraints that extend classical mean-variance portfolio selection formulation. To test the robustness of our hybridized approach, a comparative analysis with other swarm intelligence algorithms, as well as with three variants of genetic algorithm was performed. All algorithms included in comparative analysis were tested on the same portfolio model using the same data set. Results show that proposed hybridized bat algorithm has a great potential for tackling constrained portfolio problem.","Ivana Strumberger,Nebojsa Bacanin,Milan Tuba",,2016,,,
597,Evolution Strategies as a Scalable Alternative to Reinforcement Learning.,"We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.","Tim Salimans,Jonathan Ho,Xi Chen,Xi Chen,Xi Chen,Szymon Sidor,Ilya Sutskever",arXiv: Machine Learning,2017,,,
598,Online Learning for Offloading and Autoscaling in Energy Harvesting Mobile Edge Computing,"Mobile edge computing (also known as fog computing) has recently emerged to enable  in-situ  processing of delay-sensitive applications at the edge of mobile networks. Providing grid power supply in support of mobile edge computing, however, is costly and even infeasible (in certain rugged or under-developed areas), thus mandating on-site renewable energy as a major or even sole power supply in increasingly many scenarios. Nonetheless, the high intermittency and unpredictability of renewable energy make it very challenging to deliver a high quality of service to users in energy harvesting mobile edge computing systems. In this paper, we address the challenge of incorporating renewables into mobile edge computing and propose an efficient reinforcement learning-based resource management algorithm, which learns on-the-fly the optimal policy of dynamic workload offloading (to the centralized cloud) and edge server provisioning to minimize the long-term system cost (including both service delay and operational cost). Our online learning algorithm uses a decomposition of the (offline) value iteration and (online) reinforcement learning, thus achieving a significant improvement of learning rate and run-time performance when compared to standard reinforcement learning algorithms such as    ${Q}$   -learning. We prove the convergence of the proposed algorithm and analytically show that the learned policy has a simple monotone structure amenable to practical implementation. Our simulation results validate the efficacy of our algorithm, which significantly improves the edge computing performance compared to fixed or myopic optimization schemes and conventional reinforcement learning algorithms.","Jie Xu,Lixing Chen,Shaolei Ren",IEEE Transactions on Cognitive Communications and Networking,2017,,,
599,Portfolio selection problems with Markowitz's mean---variance framework: a review of literature,"Since the pioneering work of Harry Markowitz, mean---variance portfolio selection model has been widely used in both theoretical and empirical studies, which maximizes the investment return under certain risk level or minimizes the investment risk under certain return level. In this paper, we review several variations or generalizations that substantially improve the performance of Markowitz's mean---variance model, including dynamic portfolio optimization, portfolio optimization with practical factors, robust portfolio optimization and fuzzy portfolio optimization. The review provides a useful reference to handle portfolio selection problems for both researchers and practitioners. Some summaries about the current studies and future research directions are presented at the end of this paper.","Yuanyuan Zhang,Xiang Li,Sini Guo,Sini Guo,Sini Guo",Fuzzy Optimization and Decision Making,2018,,,
600,On building ensembles of stacked denoising auto-encoding classifiers and their further improvement,"Abstract   To aggregate diverse learners and to train deep architectures are the two principal avenues towards increasing the expressive capabilities of neural networks. Therefore, their combinations merit attention. In this contribution, we study how to apply some conventional diversity methods –bagging and label switching– to a general deep machine, the stacked denoising auto-encoding classifier, in order to solve a number of appropriately selected image recognition problems. The main conclusion of our work is that binarizing multi-class problems is the key to obtain benefit from those diversity methods.  Additionally, we check that adding other kinds of performance improvement procedures, such as pre-emphasizing training samples and elastic distortion mechanisms, further increases the quality of the results. In particular, an appropriate combination of all the above methods leads us to reach a new absolute record in classifying MNIST handwritten digits.  These facts reveal that there are clear opportunities for designing more powerful classifiers by means of combining different improvement techniques.","Ricardo F. Alvear-Sandoval,Aníbal R. Figueiras-Vidal",Information Fusion,2018,,,
601,A Series-based group stock portfolio optimization approach using the grouping genetic algorithm with symbolic aggregate Approximations,"Stock portfolio optimization is both an attractive research topic and a complex problem due to the rapidly changing economy. Based on optimization techniques, many algorithms have been proposed to mine different portfolios. In the previous approach, a group stock portfolio (GSP) was derived based on the investors' objective and subjective requests by the grouping genetic algorithm. Stocks were divided into groups, with those in the same group being similar. The benefit of using a GSP is that investors can replace any stock that they do not like with substitute stocks in the same group. To increase the similarity of stocks in groups, stock price series are taken into consideration, and an enhanced approach is proposed to derive a series-based GSP that can be used to provide more actionable stock portfolios to investors making decisions. In chromosome representation, grouping, stock and stock portfolio parts are used to represent a GSP as did the previous approach. To increase the return and similarity of a GSP, the stability factor is designed based on cash dividends, and the unit and price balances are utilized as well. Because the dimension of stock price series is high, the symbolic aggregate approximation (SAX) and extended symbolic aggregate approximation (ESAX) are selected to transform data points into symbols. Then, the series distance factor is presented to evaluate the similarity of stock price series in groups of a GSP. By using the new factors and the existing factors in the previous approach, two new fitness functions are developed to evaluate the quality of chromosomes. Experiments on a real-world dataset were conducted to show the merits of the proposed approach using the two fitness functions with SAX and ESAX. The results show that the return on investment (ROI) of the proposed approach using the fitness functions with SAX is approximately 16% to 18% and better than the ROI obtained with ESAX. However, the proposed approach with ESAX achieves better group similarity than does SAX.","Chun-Hao Chen,Chih-Hung Yu,Chih-Hung Yu",Knowledge Based Systems,2017,,,
602,Discovering profitable stocks for intraday trading,"Intraday traders buy and sell financial instruments in the short term, typically within the same trading day. Stocks are notable examples of financial instruments. However, since hundreds of stocks are listed on the stock exchange selecting on each trading day the most tradeable stocks is a challenging task, which is commonly addressed through manual inspection of historical stock prices and technical indicators. This paper aims at discovering tradeable stocks on a given trading day by analyzing the historical prices assumed by the same stocks or by other ones on the preceding days by means of regression and weighted sequence mining techniques. The use of regression and weighted sequence mining techniques allows traders to automatically consider a potentially large number of candidate stocks and to effectively analyze their price variations across consecutive days. The experimental results, which were achieved on data acquired from different markets and under different market conditions, show that sequence mining algorithms yield profits higher than both regression techniques and naive strategies.","Elena Baralis,Luca Cagliero,Tania Cerquitelli,Paolo Garza,Fabio Pulvirenti",Information Sciences,2017,,,
603,The Option-Critic Architecture,"Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging.We tackle this problem in the framework of options [Sutton,Precup and Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.","Pierre-Luc Bacon,Jean Harb,Doina Precup,Doina Precup,Doina Precup",,2017,,,
604,Deep decentralized multi-task multi-agent reinforcement learning under partial observability,"Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently-exploring teammates. Approaches that learn specialized policies for individual tasks face problems when applied to the real world: not only do agents have to learn and store distinct policies for each task, but in practice identities of tasks are often non-observable, making these approaches inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.","Shayegan Omidshafiei,Jason Pazis,Christopher Amato,Jonathan P. How,John Vian,John Vian",,2017,,,
605,Stock market prediction using Neural Networks and sentiment analysis of News Articles.,,"Mr.Raj Thakur,Miss.Sanchita Badkas,Miss.Meenakumari Pade,Mrs.Pallavi Khude.",International Journal of Approximate Reasoning,2017,,,
606,Convolutional Neural Networks,Convolution Neural Networks (CNNs) in essence are neural networks that employ the convolution operation (instead of a fully connected layer) as one of its layers.,"Nikhil Ketkar,Nikhil Ketkar",,2017,,,
607,The relative performance of ensemble methods with deep convolutional neural networks for image classification,"Artificial neural networks have been successfully applied to a variety of machine learning tasks, including image recognition, semantic segmentation, and machine translation. However, few studies f...","Cheng Ju,Aurélien F. Bibaut,Mark J. van der Laan,Mark J. van der Laan,Mark J. van der Laan",Journal of Applied Statistics,2018,,,
608,Fast random k-labELsets for large-scale multi-label classification,"Multi-label classification (MLC), allowing instances to have multiple labels, has been received a surge of interests in recent years due to its wide range of applications such as image annotation and document tagging. One of simplest ways to solve MLC problems is label-power set method (LP) that regards all possible label subsets as classes. LP validates traditional multi-classification classifiers such as multi-class SVM but it suffers from the increased number of classes. Therefore, several improvements have been made for LP to be scaled for large problems with many labels. Random k labELsets (RAkEL) proposed by Tsoumakas et al. solves this problem by randomly sampling a small number of labels and taking ensemble of them. However, RAkEL needs all instances for constructing each model and thus suffers from high computational complexity. In this paper, we propose a new fast algorithm for RAkEL. First, we assign each training instance to a small number of models. Then LP is applied for each model with only the assigned instances. Experiments on twelve benchmark datasets demonstrated that the proposed algorithm works faster than the conventional methods while keeping accuracy. In the best case, it was 100 times faster than baseline method (LP) and 30 times faster than the original RAkEL.","Keigo Kimura,Mineichi Kudo,Lu Sun,Sadamori Koujaku",,2016,,,
609,Estimation of the discontinuous leverage effect: Evidence from the NASDAQ order book,"An extensive empirical literature documents a generally negative correlation, named the ?leverage effect,? between asset returns and changes of volatility. It is more challenging to establish such a return-volatility relationship for jumps in high-frequency data. We propose new nonparametric methods to assess and test for a discontinuous leverage effect ? i.e. a relation between contemporaneous jumps in prices and volatility ? in high-frequency data with market microstructure noise. We present local tests and estimators for price jumps and volatility jumps. Five years of transaction data from 320 NASDAQ firms display no negative relation between price and volatility cojumps. We show, however, that there is a strong relation between price-volatility cojumps if one conditions on the sign of price jumps and whether the price jumps are market-wide or idiosyncratic.","Markus Bibinger,Christopher J. Neely,Lars Winkelmann",Journal of Econometrics,2017,,,
610,Linear discriminant analysis: A detailed tutorial,"Linear Discriminant Analysis (LDA) is a very common
technique for dimensionality reduction problems as a preprocessing
step for machine learning and pattern classification
applications. At the same time, it is usually used as a
black box, but (sometimes) not well understood. The aim of
this paper is to build a solid intuition for what is LDA, and
how LDA works, thus enabling readers of all levels be able
to get a better understanding of the LDA and to know how to
apply this technique in different applications. The paper first
gave the basic definitions and steps of how LDA technique
works supported with visual explanations of these steps.
Moreover, the two methods of computing the LDA space, i.e.
class-dependent and class-independent methods, were explained
in details. Then, in a step-by-step approach, two numerical
examples are demonstrated to show how the LDA
space can be calculated in case of the class-dependent and
class-independent methods. Furthermore, two of the most
common LDA problems (i.e. Small Sample Size (SSS) and
non-linearity problems) were highlighted and illustrated, and
state-of-the-art solutions to these problems were investigated and explained. Finally, a number of experiments was conducted
with different datasets to (1) investigate the effect of
the eigenvectors that used in the LDA space on the robustness
of the extracted feature for the classification accuracy,
and (2) to show when the SSS problem occurs and how it can
be addressed.","Alaa Tharwat,Tarek Gaber,Abdelhameed Ibrahim,Aboul Ella Hassanien",Ai Communications,2017,,,
611,Deep Incremental Boosting,"This paper introduces Deep Incremental Boosting, a new technique derived from AdaBoost, specifically adapted to work with Deep Learning methods, that reduces the required training time and improves generalisation. We draw inspiration from Transfer of Learning approaches to reduce the start-up time to training each incremental Ensemble member. We show a set of experiments that outlines some preliminary results on some common Deep Learning datasets and discuss the potential improvements Deep Incremental Boosting brings to traditional Ensemble methods in Deep Learning.","Alan Mosca,George D. Magoulas",arXiv: Machine Learning,2017,,,
612,A dual-stage attention-based recurrent neural network for time series prediction,"The Nonlinear autoregressive exogenous (NARX) model, which predicts the current value of a time series based upon its previous values as well as the current and past values of multiple driving (exogenous) series, has been studied for decades. Despite the fact that various NARX models have been developed, few of them can capture the long-term temporal dependencies appropriately and select the relevant driving series to make predictions. In this paper, we propose a dual-stage attention-based recurrent neural network (DA-RNN) to address these two issues. In the first stage, we introduce an input attention mechanism to adaptively extract relevant driving series (a.k.a., input features) at each time step by referring to the previous encoder hidden state. In the second stage, we use a temporal attention mechanism to select relevant encoder hidden states across all time steps. With this dual-stage attention scheme, our model can not only make predictions effectively, but can also be easily interpreted. Thorough empirical studies based upon the SML 2010 dataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can outperform state-of-the-art methods for time series prediction.","Yao Qin,Dongjin Song,Haifeng Cheng,Wei Cheng,Guofei Jiang,Garrison W. Cottrell",,2017,,,
613,Boosted Convolutional Neural Networks.,,"Mohammad Moghimi,Serge Belongie,Mohammad Saberian,Jian Yang,Jian Yang,Jian Yang,Jian Yang,Jian Yang,Nuno Vasconcelos,Li-Jia Li",,2016,,,
614,Roughly Balanced Bagging for Imbalanced Data.,,"Shohei Hido,Hisashi Kashima",,2008,,,
615,Merging virtual world with data sciences,Virtual world is generating large amount of data Gaming industry is one of the actors in the production of huge amount variety and velocity of data Merging of virtual world with data sciences can give better understanding of the underlying mechanics for all the stakeholders To do so the idea is to bring forth the rich data model known as Resource Description Framework RDF of Semantic Web to become a middle source of bridging Big data and Virtual platforms nbsp,"Kaleem Razzaq Malik,Muhammad Farhan",,2017,,,
616,Price manipulation in the Bitcoin ecosystem,"We identify and analyze the impact of suspicious trading activity (STA) on the Mt. Gox Bitcoin currency exchange between February and November 2013. We discuss two distinct STA periods in which approximately 600,000 bitcoin (BTC) valued at $188 million were acquired by agents who did not pay for the bitcoins. During the second period, the USD-BTC exchange rate rose by an average of $20 at Mt. Gox on days when suspicious trades took place, compared to a slight decline on days without suspicious activity. Based on rigorous analysis with extensive robustness checks, we conclude that the suspicious trading activity caused the unprecedented spike in the USD-BTC exchange rate in late 2013, when the rate jumped from around $150 to more than $1,000 in two months.","Neil Gandal,JT Hamrick,Tyler Moore,Tali Oberman",Journal of Monetary Economics,2018,,,
617,Applications of multi-variate Hawkes process to joint modelling of sentiment and market return events,"To investigate the complex interactions between market events and investor sentiment, we employ a multivariate Hawkes process to evaluate dynamic effects among four types of distinct events: positive returns, negative returns, positive sentiment, and negative sentiment. Using both intraday SP (b) there is a significant mutual-excitation between positive returns and positive sentiment and negative returns and negative sentiment; (c) decay of return events is almost twice as fast as sentiment events, which means market prices move faster than investor sentiment changes; (d) positive sentiment shocks tend to generate negative price jumps; and (e) the cross-excitation between positive and negative sentiments is stronger than their self-excitation. These findings provide further understanding of investor sentiment and its intricate interactions wi...","Steve Y. Yang,Anqi Liu,Anqi Liu,Jing Chen,Alan G. Hawkes",Quantitative Finance,2018,,,
618,Determining characteristics of successful recommendations from log data: a case study,"Academic research in recommender systems largely focuses on the problem of predicting the relevance of (long-tail) items that the individual user presumably does not know yet. Many real-world systems however also recommend items that users have inspected in the past, items that are popular at the moment, and items currently on sale. In this work we investigate the value of including such items in recommendation lists based on an analysis of the web logs of a large online retailer. An examination of the features of successful item suggestions reveals that the chances of a recommendation leading to a purchase increase when the item is recently trending, on sale, or was recently viewed by the user. Offline simulation experiments furthermore show that considering those success factors that were identified from log data in the ranking algorithms can help to increase the prediction accuracy of recommender systems.","Dietmar Jannach,Malte Ludewig",,2017,,,
619,Multimodal Machine Learning: A Survey and Taxonomy,"Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors.  Modality  refers to the way in which something happens or is experienced and a research problem is characterized as  multimodal  when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together.  Multimodal machine learning  aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.","Tadas Baltrusaitis,Chaitanya Ahuja,Louis-Philippe Morency",IEEE Transactions on Pattern Analysis and Machine Intelligence,2019,,,
620,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,"We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.","Ryan Lowe,Yi Wu,Aviv Tamar,Jean Harb,Pieter Abbeel,Igor Mordatch",arXiv: Learning,2017,,,
621,Deep learning with long short-term memory networks for financial market predictions,"Long short-term memory (LSTM) networks are a state-of-the-art technique for sequence learning. They are less commonly applied to financial time series predictions, yet inherently suitable for this domain. We deploy LSTM networks for predicting out-of-sample directional movements for the constituent stocks of the S&P 500 from 1992 until 2015. With daily returns of 0.46 percent and a Sharpe ratio of 5.8 prior to transaction costs, we find LSTM networks to outperform memory-free classification methods, i.e., a random forest (RAF), a deep neural net (DNN), and a logistic regression classifier (LOG). The outperformance relative to the general market is very clear from 1992 to 2009, but as of 2010, excess returns seem to have been arbitraged away with LSTM profitability fluctuating around zero after transaction costs. We further unveil sources of profitability, thereby shedding light into the black box of artificial neural networks. Specifically, we find one common pattern among the stocks selected for trading – they exhibit high volatility and a short-term reversal return profile. Leveraging these findings, we are able to formalize a rules-based short-term reversal strategy that yields 0.23 percent prior to transaction costs. Further regression analysis unveils low exposure of the LSTM returns to common sources of systematic risk – also compared to the three benchmark models.","Thomas G. Fischer,Thomas Fischer,Thomas Fischer,Thomas Fischer,Christopher Krauss",European Journal of Operational Research,2017,,,
622,High efficiently numerical simulation of the TDGL equation with reticular free energy in hydrogel,"In this paper, we focus on the numerical simulation of phase separation about macromolecule microsphere composite (MMC) hydrogel. The model equation is based on Time-Dependent Ginzburg-Landau (TDGL) equation with reticular free energy. We have put forward two $L^2$ stable schemes to simulate simplified TDGL equation. In numerical experiments, we observe that simulating the whole process of phase separation requires a considerably long time. We also notice that the total free energy changes significantly in initial time and varies slightly in the following time. Based on these properties, we introduce an adaptive strategy based on one of stable scheme mentioned. It is found that the introduction of the time adaptivity cannot only resolve the dynamical changes of the solution accurately but also can significantly save CPU time for the long time simulation.","Jun Han,Jun Han,Jun Han,Hui Zhang,Zhengru Zhang",arXiv: Numerical Analysis,2017,,,
623,Zombies Arena: fusion of reinforcement learning with augmented reality on NPC,Augmented reality (AR) is a discipline having less cognizance but it is the door to new advance technologies. Accustomed games doesn’t facilitate user to physically interact with the surroundings which resulted into reduced learning capabilities. Our objective is to develop AR based first person shooter game empowering reinforcement learning. This act as a building block to capacitate users to interact with the physical environment. Non-player characters will be able to learn and adopt strategy more wisely after each move to capacitate players. Game is played by hundred users at different stages. Reported results are summarized in experiment section.,"Saad Razzaq,Fahad Maqbool,Maham Khalid,Iram Tariq,Aqsa Zahoor,Muhammad Ilyas,Muhammad Ilyas",Cluster Computing,2018,,,
624,Recurrent Latent Variable Networks for Session-Based Recommendation,"In this work, we attempt to ameliorate the impact of data sparsity in the context of session-based recommendation. Specifically, we seek to devise a machine learning mechanism capable of extracting subtle and complex underlying temporal dynamics in the observed session data, so as to inform the recommendation algorithm. To this end, we improve upon systems that utilize deep learning techniques with recurrently connected units; we do so by adopting concepts from the field of Bayesian statistics, namely variational inference. Our proposed approach consists in treating the network recurrent units as stochastic latent variables with a prior distribution imposed over them. On this basis, we proceed to infer corresponding posteriors; these can be used for prediction and recommendation generation, in a way that accounts for the uncertainty in the available sparse training data. To allow for our approach to easily scale to large real-world datasets, we perform inference under an approximate amortized variational inference (AVI) setup, whereby the learned posteriors are parameterized via (conventional) neural networks. We perform an extensive experimental evaluation of our approach using challenging benchmark datasets, and illustrate its superiority over existing state-of-the-art techniques.","Sotirios P. Chatzis,Panayiotis Christodoulou,Andreas S. Andreou",,2017,,,
625,Personalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks,"Session-based recommendations are highly relevant in many modern on-line services (e.g. e-commerce, video streaming) and recommendation settings. Recently, Recurrent Neural Networks have been shown to perform very well in session-based settings. While in many session-based recommendation domains user identifiers are hard to come by, there are also domains in which user profiles are readily available. We propose a seamless way to personalize RNN models with cross-session information transfer and devise a Hierarchical RNN model that relays end evolves latent hidden states of the RNNs across user sessions. Results on two industry datasets show large improvements over the session-only RNNs.","Massimo Quadrana,Alexandros Karatzoglou,Balázs Hidasi,Paolo Cremonesi",,2017,,,
626,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,"RNNs have been shown to be excellent models for sequential data and in particular for data that is generated by users in an session-based manner. The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations. In this work we introduce novel ranking loss functions tailored to RNNs in the recommendation setting. The improved performance of these losses over alternatives, along with further tricks and refinements described in this work, allow for an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 53% over classical collaborative filtering approaches. Unlike data augmentation-based improvements, our method does not increase training times significantly. We further demonstrate the performance gain of the RNN over baselines in an online A/B test.","Balázs Hidasi,Alexandros Karatzoglou",,2018,,,
627,Attention Is All You Need,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","Ashish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszkoreit,Llion Jones,Aidan N. Gomez,Lukasz Kaiser,Illia Polosukhin",arXiv: Computation and Language,2017,,,
628,Contextual Sequence Modeling for Recommendation with Recurrent Neural Networks,"Recommendations can greatly benefit from good representations of the user state at recommendation time. Recent approaches that leverage Recurrent Neural Networks (RNNs) for session-based recommendations have shown that Deep Learning models can provide useful user representations for recommendation. However, current RNN modeling approaches summarize the user state by only taking into account the sequence of items that the user has interacted with in the past, without taking into account other essential types of context information such as the associated types of user-item interactions, the time gaps between events and the time of day for each interaction. To address this, we propose a new class of Contextual Recurrent Neural Networks for Recommendation (CRNNs) that can take into account the contextual information both in the input and output layers and modifying the behavior of the RNN by combining the context embedding with the item embedding and more explicitly, in the model dynamics, by parametrizing the hidden unit transitions as a function of context information. We compare our CRNNs approach with RNNs and non-sequential baselines and show good improvements on the next event prediction task.","Elena Smirnova,Flavian Vasile,Flavian Vasile",,2017,,,
629,Semi-supervised ensemble DNN acoustic model training,"It is very important to exploit abundant unlabeled speech for improving the acoustic model training in automatic speech recognition (ASR). Semi-supervised training methods incorporate unlabeled data in addition to labeled data to enhance the model training, but it encounters the error-prone label problem. The ensemble training scheme trains a set of models and combines them to make the model more general and robust, but it has not been applied to the unlabeled data. In this work, we propose an effective semi-supervised training of deep neural network (DNN) acoustic models by incorporating the diversity among the ensemble of models. The resultant model improved the performance in the lecture transcription task. Moreover, the proposed method has also shown a potential for DNN adaptation.","Sheng Li,Xugang Lu,Shinsuke Sakai,Masato Mimura,Tatsuya Kawahara",,2017,,,
630,Inter-Session Modeling for Session-Based Recommendation,"In recent years, research has been done on applying Recurrent Neural Networks (RNNs) as recommender systems. Results have been promising, especially in the session-based setting where RNNs have been shown to outperform state-of-the-art models. In many of these experiments, the RNN could potentially improve the recommendations by utilizing information about the user's past sessions, in addition to its own interactions in the current session. A problem for session-based recommendation, is how to produce accurate recommendations at the start of a session, before the system has learned much about the user's current interests. We propose a novel approach that extends an RNN recommender to be able to process the user's recent sessions, in order to improve recommendations. This is done by using a second RNN to learn from recent sessions, and predict the user's interest in the current session. By feeding this information to the original RNN, it is able to improve its recommendations. Our experiments on two different datasets show that the proposed approach can significantly improve recommendations throughout the sessions, compared to a single RNN working only on the current session. The proposed model especially improves recommendations at the start of sessions, and is therefore able to deal with the cold start problem within sessions.","Massimiliano Ruocco,Ole Steinar Lillestøl Skrede,Helge Langseth",,2017,,,
631,The Behavior of Individual Investors,"We provide an overview of research on the stock trading behavior of individual investors. This research documents that individual investors (1) underperform standard benchmarks (e.g. a low-cost index fund), (2) sell winning investments while holding losing investments (the â€œdisposition effectâ€ ), (3) are heavily influenced by limited attention and past return performance in their purchase decisions, (4) engage in naA¯ve reinforcement learning by repeating past behaviors that coincided with pleasure while avoiding past behaviors that generated pain, and (5) tend to hold undiversified stock portfolios. These behaviors deleteriously affect the financial well being of individual investors.","Brad M. Barber,Terrance Odean",Handbook of The Economics of Finance,2011,,,
632,A Flood Forecasting Model Based on Deep Learning Algorithm via Integrating Stacked Autoencoders with BP Neural Network,"Artificial neural network (ANN) has been widely applied in flood forecasting and got good results. However, it can still not go beyond one or two hidden layers for the problematic non-convex optimization. This paper proposes a deep learning approach by integrating stacked autoencoders (SAE) and back propagation neural networks (BPNN) for the prediction of stream flow, which simultaneously takes advantages of the powerful feature representation capability of SAE and superior predicting capacity of BPNN. To further improve the non-linearity simulation capability, we first classify all the data into several categories by the K-means clustering. Then, multiple SAE-BP modules are adopted to simulate their corresponding categories of data. The proposed approach is respectively compared with the support-vector-machine (SVM) model, the BP neural network model, the RBF neural network model and extreme learning machine (ELM) model. The experimental results show that the SAE-BP integrated algorithm performs much better than other benchmarks.","Fan Liu,Fan Liu,Feng Xu,Feng Xu,Sai Yang",,2017,,,
633,Long and Short-Term Recommendations with Recurrent Neural Networks,"Recurrent neural networks have recently been successfully applied to the session-based recommendation problem, and is part of a growing interest for collaborative filtering based on sequence prediction. This new approach to recommendations reveals an aspect that was previously overlooked: the difference between short-term and long-term recommendations. In this work we characterize the full short-term/long-term profile of many collaborative filtering methods, and we show how recurrent neural networks can be steered towards better short or long-term predictions. We also show that RNNs are not only adapted to session-based collaborative filtering, but are perfectly suited for collaborative filtering on dense datasets where it outperforms traditional item recommendation algorithms.","Robin Devooght,Hugues Bersini",,2017,,,
634,Improving Cold Start Recommendation by Mapping Feature-Based Preferences to Item Comparisons,"Many Recommender Systems (RSs) rely on user preference data in the form of ratings or likes for items. Previous research has shown that item comparisons can also be effectively used to model user preferences and build RS. However, users often express their preferences by referring to specific features of the items. For instance, a user may like Italian movies more than Indian ones or like action-thriller movies. In this paper, we map such preferences over features to comparisons between items. For instance, when a user's favorite feature is `action', we then assume that `action' movies are preferred to some of the movies that are not `action'. In this work we effectively incorporate these feature based comparisons in a RS and show that such preferences can be effectively combined along with other item comparisons. Moreover, we also study the usefulness of the available features.","Saikishore Kalloori,Francesco Ricci",,2017,,,
635,Ensemble application of convolutional and recurrent neural networks for multi-label text categorization,"Text categorization, or text classification, is one of key tasks for representing the semantic information of documents. Multi-label text categorization is finer-grained approach to text categorization which consists of assigning multiple target labels to documents. It is more challenging compared to the task of multi-class text categorization due to the exponential growth of label combinations. Existing approaches to multi-label text categorization fall short to extract local semantic information and to model label correlations. In this paper, we propose an ensemble application of convolutional and recurrent neural networks to capture both the global and the local textual semantics and to model high-order label correlations while having a tractable computational complexity. Extensive experiments show that our approach achieves the state-of-the-art performance when the CNN-RNN model is trained using a large-sized dataset.","Guibin Chen,Deheng Ye,Zhenchang Xing,Jieshan Chen,Jieshan Chen,Erik Cambria",,2017,,,
636,Dissecting Investment Strategies in the Cross Section and Time Series,"We contrast the time-series and cross-sectional performance of three popular investment strategies: carry, momentum and value. While considerable research has examined the performance of these strategies in either a directional or cross-asset settings, we offer some insights on the market conditions that favor the application of a particular setting.","Jamil Baz,Nicolas M. Granger,Campbell R. Harvey,Nicolas Le Roux,Sandy Rattray",,2015,,,
637,Session-aware Information Embedding for E-commerce Product Recommendation,"Most of the existing recommender systems assume that user's visiting history can be constantly recorded. However, in recent online services, the user identification may be usually unknown and only limited online user behaviors can be used. It is of great importance to model the temporal online user behaviors and conduct recommendation for the anonymous users. In this paper, we propose a list-wise deep neural network based architecture to model the limited user behaviors within each session. To train the model efficiently, we first design a session embedding method to pre-train a session representation, which incorporates different kinds of user search behaviors such as clicks and views. Based on the learnt session representation, we further propose a list-wise ranking model to generate the recommendation result for each anonymous user session. We conduct quantitative experiments on a recently published dataset from an e-commerce company. The evaluation results validate the effectiveness of the proposed method, which can outperform the state-of-the-art.","Chen Wu,Ming Yan,Ming Yan,Luo Si,Luo Si,Luo Si,Luo Si,Luo Si",,2017,,,
638,Data science: challenges and directions,"While it may not be possible to build a data brain identical to a human, data science can still aspire to imaginative machine thinking.",Longbing Cao,Communications of The ACM,2017,,,
639,BranchOut: Regularization for Online Ensemble Tracking with Convolutional Neural Networks,"We propose an extremely simple but effective regularization technique of convolutional neural networks (CNNs), referred to as BranchOut, for online ensemble tracking. Our algorithm employs a CNN for target representation, which has a common convolutional layers but has multiple branches of fully connected layers. For better regularization, a subset of branches in the CNN are selected randomly for online learning whenever target appearance models need to be updated. Each branch may have a different number of layers to maintain variable abstraction levels of target appearances. BranchOut with multi-level target representation allows us to learn robust target appearance models with diversity and handle various challenges in visual tracking problem effectively. The proposed algorithm is evaluated in standard tracking benchmarks and shows the state-of-the-art performance even without additional pretraining on external tracking sequences.","Bohyung Han,Jack Sim,Hartwig Adam",,2017,,,
640,TensorLayer: A Versatile Library for Efficient Deep Learning Development,"Recently we have observed emerging uses of deep learning techniques in multimedia systems. Developing a practical deep learning system is arduous and complex. It involves labor-intensive tasks for constructing sophisticated neural networks, coordinating multiple network models, and managing a large amount of training-related data. To facilitate such a development process, we propose TensorLayer which is a Python-based versatile deep learning library. TensorLayer provides high-level modules that abstract sophisticated operations towards neuron layers, network models, training data and dependent training jobs. In spite of offering simplicity, it has transparent module interfaces that allows developers to flexibly embed low-level controls within a backend engine, with the aim of supporting fine-grain tuning towards training. Real-world cluster experiment results show that TensorLayeris able to achieve competitive performance and scalability in critical deep learning tasks. TensorLayer was released in September 2016 on GitHub. Since after, it soon become one of the most popular open-sourced deep learning library used by researchers and practitioners.","Hao Dong,Akara Supratak,Luo Mai,Fangde Liu,Axel Oehmichen,Simiao Yu,Yike Guo",,2017,,,
641,Deep Learning based Recommender System: A Survey and New Perspectives.,"With the growing volume of online information, recommender systems have been an effective strategy to overcome information overload. The utility of recommender systems cannot be overstated, given their widespread adoption in many web applications, along with their potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also to the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. The field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning-based recommender systems. More concretely, we provide and devise a taxonomy of deep learning-based recommendation models, along with a comprehensive summary of the state of the art. Finally, we expand on current trends and provide new perspectives pertaining to this new and exciting development of the field.","Shuai Zhang,Shuai Zhang,Lina Yao,Aixin Sun,Yi Tay",ACM Computing Surveys,2019,,,
642,Meet Mike: epic avatars,"Meet Mike uses the latest techniques in advanced motion capture to drive complex facial rigs to enable detailed interaction in VR. This allows participants to meet, talk in VR and experience new levels of photorealistic interaction. The installation uses new advances in real time rigs, skin shaders, facial capture, deep learning and real-time rendering in UE4.","Mike Seymour,Christopher Evans,Kim Libreri",,2017,,,
643,Diversifying personalized recommendation with user-session context,"Recommender systems (RS) have become an integral part of our daily life. However, most current RS often repeatedly recommend items to users with similar profiles. We argue that recommendation should be diversified by leveraging session contexts with personalized user profiles. For this, current session-based RS (SBRS) often assume a rigidly ordered sequence over data which does not fit in many real-world cases. Moreover, personalization is often omitted in current SBRS. Accordingly, a personalized SBRS over relaxedly ordered user-session contexts is more pragmatic. In doing so, deep-structured models tend to be too complex to serve for online SBRS owing to the large number of users and items. Therefore, we design an efficient SBRS with shallow wide-in-wide-out networks, inspired by the successful experience in modern language modelings. The experiments on a real-world e-commerce dataset show the superiority of our model over the state-of-the-art methods.","Liang Hu,Longbing Cao,Shoujin Wang,Guandong Xu,Jian Cao,Zhiping Gu",,2017,,,
644,Embedding-based representation of categorical data by hierarchical value coupling learning,"Learning the representation of categorical data with hierarchical value coupling relationships is very challenging but critical for the effective analysis and learning of such data. This paper proposes a novel coupled unsupervised categorical data representation (CURE) framework and its instantiation, i.e., a coupled data embedding (CDE) method, for representing categorical data by hierarchical value-to-value cluster coupling learning. Unlike existing embedding- and similarity-based representation methods which can capture only a part or none of these complex couplings, CDE explicitly incorporates the hierarchical couplings into its embedding representation. CDE first learns two complementary feature value couplings which are then used to cluster values with different granularities. It further models the couplings in value clusters within the same granularity and with different granularities to embed feature values into a new numerical space with independent dimensions. Substantial experiments show that CDE significantly outperforms three popular unsupervised embedding methods and three state-of-the-art similarity-based representation methods.","Songlei Jian,Longbing Cao,Guansong Pang,Kai Lu,Hang Gao",,2017,,,
645,Sentiment-aware stock market prediction: A deep learning method,"Stock market prediction has attracted much attention from academia as well as business. However, it is a challenging research topic, in which many advanced computational methods have been proposed, but not yet attained a desirable and reliable performance. This study proposes a new method for stock market prediction, which adopts the Long Short-Term Memory (LSTM) neural network and incorporates investor sentiment and market factors to improve forecasting performance. By extracting investor sentiment from forum posts using Naive Bayes, this paper makes it possible to analyze the irrational component of stock price. Our empirical study on CSI300 index proves that our prediction method provides better prediction performance. It gives a prediction accuracy of 87.86%, outperforming other benchmark models by at least 6%. Furthermore, our empirical study reveals evidence that helps to better understand investor sentiment and stock behaviors. Finally, this work shows the potential of deep learning financial time series in the presence of strong noises.","Jiahong Li,Hui Bu,Junjie Wu",,2017,,,
646,Learning homophily couplings from non-IID data for joint feature selection and noise-resilient outlier detection,"This paper introduces a novel wrapper-based outlier detection framework (WrapperOD) and its instance (HOUR) for identifying outliers in noisy data (i.e., data with noisy features) with strong couplings between outlying behaviors. Existing subspace or feature selection-based methods are significantly challenged by such data, as their search of feature subset(s) is independent of outlier scoring and thus can be misled by noisy features. In contrast, HOUR takes a wrapper approach to iteratively optimize the feature subset selection and outlier scoring using a top-k outlier ranking evaluation measure as its objective function. HOUR learns homophily couplings between outlying behaviors (i.e., abnormal behaviors are not independent - they bond together) in constructing a noise-resilient outlier scoring function to produce a reliable outlier ranking in each iteration. We show that HOUR (i) retains a 2-approximation outlier ranking to the optimal one; and (ii) significantly outperforms five state-of-the-art competitors on 15 real-world data sets with different noise levels in terms of AUC and/or [emailprotected] The source code of HOUR is available at https://sites.google.com/site/gspangsite/sourcecode.","Guansong Pang,Longbing Cao,Ling Chen,Huan Liu",,2017,,,
647,3D Convolutional Networks for Session-based Recommendation with Content Features,"In many real-life recommendation settings, user profiles and past activities are not available. The recommender system should make predictions based on session data, e.g. session clicks and descriptions of clicked items. Conventional recommendation approaches, which rely on past user-item interaction data, cannot deliver accurate results in these situations. In this paper, we describe a method that combines session clicks and content features such as item descriptions and item categories to generate recommendations. To model these data, which are usually of different types and nature, we use 3-dimensional convolutional neural networks with character-level encoding of all input data. While 3D architectures provide a natural way to capture spatio-temporal patterns, character-level networks allow modeling different data types using their raw textual representation, thus reducing feature engineering effort. We applied the proposed method to predict add-to-cart events in e-commerce websites, which is more difficult then predicting next clicks. On two real datasets, our method outperformed several baselines and a state-of-the-art method based on recurrent neural networks.","Trinh Xuan Tuan,Tu Minh Phuong",,2017,,,
648,When Recurrent Neural Networks meet the Neighborhood for Session-Based Recommendation,"Deep learning methods have led to substantial progress in various application fields of AI, and in recent years a number of proposals were made to improve recommender systems with artificial neural networks. For the problem of making session-based recommendations, i.e., for recommending the next item in an anonymous session, Hidasi et al.~recently investigated the application of recurrent neural networks with Gated Recurrent Units (GRU4REC). Assessing the true effectiveness of such novel approaches based only on what is reported in the literature is however difficult when no standard evaluation protocols are applied and when the strength of the baselines used in the performance comparison is not clear. In this work we show based on a comprehensive empirical evaluation that a heuristics-based nearest neighbor (kNN) scheme for sessions outperforms GRU4REC in the large majority of the tested configurations and datasets. Neighborhood sampling and efficient in-memory data structures ensure the scalability of the kNN method. The best results in the end were often achieved when we combine the kNN approach with GRU4REC, which shows that RNNs can leverage sequential signals in the data that cannot be detected by the co-occurrence-based kNN method.","Dietmar Jannach,Malte Ludewig",,2017,,,
649,CNN for situations understanding based on sentiment analysis of twitter data,"Abstract   In this paper, we propose an approach to understand situations in the real world with the sentiment analysis of Twitter data base on deep learning techniques. With the proposed method, it is possible to predict user satisfaction of a product, happiness with some particular environment or destroy situation after disasters. Recently, deep learning is able to solve problems in computer vision or voice recognition, and convolutional neural network (CNN) works good for image analysis and image classification. The biggest reason to adopt CNN in image analysis and classification is due to CNN can extract an area of features from global information, and it is able to consider the relationship among these features. The above solution can achieve a higher accuracy in analysis and classification. For natural language processing, texts data features also can be extracted piece by piece and to consider the relationship among these features, but without the consideration of context or whole sentence, the sentiment might be understood wrong. And currently, convolutional neural network is one of the most effective methods to do image classification, CNN has a convolutional layer to extract information by a larger piece of text, so we work for sentiment analysis with convolutional neural network, and we design a simple convolutional neural network model and test it on benchmark, the result shows that it achieves better accuracy performance in twitter sentiment classification than some of traditional method such as the SVM and Naive Bayes methods.","Shiyang Liao,Junbo Wang,Ruiyun Yu,Koichi Sato,Zixue Cheng",Procedia Computer Science,2017,,,
650,Modeling User Session and Intent with an Attention-based Encoder-Decoder Architecture,"We propose an encoder-decoder neural architecture to model user session and intent using browsing and purchasing data from a large e-commerce company.   We begin by identifying the source-target transition pairs between items within each session. Then, the set of source items are passed through an encoder, whose learned representation is used by the decoder to estimate the sequence of target items. Therefore, as this process is performed pair-wise, we hypothesize that the model could capture the transition regularities in a more fine grained way. Additionally, our model incorporates an attention mechanism to explicitly learn the more expressive portions of the sequences in order to improve performance. Besides modeling the user sessions, we also extended the original architecture by means of attaching a second decoder that is jointly trained to predict the purchasing intent of user in each session. With this, we want to explore to what extent the model can capture inter session dependencies.   We performed an empirical study comparing against several baselines on a large real world dataset, showing that our approach is competitive in both item and intent prediction.","Pablo Loyola,Chen Liu,Yu Hirate,Yu Hirate",,2017,,,
651,DLRS 2017: Second Workshop on Deep Learning for Recommender Systems,"Deep learning methods became widely popular in the recommender systems community in 2016, in part thanks to the previous event of the DLRS workshop series. Now, deep learning has been embedded in the main conference as well and initial research directions have started forming, so the role of DLRS 2017 is to encourage starting new research directions, incentivize the application of very recent techniques from deep learning, and provide a venue for specialized discussion of this topic.","Balázs Hidasi,Alexandros Karatzoglou,Oren Sar-Shalom,Oren Sar-Shalom,Bracha Shapira,Domonkos Tikk,Flavian Vasile,Flavian Vasile,Sander Dieleman",,2017,,,
652,Forecasting Stock Prices from the Limit Order Book Using Convolutional Neural Networks,"In today's financial markets, where most trades are performed in their entirety by electronic means and the largest fraction of them is completely automated, an opportunity has risen from analyzing this vast amount of transactions. Since all the transactions are recorded in great detail, investors can analyze all the generated data and detect repeated patterns of the price movements. Being able to detect them in advance, allows them to take profitable positions or avoid anomalous events in the financial markets. In this work we proposed a deep learning methodology, based on Convolutional Neural Networks (CNNs), that predicts the price movements of stocks, using as input large-scale, high-frequency time-series derived from the order book of financial exchanges. The dataset that we use contains more than 4 million limit order events and our comparison with other methods, like Multilayer Neural Networks and Support Vector Machines, shows that CNNs are better suited for this kind of task.","Avraam Tsantekidis,Nikolaos Passalis,Anastasios Tefas,Juho Kanniainen,Moncef Gabbouj,Alexandros Iosifidis",,2017,,,
653,Deep Learning for Recommender Systems,"Deep Learning is one of the next big things in Recommendation Systems technology. The past few years have seen the tremendous success of deep neural networks in a number of complex machine learning tasks such as computer vision, natural language processing and speech recognition. After its relatively slow uptake by the recommender systems community, deep learning for recommender systems became widely popular in 2016.   We believe that a tutorial on the topic of deep learning will do its share to further popularize the topic. Notable recent application areas are music recommendation, news recommendation, and session-based recommendation. The aim of the tutorial is to encourage the application of Deep Learning techniques in Recommender Systems, to further promote research in deep learning methods for Recommender Systems.","Alexandros Karatzoglou,Balázs Hidasi",,2017,,,
654,StarCraft II: A New Challenge for Reinforcement Learning,"This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.","Oriol Vinyals,Timo Ewalds,Sergey Bartunov,Petko Georgiev,Alexander Vezhnevets,Michelle Yeo,Michelle Yeo,Michelle Yeo,Alireza Makhzani,Heinrich Küttler,John Agapiou,Julian Schrittwieser,John Quan,Stephen Gaffney,Stig Petersen,Karen Simonyan,Tom Schaul,Hado van Hasselt,David Silver,Timothy Lillicrap,Kevin Calderone,Paul Keet,Anthony Brunasso,David Lawrence,Anders Ekermo,Jacob Repp,Rodney Tsing",arXiv: Learning,2017,,,
655,Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation,"In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at this https URL","Yuhuai Wu,Elman Mansimov,Shun Liao,Roger Grosse,Jimmy Ba",arXiv: Learning,2017,,,
656,Sequential User-based Recurrent Neural Network Recommendations,"Recurrent Neural Networks are powerful tools for modeling sequences. They are flexibly extensible and can incorporate various kinds of information including temporal order. These properties make them well suited for generating sequential recommendations. In this paper, we extend Recurrent Neural Networks by considering unique characteristics of the Recommender Systems domain. One of these characteristics is the explicit notion of the user recommendations are specifically generated for. We show how individual users can be represented in addition to sequences of consumed items in a new type of Gated Recurrent Unit to effectively produce personalized next item recommendations. Offline experiments on two real-world datasets indicate that our extensions clearly improve objective performance when compared to state-of-the-art recommender algorithms and to a conventional Recurrent Neural Network.","Tim Donkers,Benedikt Loepp,Jürgen Ziegler",,2017,,,
657,A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning,"To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and fictitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.","Marc Lanctot,Vinicius Zambaldi,Audrunas Gruslys,Angeliki Lazaridou,Karl Tuyls,Julien Perolat,David Silver,Thore Graepel",arXiv: Artificial Intelligence,2017,,,
658,Multimodal Image Registration with Deep Context Reinforcement Learning,"Automatic and robust registration between real-time patient imaging and pre-operative data (e.g. CT and MRI) is crucial for computer-aided interventions and AR-based navigation guidance. In this paper, we present a novel approach to automatically align range image of the patient with pre-operative CT images. Unlike existing approaches based on the surface similarity optimization process, our algorithm leverages the contextual information of medical images to resolve data ambiguities and improve robustness. The proposed algorithm is derived from deep reinforcement learning algorithm that automatically learns to extract optimal feature representation to reduce the appearance discrepancy between these two modalities. Quantitative evaluations on 1788 pairs of CT and depth images from real clinical setting demonstrate that the proposed method achieves the state-of-the-art performance.","Kai Ma,Jiangping Wang,Jiangping Wang,Jiangping Wang,Vivek Kumar Singh,Vivek Kumar Singh,Birgi Tamersoy,Yao-Jen Chang,Andreas Wimmer,Terrence Chen",,2017,,,
659,"Session-based item recommendation in e-commerce: on short-term intents, reminders, trends and discounts","Many e-commerce sites present additional item recommendations to their visitors while they navigate the site, and ample evidence exists that such recommendations are valuable for both customers and providers. Academic research often focuses on the capability of recommender systems to help users discover items they presumably do not know yet and which match their long-term preference profiles. In reality, however, recommendations can be helpful for customers also for other reasons, for example, when they remind them of items they were recently interested in or when they point site visitors to items that are currently discounted. In this work, we first adopt a systematic statistical approach to analyze what makes recommendations effective in practice and then propose ways of operationalizing these insights into novel recommendation algorithms. Our data analysis is based on log data of a large e-commerce site. It shows that various factors should be considered in parallel when selecting items for recommendation, including their match with the customer's shopping interests in the previous sessions, the general popularity of the items in the last few days, as well as information about discounts. Based on these analyses we propose a novel algorithm that combines a neighborhood-based scheme with a deep neural network to predict the relevance of items for a given shopping session.","Dietmar Jannach,Malte Ludewig,Lukas Lerche",User Modeling and User-adapted Interaction,2017,,,
660,Intraday prediction of Borsa Istanbul using convolutional neural networks and feature correlations,"We have extracted different types of indicator, price and temporal features.Previous instances and correlation between features are used to design CNN.We predict the hourly direction of 100 Stocks Borsa Istanbul Stock Market.Proposed method outperforms the CNN that uses randomly ordered features.On average we perform 56.3% Macro Average F-Measure rate on 100 stocks. Stock market price data have non-linear, noisy and non-stationary structure, and therefore prediction of the price or its direction are both challenging tasks. In this paper, we propose a Convolutional Neural Network (CNN) architecture with a specifically ordered feature set to predict the intraday direction of Borsa Istanbul 100 stocks. Feature set is extracted using different indicators, price and temporal information. Correlations between instances and features are utilized to order the features before they are presented as inputs to the CNN. The proposed classifier is compared with a CNN trained with randomly ordered features and Logistic Regression. Experimental results show that the proposed classifier outperforms both Logistic Regression and CNN that utilizes randomly ordered features. Feature selection methods are also utilized to reduce training time and model complexity.","Hakan Gunduz,Yusuf Yaslan,Zehra Cataltepe",Knowledge Based Systems,2017,,,
661,Content-based Neighbor Models for Cold Start in Recommender Systems,"Cold start remains a prominent problem in recommender systems. While rich content information is often available for both users and items few existing models can fully exploit it for personalization. Slow progress in this area can be partially attributed to the lack of publicly available benchmarks to validate and compare models. This year's ACM Recommender Systems Challenge' 17 aimed to address this gap by providing a standardized framework to benchmark cold start models. The challenge organizer XING released a large scaled data collection of user-job interactions from their career oriented social network. Unlike other competitions, here the participating teams were evaluated in two phases -- offline and online. Models were first evaluated on the held-out offline test set. Top models were then A/B tested in the online phase where new target users and items were released daily and recommendations were pushed into XING's live production system. In this paper we present our approach to this challenge, we used a combination of content and neighbor-based models winning both offline and online phases. Our model produced the most consistent online performance wining four of the five online weeks, and showed excellent generalization in the live A/B setting.","Maksims Volkovs,Guang Wei Yu,Tomi Poutanen",,2017,,,
662,Content-Based approaches for Cold-Start Job Recommendations,"This paper provides an overview of the approach we adopted as team Lunatic Goats for the ACM RecSys Challenge 2017 [7]. The competition, organized by XING.com, focuses on a cold start job recommendation scenario. The goal was to design and tune a recommendation system able to predict past users' interactions, for the offline stage, and to provide recommendations pushed every day to real users through the XING portal, for the online stage. Our strategy, which saw models coming from different techniques combined in a multi-layer ensemble, granted us the first place in the offline part and the qualification as second best team in the final leaderboard. All our algorithms mainly resort to content-based approaches, that, thanks to its ability to provide good recommendations even for cold-start items allowed us, quite unexpectedly, to achieve good results in terms of prediction quality and computational time.","Mattia Bianchi,Federico Cesaro,Filippo Ciceri,Mattia Dagrada,Alberto Gasparin,Daniele Grattarola,Daniele Grattarola,Daniele Grattarola,Ilyas Inajjar,Alberto Maria Metelli,Leonardo Cella",,2017,,,
663,Incorporating Dwell Time in Session-Based Recommendations with Recurrent Neural Networks.,,"Veronika Bogina,Tsvi Kuflik",,2017,,,
664,Playlist Recommendation Based on Reinforcement Learning,"Recently, there is a surge of recommender system to alleviate the Internet information overload. A number of recommendation techniques have been proposed for many applications, among which music recommendation is a kind of popular Internet services. Unlike other recommendation services, music recommendation needs to consider the interaction and content information, as well as the inherent correlation and feedback among music playlist. Thus, in this paper, we model music recommendation as a Markov Decision Process, and consider the music recommendation as a playlist recommendation task. Along this line, we propose a novel reinforcement learning based model, called RLWRec, to exploit the optimal strategy of playlist. Two novel strategies are designed to solve the curse of state space and efficient music recommendation. Experiments on real dataset validate the effectiveness of our proposed method.","Binbin Hu,Chuan Shi,Jian Liu",,2017,,,
665,Sequence-based context-aware music recommendation,"Contextual factors greatly affect users' preferences for music, so they can benefit music recommendation and music retrieval. However, how to acquire and utilize the contextual information is still facing challenges. This paper proposes a novel approach for context-aware music recommendation, which infers users' preferences for music, and then recommends music pieces that fit their real-time requirements. Specifically, the proposed approach first learns the low dimensional representations of music pieces from users' music listening sequences using neural network models. Based on the learned representations, it then infers and models users' general and contextual preferences for music from users' historical listening records. Finally, music pieces in accordance with user's preferences are recommended to the target user. Extensive experiments are conducted on real world datasets to compare the proposed method with other state-of-the-art recommendation methods. The results demonstrate that the proposed method significantly outperforms those baselines, especially on sparse data.","Dongjing Wang,Shuiguang Deng,Shuiguang Deng,Guandong Xu",Information Retrieval,2018,,,
666,Rainbow: Combining Improvements in Deep Reinforcement Learning,"The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.","Matteo Hessel,Joseph Modayil,Hado van Hasselt,Tom Schaul,Georg Ostrovski,Will Dabney,Dan Horgan,Bilal Piot,Mohammad Gheshlaghi Azar,David Silver",arXiv: Artificial Intelligence,2017,,,
667,Emergent Complexity via Multi-Agent Competition.,"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: this https URL","Trapit Bansal,Jakub Pachocki,Szymon Sidor,Ilya Sutskever,Igor Mordatch",arXiv: Artificial Intelligence,2017,,,
668,A survey on FinTech,"Abstract   As a new term in the financial industry, FinTech has become a popular term that describes novel technologies adopted by the financial service institutions. This term covers a large scope of techniques, from data security to financial service deliveries. An accurate and up-to-date awareness of FinTech has an urgent demand for both academics and professionals. This work aims to produce a survey of FinTech by collecting and reviewing contemporary achievements, by which a theoretical data-driven FinTech framework is proposed. Five technical aspects are summarized and involved, which include security and privacy, data techniques, hardware and infrastructure, applications and management, and service models. The main findings of this work are fundamentals of forming active FinTech solutions.","Keke Gai,Meikang Qiu,Xiaotong Sun",Journal of Network and Computer Applications,2018,,,
669,SSEL-ADE: A semi-supervised ensemble learning framework for extracting adverse drug events from social media.,"Abstract   With the development of Web 2.0 technology, social media websites have become lucrative but under-explored data sources for extracting adverse drug events (ADEs), which is a serious health problem. Besides ADE, other semantic relation types (e.g., drug indication and beneficial effect) could hold between the drug and adverse event mentions, making ADE relation extraction – distinguishing ADE relationship from other relation types – necessary. However, conducting ADE relation extraction in social media environment is not a trivial task because of the expertise-dependent, time-consuming and costly annotation process, and the feature space’s high-dimensionality attributed to intrinsic characteristics of social media data. This study aims  to develop a framework for ADE relation extraction using patient-generated content in social media with better performance than that delivered by previous efforts.  To achieve the objective, a general semi-supervised ensemble learning framework, SSEL-ADE, was developed. The framework exploited various lexical, semantic, and syntactic features, and integrated ensemble learning and semi-supervised learning. A series of experiments were conducted to verify the effectiveness of the proposed framework. Empirical results demonstrate the effectiveness of each component of SSEL-ADE and reveal that our proposed framework outperforms most of existing ADE relation extraction methods The SSEL-ADE can facilitate enhanced ADE relation extraction performance, thereby providing more reliable support for pharmacovigilance. Moreover, the proposed semi-supervised ensemble methods have the potential of being applied to effectively deal with other social media-based problems.","Jing Liu,Jing Liu,Jing Liu,Songzheng Zhao,Gang Wang,Gang Wang",Artificial Intelligence in Medicine,2017,,,
670,Mastering the game of Go without human knowledge,"Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.","David Silver,Julian Schrittwieser,Karen Simonyan,Ioannis Antonoglou,Aja Huang,Arthur Guez,Thomas Hubert,Lucas Baker,Matthew Lai,Adrian Bolton,Yutian Chen,Timothy Lillicrap,Fan Hui,Laurent Sifre,George van den Driessche,Thore Graepel,Demis Hassabis",Nature,2017,,,
671,A Variational Recurrent Neural Network for Session-Based Recommendations using Bayesian Personalized Ranking.,,"Panayiotis Christodoulou,Sotirios P. Chatzis,Andreas S. Andreou",,2017,,,
672,Visual Representation and Classification by Learning Group Sparse Deep Stacking Network,"Deep stacking networks (DSNs) have been successfully applied in classification tasks. Its architecture builds upon blocks of simplified neural network modules (SNNM). The hidden units are assumed to be independent in the SNNM module. However, this assumption prevents SNNM from learning the local dependencies between hidden units to better capture the information in the input data for the classification task. In addition, the hidden representations of input data in each class can be expectantly split into a group in real-world classification applications. Therefore, we propose two kinds of group sparse SNNM modules by mixing    $l_{1}$   -norm and    $l_{2}$   -norm. The first module learns the local dependencies among hidden units by dividing them into non-overlapping groups. The second module splits the representations of samples in different classes into separate groups to cluster the samples in each class. A group sparse DSN (GS-DSN) is constructed by stacking the group sparse SNNM modules. Experimental results further verify that our GS-DSN model outperforms the relevant classification methods. Particularly, GS-DSN achieves the state-of-the-art performance (99.1%) on 15-Scene.","Jun Li,Heyou Chang,Heyou Chang,Heyou Chang,Jian Yang,Wei Luo,Yun Fu",IEEE Transactions on Image Processing,2018,,,
673,Neural Attentive Session-based Recommendation,"Given e-commerce scenarios that user profiles are invisible, session-based recommendation is proposed to generate recommendation results from short sessions. Previous work only considers the user's sequential behavior in the current session, whereas the user's main purpose in the current session is not emphasized. In this paper, we propose a novel neural networks framework, i.e., Neural Attentive Recommendation Machine (NARM), to tackle this problem. Specifically, we explore a hybrid encoder with an attention mechanism to model the user's sequential behavior and capture the user's main purpose in the current session, which are combined as a unified session representation later. We then compute the recommendation scores for each candidate item with a bi-linear matching scheme based on this unified session representation. We train NARM by jointly learning the item and session representations as well as their matchings. We carried out extensive experiments on two benchmark datasets. Our experimental results show that NARM outperforms state-of-the-art baselines on both datasets. Furthermore, we also find that NARM achieves a significant improvement on long sessions, which demonstrates its advantages in modeling the user's sequential behavior and main purpose simultaneously.","Jing Li,Pengjie Ren,Zhumin Chen,Zhaochun Ren,Jun Ma",arXiv: Information Retrieval,2017,,,
674,Multimodal sentiment analysis with word-level fusion and reinforcement learning,"With the increasing popularity of video sharing websites such as YouTube and Facebook, multimodal sentiment analysis has received increasing attention from the scientific community. Contrary to previous works in multimodal sentiment analysis which focus on holistic information in speech segments such as bag of words representations and average facial expression intensity, we propose a novel deep architecture for multimodal sentiment analysis that is able to perform modality fusion at the word level. In this paper, we propose the Gated Multimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model that is composed of 2 modules. The Gated Multimodal Embedding allows us to alleviate the difficulties of fusion when there are noisy modalities. The LSTM with Temporal Attention can perform word level fusion at a finer fusion resolution between the input modalities and attends to the most important time steps. As a result, the GME-LSTM(A) is able to better model the multimodal structure of speech through time and perform better sentiment comprehension. We demonstrate the effectiveness of this approach on the publicly-available Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset by achieving state-of-the-art sentiment classification and regression results. Qualitative analysis on our model emphasizes the importance of the Temporal Attention Layer in sentiment prediction because the additional acoustic and visual modalities are noisy. We also demonstrate the effectiveness of the Gated Multimodal Embedding in selectively filtering these noisy modalities out. These results and analysis open new areas in the study of sentiment analysis in human communication and provide new models for multimodal fusion.","Minghai Chen,Sen Wang,Sen Wang,Paul Pu Liang,Tadas Baltrusaitis,Amir Zadeh,Louis-Philippe Morency",,2017,,,
675,Converging blockchain and next-generation artificial intelligence technologies to decentralize and accelerate biomedical research and healthcare.,"//     Polina Mamoshina 1,2 , Lucy Ojomoko 1 , Yury Yanovich 3 , Alex Ostrovski 3 , Alex Botezatu 3 , Pavel Prikhodko 3 , Eugene Izumchenko 4 , Alexander Aliper 1 , Konstantin Romantsov 1 , Alexander Zhebrak 1 , Iraneus Obioma Ogu 5  and Alex Zhavoronkov 1,6     1  Pharmaceutical Artificial Intelligence Department, Insilico Medicine, Inc., Emerging Technology Centers, Johns Hopkins University at Eastern, Baltimore, Maryland, USA    2  Department of Computer Science, University of Oxford, Oxford, United Kingdom    3  The Bitfury Group, Amsterdam, Netherlands    4  Department of Otolaryngology-Head & Neck Surgery, Johns Hopkins University School of Medicine, Baltimore, MD, USA    5  Africa Blockchain Artificial Intelligence for Healthcare Initiative, Insilico Medicine, Inc, Abuja, Nigeria    6  The Biogerontology Research Foundation, London, United Kingdom   Correspondence to:   Alex Zhavoronkov, email:  //       Keywords : artificial intelligence; deep learning; data management; blockchain; digital health; health data marketplace    Received : October 19, 2017  Accepted : November 02, 2017  Published : November 09, 2017    Abstract   The increased availability of data and recent advancements in artificial intelligence present the unprecedented opportunities in healthcare and major challenges for the patients, developers, providers and regulators. The novel deep learning and transfer learning techniques are turning any data about the person into medical data transforming simple facial pictures and videos into powerful sources of data for predictive analytics. Presently, the patients do not have control over the access privileges to their medical records and remain unaware of the true value of the data they have. In this paper, we provide an overview of the next-generation artificial intelligence and blockchain technologies and present innovative solutions that may be used to accelerate the biomedical research and enable patients with new tools to control and profit from their personal data as well with the incentives to undergo constant health monitoring. We introduce new concepts to appraise and evaluate personal records, including the combination-, time- and relationship-value of the data. We also present a roadmap for a blockchain-enabled decentralized personal health data ecosystem to enable novel approaches for drug discovery, biomarker development, and preventative healthcare. A secure and transparent distributed personal data marketplace utilizing blockchain and deep learning technologies may be able to resolve the challenges faced by the regulators and return the control over personal data including medical records back to the individuals.","Polina Mamoshina,Polina Mamoshina,Lucy Ojomoko,Yury Yanovich,Alex Ostrovski,Alex Botezatu,Pavel Prikhodko,Pavel Prikhodko,Eugene Izumchenko,Alexander Aliper,Konstantin Romantsov,Alexander Zhebrak,Iraneus Obioma Ogu,Alex Zhavoronkov",Oncotarget,2015,,,
676,Selective Value Coupling Learning for Detecting Outliers in High-Dimensional Categorical Data,"This paper introduces a novel framework, namely SelectVC and its instance POP, for learning selective value couplings (i.e., interactions between the full value set and a set of outlying values) to identify outliers in high-dimensional categorical data. Existing outlier detection methods work on a full data space or feature subspaces that are identified independently from subsequent outlier scoring. As a result, they are significantly challenged by overwhelming irrelevant features in high-dimensional data due to the noise brought by the irrelevant features and its huge search space. In contrast, SelectVC works on a clean and condensed data space spanned by selective value couplings by jointly optimizing outlying value selection and value outlierness scoring. Its instance POP defines a value outlierness scoring function by modeling a partial outlierness propagation process to capture the selective value couplings. POP further defines a top-k outlying value selection method to ensure its scalability to the huge search space. We show that POP (i) significantly outperforms five state-of-the-art full space- or subspace-based outlier detectors and their combinations with three feature selection methods on 12 real-world high-dimensional data sets with different levels of irrelevant features; and (ii) obtains good scalability, stable performance w.r.t. k, and fast convergence rate.","Guansong Pang,Hongzuo Xu,Longbing Cao,Wentao Zhao",,2017,,,
677,Learning Graph-based Embedding For Time-Aware Product Recommendation,"In this paper, we propose a novel Product Graph Embedding (PGE) model to investigate time-aware product recommendation by leveraging the network representation learning technique. Our model captures the sequential influences of products by transforming the historical purchase records into a product graph. Then the product can be transformed into a low dimensional vector by the network embedding model. Once products are projected into the latent space, we present a novel method to compute user's latest preferences, which projects users into the same latent space as products. This method is based on time-decay functions and the embedding of sequential products that the user purchased. Thus, relatedness between a product and a user can be measured by the similarity between the embedding vectors which represent the product and the user's preferences. The experimental results on purchase records crawled from JINGDONG, show the superiority of our proposed framework for personalized product recommendation.","Yuqi Li,Weizheng Chen,Hongfei Yan",,2017,,,
678,Deep Neural Networks for News Recommendations,"A fundamental role of news websites is to recommend articles that are interesting to read. The key challenge of news recommendation is to recommend newly published articles. Unlike other domains, outdated items are considered to be irrelevant in the news recommendation task. Another challenge is that the recommendation candidates are not seen in the training phase. In this paper, we introduce deep neural network models to overcome these challenges. we propose a modified session-based Recurrent Neural Network (RNN) model tailored to news recommendation as well as a history-based RNN model that spans the whole user's past histories. Finally, we propose a Convolutional Neural Network (CNN) model to capture user preferences and to personalize recommendation results. Experimental results on real-world news dataset shows that our model outperforms competitive baselines.","Keunchan Park,Ji-Soo Lee,Jaeho Choi",,2017,,,
679,From virtual demonstration to real-world manipulation using LSTM and MDN,"Robots assisting the disabled or elderly must perform complex manipulation tasks and must adapt to the home environment and preferences of their user. Learning from demonstration is a promising choice, that would allow the non-technical user to teach the robot different tasks. However, collecting demonstrations in the home environment of a disabled user is time consuming, disruptive to the comfort of the user, and presents safety challenges. It would be desirable to perform the demonstrations in a virtual environment. In this paper we describe a solution to the challenging problem of behavior transfer from virtual demonstration to a physical robot. The virtual demonstrations are used to train a deep neural network based controller, which is using a Long Short Term Memory (LSTM) recurrent neural network to generate trajectories. The training process uses a Mixture Density Network (MDN) to calculate an error signal suitable for the multimodal nature of demonstrations. The controller learned in the virtual environment is transferred to a physical robot (a Rethink Robotics Baxter). An off-the-shelf vision component is used to substitute for geometric knowledge available in the simulation and an inverse kinematics module is used to allow the Baxter to enact the trajectory. Our experimental studies validate the three contributions of the paper: (1) the controller learned from virtual demonstrations can be used to successfully perform the manipulation tasks on a physical robot, (2) the LSTM+MDN architectural choice outperforms other choices, such as the use of feedforward networks and mean-squared error based training signals and (3) allowing imperfect demonstrations in the training set also allows the controller to learn how to correct its manipulation mistakes.","Rouhollah Rahmatizadeh,Pooya Abolghasemi,Aman Behal,Ladislau Bölöni",arXiv: Robotics,2016,,,
680,TwinsCoin: A Cryptocurrency via Proof-of-Work and Proof-of-Stake,"We design and implement TwinsCoin, the first cryptocurrency based on a provably secure and scalable public blockchain design using both proof-of-work and proof-of-stake mechanisms. Different from the proof-of-work based Bitcoin, our construction uses two types of resources, computing power and coins (i.e., stake). The blockchain in our system is more robust than that in a pure proof-of-work based system; even if the adversary controls the majority of mining power, we can still have the chance to secure the system by relying on honest stake. In contrast, Bitcoin blockchain will be insecure if the adversary controls more than 50% of mining power.    Our design follows a recent provably secure proof-of-work/proof-of-stake hybrid blockchain[11]. In order to make our construction practical, we considerably enhance its design. In particular, we introduce a new strategy for difficulty adjustment in the hybrid blockchain and provide a theoretical analysis of it. We also show how to construct a light client for proof-of-stake cryptocurrencies and evaluate the proposal practically.    We implement our new design. Our implementation uses a recent modular development framework for blockchains, called Scorex. It allows us to change only certain parts of an application leaving other codebase intact. In addition to the blockchain implementation, a testnet is deployed. Source code is publicly available.","Tuyet Duong,Alexander Chepurnoy,Lei Fan,Hong-Sheng Zhou",,2018,,,
681,Stock market predication using a linear regression,"It is a serious challenge for investors and corporate stockholders to forecast the daily behavior of stock market which helps them to invest with more confidence by taking risks and fluctuations into consideration. In this paper, by applying linear regression for forecasting behavior of TCS data set, we prove that our proposed method is best to compare the other regression techniquemethod and the stockholders can invest confidentially based on that.","Dinesh Bhuriya,Girish Kaushal,Ashish Sharma,Ashish Sharma,Upendra K. Singh",,2017,,,
682,Deep Additive Least Squares Support Vector Machines for Classification With Model Transfer,"The additive kernel least squares support vector machine (AK-LS-SVM) has been well used in classification tasks due to its inherent advantages. For example, additive kernels work extremely well for some specific tasks, such as computer vision classification, medical research, and some specialized scenarios. Moreover, the analytical solution using AK-LS-SVM can formulate leave-one-out cross-validation error estimates in a closed form for parameter tuning, which drastically reduces the computational cost and guarantee the generalization performance especially on small and medium datasets. However, AK-LS-SVM still faces two main challenges: 1) improving the classification performance of AK-LS-SVM and 2) saving time when performing a grid search for model selection. Inspired by the stacked generalization principle and the transfer learning mechanism, a layer-by-layer combination of AK-LS-SVM classifiers embedded with transfer learning is proposed in this paper. This new classifier is called deep transfer additive kernel least square support vector machine (DTA-LS-SVM) which overcomes these two challenges. Also, considering that imbalanced datasets are involved in many real-world scenarios, especially for medical data analysis, the deep-transfer element is extended to compensate for this imbalance, thus leading to the development of another new classifier iDTA-LS-SVM. In the hierarchical structure of both DTA-LS-SVM and iDTA-LS-SVM, each layer has an AK-LS-SVM and the predictions from the previous layer act as an additional input feature for the current layer. Importantly, transfer learning is also embedded to guarantee generalization consistency between the adjacent layers. Moreover, both iDTA-LS-SVM and DTA-LS-SVM can ensure the minimal leave-one-out error by using the proposed fast leave-one-out cross validation strategy on the training set in each layer. We compared the proposed classifiers DTA-LS-SVM and iDTA-LS-SVM with the traditional LS-SVM and SVM using additive kernels on seven public UCI datasets and one real world dataset. The experimental results show that both DTA-LS-SVM and iDTA-LS-SVM exhibit better generalization performance and faster learning speed.","Guanjin Wang,Guangquan Zhang,Kup-Sze Choi,Jie Lu",,2019,,,
683,Informed trading in the Bitcoin market,"Bitcoin’s price sensitivity to the material events makes informed trading very profitable in this new market. We propose a novel indicator to assess informed trades ahead of cryptocurrency-related events. Using trade-level data of USD/BTC exchange rates, we find evidence of informed trading in the Bitcoin market prior to large events: Quantiles of the order sizes of buyer-initiated (seller-initiated) orders are abnormally high before large positive (negative) events, compared to the quantiles of seller-initiated (buyer-initiated) orders. When examining the timing of informed trades, we further notice that informed traders prefer to build their positions two days before large positive events and one day before large negative events. The profits of informed trading in the Bitcoin market are estimated to be considerably large.","Wenjun Feng,Yiming Wang,Zhengjun Zhang",Finance Research Letters,2017,,,
684,Latent Cross: Making Use of Context in Recurrent Recommender Systems,"The success of recommender systems often depends on their ability to understand and make use of the context of the recommendation request. Significant research has focused on how time, location, interfaces, and a plethora of other contextual features affect recommendations. However, in using deep neural networks for recommender systems, researchers often ignore these contexts or incorporate them as ordinary features in the model.   In this paper, we study how to effectively treat contextual data in neural recommender systems. We begin with an empirical analysis of the conventional approach to context as features in feed-forward recommenders and demonstrate that this approach is inefficient in capturing common feature crosses. We apply this insight to design a state-of-the-art RNN recommender system. We first describe our RNN-based recommender system in use at YouTube. Next, we offer ""Latent Cross,"" an easy-to-use technique to incorporate contextual data in the RNN by embedding the context feature first and then performing an element-wise product of the context embedding with model's hidden states. We demonstrate the improvement in performance by using this Latent Cross technique in multiple experimental settings.","Alex Beutel,Alex Beutel,Alex Beutel,Paul Covington,Sagar Jain,Can Xu,Can Xu,Can Xu,Can Xu,Jia Li,Vince Gatto,Ed H. Chi,Ed H. Chi",,2018,,,
685,"Stock price prediction using LSTM, RNN and CNN-sliding window model","Stock market or equity market have a profound impact in today's economy. A rise or fall in the share price has an important role in determining the investor's gain. The existing forecasting methods make use of both linear (AR, MA, ARIMA) and non-linear algorithms (ARCH, GARCH, Neural Networks), but they focus on predicting the stock index movement or price forecasting for a single company using the daily closing price. The proposed method is a model independent approach. Here we are not fitting the data to a specific model, rather we are identifying the latent dynamics existing in the data using deep learning architectures. In this work we use three different deep learning architectures for the price prediction of NSE listed companies and compares their performance. We are applying a sliding window approach for predicting future values on a short term basis. The performance of the models were quantified using percentage error.","Sreelekshmy Selvin,R. Vinayakumar,E. A. Gopalakrishnan,E. A. Gopalakrishnan,Vijay Krishna Menon,K. P. Soman,K. P. Soman",,2017,,,
686,Inferring Implicit Rules by Learning Explicit and Hidden Item Dependency,"Revealing complex relations between entities (e.g., items within or between transactions) is of great significance for business optimization, prediction, and decision making. Such relations include not only co-occurrence-based explicit relations but also nonco-occurrence-based implicit ones. Explicit relations have been substantially studied by rule mining-based approaches, including association rule mining and causal rule discovery. In contrast, implicit relations have received much less attention but could be more actionable. In this paper, we focus on the implicit relations between items which rarely or never co-occur while each of them co-occurs with other identical items (link items) with a high probability. A framework integrates both explicit and hidden item dependencies and a corresponding efficient algorithm IRRMiner captures such implicit relations with implicit rule inference. Experimental results show that IRRMiner not only infers implicit rules of various sizes consisting of both frequent and infrequent items effectively, it also runs at least four times faster than IARMiner, a typical indirect association rule mining algorithm which can only mine size-2 indirect association rules between frequent items. IRRMiner is applied to make recommendations and shows that the identified implicit rules can increase recommendation reliability.","Shoujin Wang,Longbing Cao","IEEE Transactions on Systems, Man, and Cybernetics",2020,,,
687,Perceiving the Next Choice with Comprehensive Transaction Embeddings for Online Recommendation,"To predict customer’s next choice in the context of what he/she has bought in a session is interesting and critical in the transaction domain especially for online shopping. Precise prediction leads to high quality recommendations and thus high benefit. Such kind of recommendation is usually formalized as transaction-based recommender systems (TBRS). Existing TBRS either tend to recommend popular items while ignore infrequent and newly-released ones (e.g., pattern-based RS) or assume a rigid order between items within a transaction (e.g., Markov Chain-based RS) which does not satisfy real-world cases in most time. In this paper, we propose a neural network-based comprehensive transaction embedding model (NTEM) which can effectively perceive the next choice in a transaction context. Specifically, we learn these comprehensive embeddings of both items and their features from relaxed ordered transactions. The relevance between items revealed by the transactions is encoded into such embeddings. With rich information embedded, such embeddings are powerful to predict the next choices given those already bought items. NTEM is a shallow wide-in-wide-out network, which is more efficient than deep networks considering large numbers of items and transactions. Experimental results on real-world datasets show that NTEM outperforms three typical TBRS models FPMC, PRME and GRU4Rec in terms of recommendation accuracy and novelty. Our implementation is available at https://github.com/shoujin88/NTEM-model.","Shoujin Wang,Liang Hu,Longbing Cao",,2017,,,
688,BIER — Boosting Independent Embeddings Robustly,"Learning similarity functions between image pairs with deep neural networks yields highly correlated activations of large embeddings. In this work, we show how to improve the robustness of embeddings by exploiting independence in ensembles. We divide the last embedding layer of a deep network into an embedding ensemble and formulate training this ensemble as an online gradient boosting problem. Each learner receives a reweighted training sample from the previous learners. This leverages large embedding sizes more effectively by significantly reducing correlation of the embedding and consequently increases retrieval accuracy of the embedding. Our method does not introduce any additional parameters and works with any differentiable loss function. We evaluate our metric learning method on image retrieval tasks and show that it improves over state-ofthe- art methods on the CUB-200-2011, Cars-196, Stanford Online Products, In-Shop Clothes Retrieval and VehicleID datasets by a significant margin.","Michael Opitz,Georg Waltner,Horst Possegger,Horst Bischof",,2017,,,
689,Leveraging Long and Short-term Information in Content-aware Movie Recommendation.,"Movie recommendation systems provide users with ranked lists of movies based on individual's preferences and constraints. Two types of models are commonly used to generate ranking results: long-term models and session-based models. While long-term models represent the interactions between users and movies that are supposed to change slowly across time, session-based models encode the information of users' interests and changing dynamics of movies' attributes in short terms. In this paper, we propose an LSIC model, leveraging Long and Short-term Information in Content-aware movie recommendation using adversarial training. In the adversarial process, we train a generator as an agent of reinforcement learning which recommends the next movie to a user sequentially. We also train a discriminator which attempts to distinguish the generated list of movies from the real records. The poster information of movies is integrated to further improve the performance of movie recommendation, which is specifically essential when few ratings are available. The experiments demonstrate that the proposed model has robust superiority over competitors and sets the state-of-the-art. We will release the source code of this work after publication.","Wei Zhao,Wei Zhao,Benyou Wang,Jianbo Ye,Yongqiang Gao,Min Yang,Min Yang,Zhou Zhao,Xiaojun Chen,Xiaojun Chen",arXiv: Information Retrieval,2017,,,
690,Legal Risks and the Countermeasures of Developing Intelligent Investment Advisor in China,"The current Chinese intelligent investment advisor development is still in its infancy, the new formats, business models it brings and the current regulations cannot adapt to each other, which leads to the handicap of market development. In order to solve the legal risks faced by Chinese intelligent investment advisor development, the article thinks that few measures should be taken. Existing barriers in legal system shall be removed and a new solution shall be found based on the balance between “financial security” and “financial efficiency” to facilitate the pace of financial innovation like Robo-advisor.",Cgeng-yong Liu,,2018,,,
691,Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,"Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy - that is, succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.","Tuomas Haarnoja,Aurick Zhou,Pieter Abbeel,Sergey Levine",arXiv: Learning,2018,,,
692,Deep Reinforcement Learning for List-wise Recommendations,"Recommender systems play a crucial role in mitigating the problem of information overload by suggesting users' personalized items or services. The vast majority of traditional recommender systems consider the recommendation procedure as a static process and make recommendations following a fixed strategy. In this paper, we propose a novel recommender system with the capability of continuously improving its strategies during the interactions with users. We model the sequential interactions between users and a recommender system as a Markov Decision Process (MDP) and leverage Reinforcement Learning (RL) to automatically learn the optimal strategies via recommending trial-and-error items and receiving reinforcements of these items from users' feedbacks. In particular, we introduce an online user-agent interacting environment simulator, which can pre-train and evaluate model parameters offline before applying the model online. Moreover, we validate the importance of list-wise recommendations during the interactions between users and agent, and develop a novel approach to incorporate them into the proposed framework LIRD for list-wide recommendations. The experimental results based on a real-world e-commerce dataset demonstrate the effectiveness of the proposed framework.","Xiangyu Zhao,Xiangyu Zhao,Xiangyu Zhao,Liang Zhang,Zhuoye Ding,Dawei Yin,Yihong Zhao,Jiliang Tang",arXiv: Learning,2017,,,
693,Short-Term Satisfaction and Long-Term Coverage: Understanding How Users Tolerate Algorithmic Exploration,"Any learning algorithm for recommendation faces a fundamental trade-off between exploiting partial knowledge of a user»s interests to maximize satisfaction in the short term and discovering additional user interests to maximize satisfaction in the long term. To enable discovery, a machine learning algorithm typically elicits feedback on items it is uncertain about, which is termed algorithmic exploration in machine learning. This exploration comes with a cost to the user, since the items an algorithm chooses for exploration frequently turn out to not match the user»s interests. In this paper, we study how users tolerate such exploration and how presentation strategies can mitigate the exploration cost. To this end, we conduct a behavioral study with over 600 people, where we vary how algorithmic exploration is mixed into the set of recommendations. We find that users respond non-linearly to the amount of exploration, where some exploration mixed into the set of recommendations has little effect on short-term satisfaction and behavior. For long-term satisfaction, the overall goal is to learn via exploration about the items presented. We therefore also analyze the quantity and quality of implicit feedback signals such as clicks and hovers, and how they vary with different amounts of mix-in exploration. Our findings provide insights into how to design presentation strategies for algorithmic exploration in interactive recommender systems, mitigating the short-term costs of algorithmic exploration while aiming to elicit informative feedback data for learning.","Tobias Schnabel,Paul N. Bennett,Susan T. Dumais,Thorsten Joachims",,2018,,,
694,Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding,"Top-N sequential recommendation models each user as a sequence of items interacted in the past and aims to predict top-N ranked items that a user will likely interact in a »near future». The order of interaction implies that sequential patterns play an important role where more recent items in a sequence have a larger impact on the next item. In this paper, we propose a Convolutional Sequence Embedding Recommendation Model »Caser» as a solution to address this requirement. The idea is to embed a sequence of recent items into an »image» in the time and latent spaces and learn sequential patterns as local features of the image using convolutional filters. This approach provides a unified and flexible network structure for capturing both general preferences and sequential patterns. The experiments on public data sets demonstrated that Caser consistently outperforms state-of-the-art sequential recommendation methods on a variety of common evaluation metrics.","Jiaxi Tang,Ke Wang",,2018,,,
695,Heterogeneous Metric Learning of Categorical Data with Hierarchical Couplings,"Learning appropriate metric is critical for effectively capturing complex data characteristics. The metric learning of categorical data with hierarchical coupling relationships and local heterogeneous distributions is very challenging yet rarely explored. This paper proposes a Heterogeneous mEtric Learning with hIerarchical Couplings (HELIC for short) for this type of categorical data. HELIC captures both low-level value-to-attribute and high-level attribute-to-class hierarchical couplings, and reveals the intrinsic heterogeneities embedded in each level of couplings. Theoretical analyses of the effectiveness and generalization error bound verify that HELIC effectively represents the above complexities. Extensive experiments on 30 data sets with diverse characteristics demonstrate that HELIC-enabled classification significantly enhances the accuracy (up to 40.93 percent), compared with five state-of-the-art baselines.","Chengzhang Zhu,Longbing Cao,Qiang Liu,Qiang Liu,Jianping Yin,Vipin Kumar",IEEE Transactions on Knowledge and Data Engineering,2018,,,
696,Digital Twin and Big Data Towards Smart Manufacturing and Industry 4.0: 360 Degree Comparison,"With the advances in new-generation information technologies, especially big data and digital twin, smart manufacturing is becoming the focus of global manufacturing transformation and upgrading. Intelligence comes from data. Integrated analysis for the manufacturing big data is beneficial to all aspects of manufacturing. Besides, the digital twin paves a way for the cyber-physical integration of manufacturing, which is an important bottleneck to achieve smart manufacturing. In this paper, the big data and digital twin in manufacturing are reviewed, including their concept as well as their applications in product design, production planning, manufacturing, and predictive maintenance. On this basis, the similarities and differences between big data and digital twin are compared from the general and data perspectives. Since the big data and digital twin can be complementary, how they can be integrated to promote smart manufacturing are discussed.","Qinglin Qi,Fei Tao",IEEE Access,2018,,,
697,Sequential Recommendation with User Memory Networks,"User preferences are usually dynamic in real-world recommender systems, and a user»s historical behavior records may not be equally important when predicting his/her future interests. Existing recommendation algorithms -- including both shallow and deep approaches -- usually embed a user»s historical records into a single latent vector/representation, which may have lost the per item- or feature-level correlations between a user»s historical records and future interests. In this paper, we aim to express, store, and manipulate users» historical records in a more explicit, dynamic, and effective manner. To do so, we introduce the memory mechanism to recommender systems. Specifically, we design a memory-augmented neural network (MANN) integrated with the insights of collaborative filtering for recommendation. By leveraging the external memory matrix in MANN, we store and update users» historical records explicitly, which enhances the expressiveness of the model. We further adapt our framework to both item- and feature-level versions, and design the corresponding memory reading/writing operations according to the nature of personalized recommendation scenarios. Compared with state-of-the-art methods that consider users» sequential behavior for recommendation, e.g., sequential recommenders with recurrent neural networks (RNN) or Markov chains, our method achieves significantly and consistently better performance on four real-world datasets. Moreover, experimental analyses show that our method is able to extract the intuitive patterns of how users» future actions are affected by previous behaviors.","Xu Chen,Xu Chen,Xu Chen,Xu Chen,Hongteng Xu,Yongfeng Zhang,Jiaxi Tang,Yixin Cao,Yixin Cao,Zheng Qin,Hongyuan Zha",,2018,,,
698,TOSCA Solves Big Problems in the Cloud and Beyond,"TOSCA, the Topology and Orchestration Specification for Cloud Applications offers an OASIS-recognized, open standard domain-specific language (DSL) that enables portability and automated management of applications, services, and resources regardless of underlying cloud platform, software defined environment, or infrastructure. With a growing, interoperable eco-system of open source projects, solutions from leading cloud platform and service providers, and research, TOSCA empowers the definition and modeling of applications and their services (microservices or traditional services) across their entire lifecycle by describing their components, relationships, dependencies, requirements, and capabilities for orchestrating software in the context of associated operational policies. The authors introduce important TOSCA concepts and benefits in the context of commonly understood cloud use cases as a foundation to future discussions regarding advanced TOSCA concepts and additional breakthrough issues.","Paul Lipton,Derek Palma,Matt F. Rutkowski,Matthew Francis Rutkowski,Damian A. Tamburri",IEEE Cloud Computing,2018,,,
699,Forward Forecast of Stock Price Using Sliding-Window Metaheuristic-Optimized Machine-Learning Regression,"Time series forecasting has been widely used to determine the future prices of stock, and the analysis and modeling of finance time series importantly guide investors’ decisions and trades. In addition, in a dynamic environment such as the stock market, the nonlinearity of the time series is pronounced, immediately affecting the efficacy of stock price forecasts. Thus, this paper proposes an intelligent time series prediction system that uses sliding-window metaheuristic optimization for the purpose of predicting the stock prices of Taiwan construction companies one step ahead. It may be of great interest to home brokers who do not possess sufficient knowledge to invest in such companies. The system has a graphical user interface and functions as a stand-alone application. The developed hybrid system exhibited outstanding prediction performance and it improves overall profit for investment performance. The proposed model is a promising predictive technique for highly nonlinear time series, whose patterns are difficult to capture by traditional models.","Jui-Sheng Chou,Thi-Kha Nguyen",IEEE Transactions on Industrial Informatics,2018,,,
700,Bitcoin: Medium of Exchange or Speculative Assets?,"Bitcoin is defined as digital money within a decentralized peer-to-peer payment network. It is a hybrid between fiat currency and commodity currency without intrinsic value and independent of any government or monetary authority. This paper analyses the question of whether Bitcoin is a medium of exchange or an asset and more specifically, what is its current usage and what usage will prevail in the future given its characteristics. We analyse the statistical properties of Bitcoin and find that it is uncorrelated with traditional asset classes such as stocks, bonds and commodities both in normal times and in periods of financial turmoil. The analysis of transaction data of Bitcoin accounts shows that Bitcoins are mainly used as a speculative investment and not as an alternative currency and medium of exchange.","Dirk G. Baur,Ki Hoon Hong,Ki Hoon Hong,Adrian Lee","Journal of International Financial Markets, Institutions and Money",2017,,,
701,Credit Card Fraud Detection Using AdaBoost and Majority Voting,"Credit card fraud is a serious problem in financial services. Billions of dollars are lost due to credit card fraud every year. There is a lack of research studies on analyzing real-world credit card data owing to confidentiality issues. In this paper, machine learning algorithms are used to detect credit card fraud. Standard models are first used. Then, hybrid methods which use AdaBoost and majority voting methods are applied. To evaluate the model efficacy, a publicly available credit card data set is used. Then, a real-world credit card data set from a financial institution is analyzed. In addition, noise is added to the data samples to further assess the robustness of the algorithms. The experimental results positively indicate that the majority voting method achieves good accuracy rates in detecting fraud cases in credit cards.","Kuldeep Randhawa,Chu Kiong Loo,Manjeevan Seera,Chee Peng Lim,Asoke K. Nandi",IEEE Access,2018,,,
702,A Multi-objective Deep Reinforcement Learning Approach for Stock Index Future’s Intraday Trading,"Modern artificial intelligence has been widely discussed to practice in automated financial asserts trading. Automated intraday trading means that the agent can react to the market conditions automatically, while simultaneously make the right decisions. Besides, the profits will be made within a day considering transaction cost charged by the brokerage company. In this paper, we introduce a multiobjective deep reinforcement learning approach for intraday financial signal representation and trading. We design the deep neural networks to automatically discover the dynamic market features, then a reinforcement learning method implemented by a special kind of recurrent neural network (LSTM) is applied to make continuous trading decisions. In terms of balancing the profit and risk, we implement a multi-objective structure which includes two objectives with different weights. We conduct experiments on stock index futures data, and our analysis and experiments not only offer insights into financial market features mining, but also provide a straightforward and reliable method to make profits, which sheds light on its wide application on automated financial trading.","Weiyu Si,Jinke Li,Peng Ding,Ruonan Rao",,2017,,,
703,Enabling Cognitive Smart Cities Using Big Data and Machine Learning: Approaches and Challenges,"The development of smart cities and their fast-paced deployment is resulting in the generation of large quantities of data at unprecedented rates. Unfortunately, most of the generated data is wasted without extracting potentially useful information and knowledge because of the lack of established mechanisms and standards that benefit from the availability of such data. Moreover, the highly dynamic nature of smart cities calls for a new generation of machine learning approaches that are flexible and adaptable to cope with the dynamicity of data to perform analytics and learn from real-time data. In this article, we shed light on the challenge of underutilizing the big data generated by smart cities from a machine learning perspective. In particular, we present the phenomenon of wasting unlabeled data. We argue that semi-supervision is a must for smart cities to address this challenge. We also propose a three-level learning framework for smart cities that matches the hierarchical nature of big data generated by smart cities with a goal of providing different levels of knowledge abstraction. The proposed framework is scalable to meet the needs of smart city services. Fundamentally, the framework benefits from semi-supervised deep reinforcement learning where a small amount of data that has users' feedback serves as labeled data, while a larger amount without such users' feedback serves as unlabeled data. The framework utilizes a mix of labeled and unlabeled data to converge toward better control policies instead of wasting the unlabeled data. This article also explores how deep reinforcement learning and its shift toward semi-supervision can handle the cognitive side of smart city services and improve their performance by providing several use cases spanning the different domains of smart cities. We also highlight several challenges as well as promising future research directions for incorporating machine learning and high-level intelligence into smart city services.","Mehdi Mohammadi,Ala Al-Fuqaha",IEEE Communications Magazine,2018,,,
704,Evolved Policy Gradients.,"We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent's experience. Because this loss is highly flexible in its ability to take into account the agent's history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG's learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.","Rein Houthooft,Richard Y. Chen,Phillip Isola,Bradly C. Stadie,Filip Wolski,Jonathan Ho,Pieter Abbeel",arXiv: Learning,2018,,,
705,Addressing Function Approximation Error in Actor-Critic Methods,"In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and critic. Our algorithm takes the minimum value between a pair of critics to restrict overestimation and delays policy updates to reduce per-update error. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.","Scott Fujimoto,Herke van Hoof,David Meger",arXiv: Artificial Intelligence,2018,,,
706,Meta-Reinforcement Learning of Structured Exploration Strategies,"Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise (MAESN) -- to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.","Abhishek Gupta,Abhishek Gupta,Abhishek Gupta,Russell Mendonca,YuXuan Liu,Pieter Abbeel,Sergey Levine,Sergey Levine",arXiv: Learning,2018,,,
707,Attention-based transactional context embedding for next-item recommendation,"To recommend the next item to a user in a transactional context is practical yet challenging in applications such as marketing campaigns. Transactional context refers to the items that are observable in a transaction. Most existing transaction based recommender systems (TBRSs) make recommendations by mainly considering recently occurring items instead of all the ones observed in the current context. Moreover, they often assume a rigid order between items within a transaction, which is not always practical. More importantly, a long transaction often contains many items irreverent to the next choice, which tends to overwhelm the influence of a few truly relevant ones. Therefore, we posit that a good TBRS should not only consider all the observed items in the current transaction but also weight them with different relevance to build an attentive context that outputs the proper next item with a high probability. To this end, we design an effective attention based transaction embedding model (ATEM) for context embedding to weight each observed item in a transaction without assuming order. The empirical study on real-world transaction datasets proves that ATEM significantly outperforms the state-of-the-art methods in terms of both accuracy and novelty.","Shoujin Wang,Liang Hu,Longbing Cao,Xiaoshui Huang,Defu Lian,Wei Liu,Wei Liu,Wei Liu",,2018,,,
708,Ensemble learning: A survey,,"Omer Sagi,Lior Rokach",Wiley Interdisciplinary Reviews-Data Mining and Knowledge Discovery,2018,,,
709,Diagnostic Inferencing via Improving Clinical Concept Extraction with Deep Reinforcement Learning: A Preliminary Study,,"Yuan Ling,Sadid A. Hasan,Vivek V. Datla,Ashequl Qadir,Kathy Lee,Joey Liu,Oladimeji Farri",,2017,,,
710,Sparse Deep Stacking Network for Fault Diagnosis of Motor,"A sparse deep learning method is proposed to overcome overfitting risk of deep networks with a large number of nodes and layers. Deep stacking network (DSN) is a classic and effective deep learning method, and its sparse form is presented to generate the sparse deep learning method. In DSN, output labels are encoded as a series consisted of 1 and 0. This coding strategy makes output labels to be sparse. However, sparsity of output labels is not considered in DSN model. Considering this limitation, sparse DSN (SDSN) is developed in this paper. The SDSN extends tradition DSN in sparsity characterization using a sparse regularization term. By this term, predicted output label is constrained to be similar with ideal output label that is binary and consisted of continuous 1 and 0 with a sidestep shape. The sparse regularization term is used as a soft threshold strategy to set irrelevant element to be zero, by which effectiveness of SDSN is enhanced. Case studies about fault diagnosis of motor are used to validate performance of SDSN. Comparison between SDSN and commonly used deep networks is further conducted. The results show advance of SDSN for fault classification.","Chuang Sun,Meng Ma,Zhibin Zhao,Xuefeng Chen",IEEE Transactions on Industrial Informatics,2018,,,
711,The lncLocator: a subcellular localization predictor for long non-coding RNAs based on a stacked ensemble classifier.,"The long non-coding RNA (lncRNA) studies have been hot topics in the field of RNA biology. Recent studies have shown that their subcellular localizations carry important information for understanding their complex biological functions. Considering the costly and time-consuming experiments for identifying subcellular localization of lncRNAs, computational methods are urgently desired. However, to the best of our knowledge, there are no computational tools for predicting the lncRNA subcellular locations to date. In this study, we report an ensemble classifier-based predictor, lncLocator, for predicting the lncRNA subcellular localizations. To fully exploit lncRNA sequence information, we adopt both k-mer features and high-level abstraction features generated by unsupervised deep models, and construct four classifiers by feeding these two types of features to support vector machine (SVM) and random forest (RF), respectively. Then we use a stacked ensemble strategy to combine the four classifiers and get the final prediction results. The current lncLocator can predict five subcellular localizations of lncRNAs, including cytoplasm, nucleus, cytosol, ribosome and exosome, and yield an overall accuracy of 0.59 on the constructed benchmark dataset. The lncLocator is available at www.csbio.sjtu.edu.cn/bioinf/lncLocator. Supplementary data are available at Bioinformatics online.","Zhen Cao,Xiaoyong Pan,Yang Yang,Yan Huang,Hong-Bin Shen",Bioinformatics,2018,,,
712,"Trades, Quotes and Prices: Financial Markets Under the Microscope","The widespread availability of high-quality, high-frequency data has revolutionised the study of financial markets. By describing not only asset prices, but also market participants' actions and interactions, this wealth of information offers a new window into the inner workings of the financial ecosystem. In this original text, the authors discuss empirical facts of financial markets and introduce a wide range of models, from the micro-scale mechanics of individual order arrivals to the emergent, macro-scale issues of market stability. Throughout this journey, data is king. All discussions are firmly rooted in the empirical behaviour of real stocks, and all models are calibrated and evaluated using recent data from Nasdaq. By confronting theory with empirical facts, this book for practitioners, researchers and advanced students provides a fresh, new, and often surprising perspective on topics as diverse as optimal trading, price impact, the fragile nature of liquidity, and even the reasons why people trade at all.","Jean-Philippe Bouchaud,Julius Bonart,Jonathan Donier,Jonathan Donier,Martin D. Gould",,2018,,,
713,Universal features of price formation in financial markets: perspectives from Deep Learning,"Using a large-scale Deep Learning approach applied to a high-frequency database containing billions of electronic market quotes and transactions for US equities, we uncover nonparametric evidence for the existence of a universal and stationary price formation mechanism relating the dynamics of supply and demand for a stock, as revealed through the order book, to subsequent variations in its market price. We assess the model by testing its out-of-sample predictions for the direction of price moves given the history of price and order flow, across a wide range of stocks and time periods. The universal price formation model is shown to exhibit a remarkably stable out-of-sample prediction accuracy across time, for a wide range of stocks from different sectors. Interestingly, these results also hold for stocks which are not part of the training sample, showing that the relations captured by the model are universal and not asset-specific. 
The universal model --- trained on data from all stocks --- outperforms, in terms of out-of-sample prediction accuracy, asset-specific linear and nonlinear models trained on time series of any given stock, showing that the universal nature of price formation weighs in favour of pooling together financial data from various stocks, rather than designing asset- or sector-specific models as commonly done. Standard data normalizations based on volatility, price level or average spread, or partitioning the training data into sectors or categories such as large/small tick stocks, do not improve training results. On the other hand, inclusion of price and order flow history over many past observations is shown to improve forecasting performance, showing evidence of path-dependence in price dynamics.","Justin A. Sirignano,Rama Cont",arXiv: Statistical Finance,2018,,,
714,Heterogeneous Ensemble for Default Prediction of Peer-to-Peer Lending in China,"As a novel financing method, peer-to-peer (P2P) lending has drawn extensive attention as it provides those financers who cannot participate in the traditional financial market with funds. In P2P lending marketplaces, one of the crucial challenges that P2P online lending platforms are facing is to accurately predict the default risk of each loan by tapping into default prediction models, thus effectively helping P2P lending companies avoid credit risks. That traditional credit risk prediction models fail to meet the demand of P2P lending companies for default risk prediction, which is because of the uneven distribution of credit data samples in the P2P lending marketplaces (i.e., the default sampled data are scarce). In this paper, we designed a multi-round ensemble learning model based on heterogeneous ensemble frameworks to predict default risk. In this model, an extreme gradient boosting (XGBoost) is initially used for ensemble learning, and the XGBoost, deep neural network, and logistic regression are then regarded as heterogeneous individual learners to undergo a linear weighted fusion. To verify the designed default risk prediction model, real credit data from a famous P2P online lending marketplace in China were used in a test. The results of the experiment indicate that this model can effectively increase the predictive accuracy compared with traditional machine learning models and ensemble learning models.","Wei Li,Shuai Ding,Shuai Ding,Yi Chen,Shan-Lin Yang,Shanlin Yang",IEEE Access,2018,,,
715,Ensemble incremental learning Random Vector Functional Link network for short-term electric load forecasting,"Abstract   Short-term electric load forecasting plays an important role in the management of modern power systems. Improving the accuracy and efficiency of electric load forecasting can help power utilities design reasonable operational planning which will lead to the improvement of economic and social benefits of the systems. A hybrid incremental learning approach composed of Discrete Wavelet Transform (DWT), Empirical Mode Decomposition (EMD) and Random Vector Functional Link network (RVFL) is presented in this work. RVFL network is a universal approximator with good efficiency because of the randomly generated weights between input and hidden layers and the close form solution for parameter computation. By introducing incremental learning, along with ensemble approach via DWT and EMD into RVFL network, the forecasting performance can be significantly improved with respect to both efficiency and accuracy. The electric load datasets from Australian Energy Market Operator (AEMO) were used to evaluate the effectiveness of the proposed incremental DWT-EMD based RVFL network. Moreover, the attractiveness of the proposed method can be demonstrated by the comparison with eight benchmark forecasting methods.","Xueheng Qiu,Ponnuthurai Nagaratnam Suganthan,Gehan A. J. Amaratunga",Knowledge Based Systems,2018,,,
716,Blockchain Meets IoT: An Architecture for Scalable Access Management in IoT,"The Internet of Things (IoT) is stepping out of its infancy into full maturity and establishing itself as a part of the future Internet. One of the technical challenges of having billions of devices deployed worldwide is the ability to manage them. Although access management technologies exist in IoT, they are based on centralized models which introduce a new variety of technical limitations to manage them globally. In this paper, we propose a new architecture for arbitrating roles and permissions in IoT. The new architecture is a fully distributed access control system for IoT based on blockchain technology. The architecture is backed by a proof of concept implementation and evaluated in realistic IoT scenarios. The results show that the blockchain technology could be used as access management technology in specific scalable IoT scenarios.",Oscar Novo,IEEE Internet of Things Journal,2018,,,
717,Unsupervised Coupled Metric Similarity for Non-IID Categorical Data,"Appropriate similarity measures always play a critical role in data analytics, learning, and processing. Measuring the intrinsic similarity of categorical data for unsupervised learning has not been substantially addressed, and even less effort has been made for the similarity analysis of categorical data that is not independent and identically distributed (non-IID). In this work, a Coupled Metric Similarity (CMS) is defined for unsupervised learning which flexibly captures the value-to-attribute-to-object heterogeneous coupling relationships. CMS learns the similarities in terms of intrinsic heterogeneous intra- and inter-attribute couplings and attribute-to-object couplings in categorical data. The CMS validity is guaranteed by satisfying metric properties and conditions, and CMS can flexibly adapt to IID to non-IID data. CMS is incorporated into spectral clustering and k-modes clustering and compared with relevant state-of-the-art similarity measures that are not necessarily metrics. The experimental results and theoretical analysis show the CMS effectiveness of capturing independent and coupled data characteristics, which significantly outperforms other similarity measures on most datasets.","Songlei Jian,Longbing Cao,Kai Lu,Hang Gao",IEEE Transactions on Knowledge and Data Engineering,2018,,,
718,Trading financial indices with reinforcement learning agents,"Abstract   Intelligent agents are often used in professional portfolio management. The use of intelligent agents in personal retirement portfolio management is not investigated in the past. In this research, we consider a two-asset personal retirement portfolio and propose several reinforcement learning agents for trading portfolio assets. In particular, we design an on-policy SARSA (λ) and an off-policy Q(λ) discrete state and discrete action agents that maximize either portfolio returns or differential Sharpe ratios. Additionally, we design a temporal-difference learning, TD(λ), agent that uses a linear valuation function in discrete state and continuous action settings. Using two different two-asset portfolios, the first asset being the S&P 500 Index and the second asset being either a broad bond market index or a 10-year U.S. Treasury note (T-note), we test the performance of different agents on different holdout (test) samples. The results of our experiments indicate that the high-learning frequency (i.e., adaptive learning) TD(λ) agent consistently beats both the single asset stock and bond cumulative returns by a significant margin.","Parag C. Pendharkar,Patrick J. Cusatis",Expert Systems With Applications,2018,,,
719,Forecasting Stock Market Movement Direction Using Sentiment Analysis and Support Vector Machine,"Investor sentiment plays an important role on the stock market. User-generated textual content on the Internet provides a precious source to reflect investor psychology and predicts stock prices as a complement to stock market data. This paper integrates sentiment analysis into a machine learning method based on support vector machine. Furthermore, we take the day-of-week effect into consideration and construct more reliable and realistic sentiment indexes. Empirical results illustrate that the accuracy of forecasting the movement direction of the SSE 50 Index can be as high as 89.93% with a rise of 18.6% after introducing sentiment variables. And, meanwhile, our model helps investors make wiser decisions. These findings also imply that sentiment probably contains precious information about the asset fundamental values and can be regarded as one of the leading indicators of the stock market.","Rui Ren,Rui Ren,Desheng Dash Wu,Desheng Dash Wu,Tianxiang Liu",IEEE Systems Journal,2019,,,
720,Evaluation of Session-based Recommendation Algorithms,"Recommender systems help users find relevant items of interest, for example on e-commerce or media streaming sites. Most academic research is concerned with approaches that personalize the recommendations according to long-term user profiles. In many real-world applications, however, such long-term profiles often do not exist and recommendations therefore have to be made solely based on the observed behavior of a user during an ongoing session. Given the high practical relevance of the problem, an increased interest in this problem can be observed in recent years, leading to a number of proposals for session-based recommendation algorithms that typically aim to predict the user’s immediate next actions. In this work, we present the results of an in-depth performance comparison of a number of such algorithms, using a variety of datasets and evaluation measures. Our comparison includes the most recent approaches based on recurrent neural networks like gru4rec, factorized Markov model approaches such as fism or fossil, as well as simpler methods based, e.g., on nearest neighbor schemes. Our experiments reveal that algorithms of this latter class, despite their sometimes almost trivial nature, often perform equally well or significantly better than today’s more complex approaches based on deep neural networks. Our results therefore suggest that there is substantial room for improvement regarding the development of more sophisticated session-based recommendation algorithms.","Malte Ludewig,Dietmar Jannach",User Modeling and User-adapted Interaction,2018,,,
721,Ensemble Network Architecture for Deep Reinforcement Learning,"The popular deep  learning algorithm is known to be instability because of the -value’s shake and overestimation action values under certain conditions. These issues tend to adversely affect their performance. In this paper, we develop the ensemble network architecture for deep reinforcement learning which is based on value function approximation. The temporal ensemble stabilizes the training process by reducing the variance of target approximation error and the ensemble of target values reduces the overestimate and makes better performance by estimating more accurate -value. Our results show that this architecture leads to statistically significant better value evaluation and more stable and better performance on several classical control tasks at OpenAI Gym environment.","Xiliang Chen,Lei Cao,Chen-xi Li,Zhixiong Xu,Jun Lai",Mathematical Problems in Engineering,2018,,,
722,Deep Hedging,"We present a framework for hedging a portfolio of derivatives in the presence of market frictions such as transaction costs, market impact, liquidity constraints or risk limits using modern deep reinforcement machine learning methods.  We discuss how standard reinforcement learning methods can be applied to non-linear reward structures, i.e. in our case convex risk measures. As a general contribution to the use of deep learning for stochastic processes, we also show that the set of constrained trading strategies used by our algorithm is large enough to $\epsilon$-approximate any optimal solution.  Our algorithm can be implemented efficiently even in high-dimensional situations using modern machine learning tools. Its structure does not depend on specific market dynamics, and generalizes across hedging instruments including the use of liquid derivatives. Its computational performance is largely invariant in the size of the portfolio as it depends mainly on the number of hedging instruments available.  We illustrate our approach by showing the effect on hedging under transaction costs in a synthetic market driven by the Heston model, where we outperform the standard ""complete market"" solution.","Hans Bühler,Lukas Gonon,Josef Teichmann,Ben Wood",arXiv: Computational Finance,2018,,,
723,Latent Space Policies for Hierarchical Reinforcement Learning,"We address the problem of learning hierarchical deep neural network policies for reinforcement learning. In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective. Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of that layer. The maximum entropy objective causes these latent variables to be incorporated into the layer's policy, and the higher level layer can directly control the behavior of the lower layer through this latent space. Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the higher layers nor the lower layers are constrained in their behavior. Our experimental evaluation demonstrates that we can improve on the performance of single-layer policies on standard benchmark tasks simply by adding additional layers, and that our method can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.","Tuomas Haarnoja,Kristian Hartikainen,Pieter Abbeel,Sergey Levine",arXiv: Learning,2018,,,
724,Market Making via Reinforcement Learning,"Market making is a fundamental trading problem in which an agent provides liquidity by continually offering to buy and sell a security. The problem is challenging due to inventory risk, the risk of accumulating an unfavourable position and ultimately losing money. In this paper, we develop a high-fidelity simulation of limit order book markets, and use it to design a market making agent using temporal-difference reinforcement learning. We use a linear combination of tile codings as a value function approximator, and design a custom reward function that controls inventory risk. We demonstrate the effectiveness of our approach by showing that our agent outperforms both simple benchmark strategies and a recent online learning approach from the literature.","Thomas Spooner,John Fearnley,Rahul Savani,Andreas Koukorinis",arXiv: Artificial Intelligence,2018,,,
725,A Deep Learning Algorithm for Prediction of Age-Related Eye Disease Study Severity Scale for Age-Related Macular Degeneration from Color Fundus Photography,"Purpose  Age-related macular degeneration (AMD) is a common threat to vision. While classification of disease stages is critical to understanding disease risk and progression, several systems based on color fundus photographs are known. Most of these require in-depth and time-consuming analysis of fundus images. Herein, we present an automated computer-based classification algorithm.  Design  Algorithm development for AMD classification based on a large collection of color fundus images. Validation is performed on a cross-sectional, population-based study.  Participants  We included 120656 manually graded color fundus images from 3654 Age-Related Eye Disease Study (AREDS) participants. AREDS participants were >55 years of age, and non-AMD sight-threatening diseases were excluded at recruitment. In addition, performance of our algorithm was evaluated in 5555 fundus images from the population-based Kooperative Gesundheitsforschung in der Region Augsburg (KORA; Cooperative Health Research in the Region of Augsburg) study.  Methods  We defined 13 classes (9 AREDS steps, 3 late AMD stages, and 1 for ungradable images) and trained several convolution deep learning architectures. An ensemble of network architectures improved prediction accuracy. An independent dataset was used to evaluate the performance of our algorithm in a population-based study.  Main Outcome Measures  κ Statistics and accuracy to evaluate the concordance between predicted and expert human grader classification.  Results  A network ensemble of 6 different neural net architectures predicted the 13 classes in the AREDS test set with a quadratic weighted κ of 92% (95% confidence interval, 89%–92%) and an overall accuracy of 63.3%. In the independent KORA dataset, images wrongly classified as AMD were mainly the result of a macular reflex observed in young individuals. By restricting the KORA analysis to individuals >55 years of age and prior exclusion of other retinopathies, the weighted and unweighted κ increased to 50% and 63%, respectively. Importantly, the algorithm detected 84.2% of all fundus images with definite signs of early or late AMD. Overall, 94.3% of healthy fundus images were classified correctly.  Conclusions  Our deep learning algoritm revealed a weighted κ outperforming human graders in the AREDS study and is suitable to classify AMD fundus images in other datasets using individuals >55 years of age.","Felix Grassmann,Judith Mengelkamp,Caroline Brandl,Sebastian Harsch,Martina E. Zimmermann,Birgit Linkohr,Annette Peters,Iris M. Heid,Christoph Palm,Bernhard H. F. Weber",Ophthalmology,2018,,,
726,Adaptive Markets: Financial Evolution at the Speed of Thought,"A new, evolutionary explanation of markets and investor behavior

Half of all Americans have money in the stock market, yet economists can't agree on whether investors and markets are rational and efficient, as modern financial theory assumes, or irrational and inefficient, as behavioral economists believe—and as financial bubbles, crashes, and crises suggest. This is one of the biggest debates in economics and the value or futility of investment management and financial regulation hang on the outcome. In this groundbreaking book, Andrew Lo cuts through this debate with a new framework, the Adaptive Markets Hypothesis, in which rationality and irrationality coexist.

Drawing on psychology, evolutionary biology, neuroscience, artificial intelligence, and other fields, Adaptive Markets shows that the theory of market efficiency isn't wrong but merely incomplete. When markets are unstable, investors react instinctively, creating inefficiencies for others to exploit. Lo's new paradigm explains how financial evolution shapes behavior and markets at the speed of thought—a fact revealed by swings between stability and crisis, profit and loss, and innovation and regulation.

A fascinating intellectual journey filled with compelling stories, Adaptive Markets starts with the origins of market efficiency and its failures, turns to the foundations of investor behavior, and concludes with practical implications—including how hedge funds have become the Galapagos Islands of finance, what really happened in the 2008 meltdown, and how we might avoid future crises.

An ambitious new answer to fundamental questions in economics, Adaptive Markets is essential reading for anyone who wants to know how markets really work.",Andrew W. Lo,,2017,,,
727,FinSSLx: A Sentiment Analysis Model for the Financial Domain Using Text Simplification,"Financial decisions are increasingly being mediated by methods which rely on automated data analysis, a fact which is reflected in the scale and growth of investment in the Financial Technology sector. NLP progressively plays a more prominent role in the spectrum of technologies which affect the financial decision-making process, where Sentiment Analysis techniques are becoming more central in measuring the mood and perception of the market and supporting the analysis of financial events at scale. This paper presents FinSSLx, a sentiment-based prediction model for the financial domain which uses the combination of a clausal/phrasal sentence simplification step. A complex sentence is simplified into syntactically sound independent shorter sentences which are then classified according to the polarity and a distant supervision step for large-scale polarity lexical acquisition.","Macedo Maia,Andr Freitas,Siegfried Handschuh,Siegfried Handschuh",,2018,,,
728,Stock Movement Prediction from Tweets and Historical Prices,"Stock movement prediction is a challenging problem: the market is highly stochastic, and we make temporally-dependent predictions from chaotic data. We treat these three complexities and present a novel deep generative model jointly exploiting text and price signals for this task. Unlike the case with discriminative or topic modeling, our model introduces recurrent, continuous latent variables for a better treatment of stochasticity, and uses neural variational inference to address the intractable posterior inference. We also provide a hybrid objective with temporal auxiliary to flexibly capture predictive dependencies. We demonstrate the state-of-the-art performance of our proposed model on a new stock movement prediction dataset which we collected.","Yumo Xu,Yumo Xu,Shay B. Cohen",,2018,,,
729,Crowd Counting with Deep Negative Correlation Learning,"Deep convolutional networks (ConvNets) have achieved unprecedented performances on many computer vision tasks. However, their adaptations to crowd counting on single images are still in their infancy and suffer from severe over-fitting. Here we propose a new learning strategy to produce generalizable features by way of deep negative correlation learning (NCL). More specifically, we deeply learn a pool of decorrelated regressors with sound generalization capabilities through managing their intrinsic diversities. Our proposed method, named decorrelated ConvNet (D-ConvNet), is end-to-end-trainable and independent of the backbone fully-convolutional network architectures. Extensive experiments on very deep VGGNet as well as our customized network structure indicate the superiority of D-ConvNet when compared with several state-of-the-art methods. Our implementation will be released at https://github.com/shizenglin/Deep-NCL","Zenglin Shi,Le Zhang,Le Zhang,Yun Liu,Xiaofeng Cao,Xiaofeng Cao,Yangdong Ye,Ming-Ming Cheng,Guoyan Zheng",,2018,,,
730,Multiagent Soft Q-Learning,"Policy gradient methods are often applied to reinforcement learning in continuous multiagent games. These methods perform local search in the joint-action space, and as we show, they are susceptable to a game-theoretic pathology known as relative overgeneralization. To resolve this issue, we propose Multiagent Soft Q-learning, which can be seen as the analogue of applying Q-learning to continuous controls. We compare our method to MADDPG, a state-of-the-art approach, and show that our method achieves better coordination in multiagent cooperative tasks, converging to better local optima in the joint action space.","Ermo Wei,Drew Wicke,David Freelan,Sean Luke",arXiv: Artificial Intelligence,2018,,,
731,Incremental Matrix Co-factorization for Recommender Systems with Implicit Feedback,"Recommender systems try to predict which items a user will prefer. Traditional models for recommendation only take into account the user-item interaction, usually expressed by explicit ratings. However, in these days, web services continuously generate auxiliary data from users and items that can be incorporated into the recommendation model to improve recommendations. In this work, we propose an incremental Matrix Co-factorization model with implicit user feedback, considering a real-world data-stream scenario. This model can be seen as an extension of the conventional Matrix Factorization that includes additional dimensions to be decomposed in the common latent factor space. We test our proposal against a baseline algorithm that relies exclusively on interaction data, using prequential evaluation. Our experimental results show a significant improvement in the accuracy of recommendations, after incorporating an additional dimension in three music domain datasets.","Susan C. Anyosa,João Vinagre,Alípio Mário Jorge",,2018,,,
732,K-plet Recurrent Neural Networks for Sequential Recommendation,"Recurrent Neural Networks have been successful in learning meaningful representations from sequence data, such as text and speech. However, recurrent neural networks attempt to model only the overall structure of each sequence independently, which is unsuitable for recommendations. In recommendation system, an optimal model should not only capture the global structure, but also the localized relationships. This poses a great challenge in the application of recurrent neural networks to the sequence prediction problem. To tackle this challenge, we incorporate the neighbor sequences into recurrent neural networks to help detect local relationships. Thus we propose a K -plet R ecurrent Neural Network (Kr Network for short) to accommodate multiple sequences jointly, and then introduce two ways to model their interactions between sequences. Experimental results on benchmark datasets show that our proposed architecture Kr Network outperforms state-of-the-art baseline methods in terms of generalization, short-term and long term prediction accuracy.","Xiang Lin,Shuzi Niu,Yiqiao Wang,Yucheng Li,Yucheng Li",,2018,,,
733,The Power of Ensembles for Active Learning in Image Classification,"Deep learning methods have become the de-facto standard for challenging image processing tasks such as image classification. One major hurdle of deep learning approaches is that large sets of labeled data are necessary, which can be prohibitively costly to obtain, particularly in medical image diagnosis applications. Active learning techniques can alleviate this labeling effort. In this paper we investigate some recently proposed methods for active learning with high-dimensional data and convolutional neural network classifiers. We compare ensemble-based methods against Monte-Carlo Dropout and geometric approaches. We find that ensembles perform better and lead to more calibrated predictive uncertainties, which are the basis for many active learning algorithms. To investigate why Monte-Carlo Dropout uncertainties perform worse, we explore potential differences in isolation in a series of experiments. We show results for MNIST and CIFAR-10, on which we achieve a test set accuracy of 90% with roughly 12,200 labeled images, and initial results on ImageNet. Additionally, we show results on a large, highly class-imbalanced diabetic retinopathy dataset. We observe that the ensemble-based active learning effectively counteracts this imbalance during acquisition.","William H. Beluch,William H. Beluch,Tim Genewein,Andreas Nürnberger,Jan M. Köhler",,2018,,,
734,Deep Reinforcement Learning for Page-wise Recommendations,"Recommender systems can mitigate the information overload problem by suggesting users' personalized items. In real-world recommendations such as e-commerce, a typical interaction between the system and its users is - users are recommended a page of items and provide feedback; and then the system recommends a new page of items. To effectively capture such interaction for recommendations, we need to solve two key problems - (1) how to update recommending strategy according to user's real-time feedback, and 2) how to generate a page of items with proper display, which pose tremendous challenges to traditional recommender systems. In this paper, we study the problem of page-wise recommendations aiming to address aforementioned two challenges simultaneously. In particular, we propose a principled approach to jointly generate a set of complementary items and the corresponding strategy to display them in a 2-D page; and propose a novel page-wise recommendation framework based on deep reinforcement learning, DeepPage, which can optimize a page of items with proper display based on real-time feedback from users. The experimental results based on a real-world e-commerce dataset demonstrate the effectiveness of the proposed framework.","Xiangyu Zhao,Xiangyu Zhao,Xiangyu Zhao,Long Xia,Liang Zhang,Zhuoye Ding,Dawei Yin,Jiliang Tang",,2018,,,
735,"RETRACTED: Internet of Things (IoT) and its impact on supply chain: A framework for building smart, secure and efficient systems","Abstract   The traditional supply chains faces several challenges such as uncertainty, cost, complexity and vulnerable problems. To overcome these problems the supply chains must be more smarter. For establishing a large-scale of smart infrastructure to merge data, information, products, physical objects and all processes of supply chain, we applies the internet of things (IOT) in supply chain management (SCM) through building a smart and secure system of SCM. We have prepared a website for suppliers and managers. We tracked the flow of products at each stage in supply chain management through the Radio Frequency Identification (RFID) technology. Each product attached with RFID tag and scanned through RFID reader and ESP8266 at each phase of supply chain management. After scanning the tag we stores tag id in the database. All information about products will be entered by suppliers and then uploaded to managers. In our system the supplier and manager gets perfect information of the entire life cycle of goods, and this will achieve transparency of supply chain management. For assessing security criteria of proposed system of supply chain management, we also proposed a framework which integrates neutrosophic Decision Making Trial and Evaluation Laboratory (N-DEMATEL) technique with analytic hierarchy process (AHP). The neutrosophic Decision Making Trial and Evaluation Laboratory (N-DEMATEL) technique is utilized to infer cause and effect interrelationships among criteria of smart supply chain security requirements. Depending on obtained information from (N-DEMATEL) the neutrosophic AHP is utilized to calculate weight of criteria and sub-criteria. Then the integrated framework will help researchers and practitioners to design secure system of supply chains. We presented DEMATEL and AHP in neutrosophic environment to deal effectively with vague, uncertain and incomplete information. So the proposed system of supply chain management will be able to overcome all challenges of traditional SCM and provide secure environment of SCM processes.","Mohamed Abdel-Basset,Gunasekaran Manogaran,Mai Mohamed",Future Generation Computer Systems,2018,,,
736,Novel genetic-based negative correlation learning for estimating soil temperature,"ABSTRACTA genetic-based neural network ensemble (GNNE) is applied for estimation of daily soil temperatures (DST) at distinct depths. A sequential genetic-based negative correlation learning algorithm (SGNCL) is adopted to train the GNNE parameters. CLMS algorithm is used to achieve the optimum weights of components. Recorded data for two different stations located in Iran are used for the development of the GNNE models. Furthermore, the GNNE predictions are compared with the existing machine-learning models. The results demonstrate that GNNE outperforms other methods for the prediction of DSTs.","S. M. R. Kazemi,Seyed Mahmood Kazemi,Behrouz Minaei Bidgoli,Shahaboddin Shamshirband,Seyed Mehdi Karimi,Mohammad Ali Ghorbani,Kwok-wing Chau,Reza Kazem Pour",Engineering Applications of Computational Fluid Mechanics,2018,,,
737,Reinforcement learning in Portfolio Management and its interpretation,"Since all machine learning methods commonly in use today are viewed
as black boxes, the goal of this paper is to make one of these meth-
ods transparent in the context of Portfolio management. I interpret
the strategies implied by reinforcement learning (RL) and relate them
to the strategies implied by academic portfolio advice with the help of
their classical portfolio (CP) management models. Because RL is ac-
tually approximate dynamic programming (DP), it is perfectly suited
for the volatile DP environment of portfolio management compared to
other machine learning methods in use. In terms of performance, this
RL method is able to: 1) achieve the same average terminal wealth of
1.33, which is an increase of 33% in portfolio value over ve years, as
the CP model with a low risk-aversion 2) diminish the standard devia-
tion of the terminal wealth by 30% from 0.35 to 0.25 3) have a lower
turnover than the same CP model by three percent. This can mostly
be explained by the conservative investing of the reinforcement learning
method overall.",L.J.R. Weijs,,2018,,,
738,Stein Variational Gradient Descent Without Gradient,"Stein variational gradient decent (SVGD) has been shown to be a powerful approximate inference algorithm for complex distributions. However, the standard SVGD requires calculating the gradient of the target density and cannot be applied when the gradient is unavailable. In this work, we develop a gradient-free variant of SVGD (GF-SVGD), which replaces the true gradient with a surrogate gradient, and corrects the induced bias by re-weighting the gradients in a proper form. We show that our GF-SVGD can be viewed as the standard SVGD with a special choice of kernel, and hence directly inherits the theoretical properties of SVGD. We shed insights on the empirical choice of the surrogate gradient and propose an annealed GF-SVGD that leverages the idea of simulated annealing to improve the performance on high dimensional complex distributions. Empirical studies show that our method consistently outperforms a number of recent advanced gradient-free MCMC methods.","Jun Han,Qiang Liu,Qiang Liu",arXiv: Machine Learning,2018,,,
739,Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward,"We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the ""lazy agent'' problem, which arises due to partial observability. We address these problems by training individual agents with a novel value-decomposition network architecture, which learns to decompose the team value function into agent-wise value functions.","Peter Sunehag,Guy Lever,Guy Lever,Audrunas Gruslys,Wojciech Marian Czarnecki,Vinicius Zambaldi,Max Jaderberg,Marc Lanctot,Nicolas Sonnerat,Joel Z. Leibo,Karl Tuyls,Thore Graepel",,2018,,,
740,Content-Aware Hierarchical Point-of-Interest Embedding Model for Successive POI Recommendation,"Recommending a point-of-interest (POI) a user will visit next based on temporal and spatial context information is an important task in mobile-based applications. Recently, several POI recommendation models based on conventional sequential-data modeling approaches have been proposed. However, such models focus on only a user's check-in sequence information and the physical distance between POIs. Furthermore, they do not utilize the characteristics of POIs or the relationships between POIs. To address this problem, we propose CAPE, the first content-aware POI embedding model which utilizes text content that provides information about the characteristics of a POI. CAPE consists of a check-in context layer and a text content layer. The check-in context layer captures the geographical influence of POIs from the check-in sequence of a user, while the text content layer captures the characteristics of POIs from the text content. To validate the efficacy of CAPE, we constructed a large-scale POI dataset. In the experimental evaluation, we show that the performance of the existing POI recommendation models can be significantly improved by simply applying CAPE to the models.","Buru Chang,Yonggyu Park,Donghyeon Park,Seongsoon Kim,Jaewoo Kang",,2018,,,
741,Sequential recommender system based on hierarchical attention network,"With a large amount of user activity data accumulated, it is crucial to exploit user sequential behavior for sequential recommendations. Conventionally, user general taste and recent demand are combined to promote recommendation performances. However, existing methods often neglect that user long-term preference keep evolving over time, and building a static representation for user general taste may not adequately reflect the dynamic characters. Moreover, they integrate user-item or itemitem interactions through a linear way which limits the capability of model. To this end, in this paper, we propose a novel two-layer hierarchical attention network, which takes the above properties into account, to recommend the next item user might be interested. Specifically, the first attention layer learns user long-term preferences based on the historical purchased item representation, while the second one outputs final user representation through coupling user long-term and short-term preferences. The experimental study demonstrates the superiority of our method compared with other state-of-the-art ones.","Haochao Ying,Haochao Ying,Fuzhen Zhuang,Fuzheng Zhang,Yanchi Liu,Guandong Xu,Xing Xie,Hui Xiong,Jian Wu,Jian Wu,Jian Wu",,2018,,,
742,Learning from History and Present: Next-item Recommendation via Discriminatively Exploiting User Behaviors,"In the modern e-commerce, the behaviors of customers contain rich information, e.g., consumption habits, the dynamics of preferences. Recently, session-based recommendationsare becoming popular to explore the temporal characteristics of customers' interactive behaviors. However, existing works mainly exploit the short-term behaviors without fully taking the customers' long-term stable preferences and evolutions into account. In this paper, we propose a novel Behavior-Intensive Neural Network (BINN) for next-item recommendation by incorporating both users' historical stable preferences and present consumption motivations. Specifically, BINN contains two main components, i.e., Neural Item Embedding, and Discriminative Behaviors Learning. Firstly, a novel item embedding method based on user interactions is developed for obtaining an unified representation for each item. Then, with the embedded items and the interactive behaviors over item sequences, BINN discriminatively learns the historical preferences and present motivations of the target users. Thus, BINN could better perform recommendations of the next items for the target users. Finally, for evaluating the performances of BINN, we conduct extensive experiments on two real-world datasets, i.e., Tianchi and JD. The experimental results clearly demonstrate the effectiveness of BINN compared with several state-of-the-art methods.","Zhi Li,Hongke Zhao,Qi Liu,Qi Liu,Qi Liu,Qi Liu,Zhenya Huang,Tao Mei,Tao Mei,Tao Mei,Enhong Chen",,2018,,,
743,STAMP: Short-Term Attention/Memory Priority Model for Session-based Recommendation,"Predicting users' actions based on anonymous sessions is a challenging problem in web-based behavioral modeling research, mainly due to the uncertainty of user behavior and the limited information. Recent advances in recurrent neural networks have led to promising approaches to solving this problem, with long short-term memory model proving effective in capturing users' general interests from previous clicks. However, none of the existing approaches explicitly take the effects of users' current actions on their next moves into account. In this study, we argue that a long-term memory model may be insufficient for modeling long sessions that usually contain user interests drift caused by unintended clicks. A novel short-term attention/memory priority model is proposed as a remedy, which is capable of capturing users' general interests from the long-term memory of a session context, whilst taking into account users' current interests from the short-term memory of the last-clicks. The validity and efficacy of the proposed attention mechanism is extensively evaluated on three benchmark data sets from the RecSys Challenge 2015 and CIKM Cup 2016. The numerical results show that our model achieves state-of-the-art performance in all the tests.","Qiao Liu,Qiao Liu,Yifu Zeng,Refuoe Mokhosi,Refuoe Mokhosi,Haibin Zhang",,2018,,,
744,Vine Copula-Based Asymmetry and Tail Dependence Modeling,"Financial variables such as asset returns in the massive market contain various hierarchical and horizontal relationships that form complicated dependence structures. Modeling these structures is challenging due to the stylized facts of market data. Many research works in recent decades showed that copula is an effective method to describe relations among variables. Vine structures were introduced to represent the decomposition of multivariate copula functions. However, the model construction of vine structures is still a tough problem owing to the geometrical data, conditional independent assumptions and the stylized facts. In this paper, we introduce a new bottom-to-up method to construct regular vine structures and applies the model to 12 currencies over 16 years as a case study to analyze the asymmetric and fat tail features. The out-of-sample performance of our model is evaluated by Value at Risk, a widely used industrial benchmark. The experimental results show that our model and its intrinsic design significantly outperform industry baselines, and provide financially interpretable knowledge and profound insights into the dependence structures of multi-variables with complex dependencies and characteristics.","Jia Xu,Longbing Cao",,2018,,,
745,Investor-Imitator: A Framework for Trading Knowledge Extraction,"Stock trading is a popular investment approach in real world. However, since lacking enough domain knowledge and experience, it is very difficult for common investors to analyze thousands of stocks manually. Algorithmic investment provides another rational way to formulate human knowledge as a trading agent. However, it still requires well-built knowledge and experience to design effective trading algorithms in such a volatile market. Fortunately, various kinds of historical trading records are easy to obtain in this big-data era, it is invaluable of us to extract the trading knowledge hidden in the data to help people make better decisions. In this paper, we propose a reinforcement learning driven Investor-Imitator framework to formalize the trading knowledge, by imitating an investor's behavior with a set of logic descriptors. In particular, to instantiate specific logic descriptors, we introduce the Rank-Invest model that can keep the diversity of logic descriptors by learning to optimize different evaluation metrics. In the experiment, we first simulate three types of investors, representing different degrees of information disclosure we may meet in real market. By learning towards these investors, we can tell the inherent trading logic of the target investor with the Investor-Imitator empirically, and the extracted interpretable knowledge can help us better understand and construct trading portfolios. Experimental results in this paper sufficiently demonstrate the designed purpose of Investor-Imitator, it makes the Investor-Imitator an applicable and meaningful intelligent trading framework in financial investment research.","Yi Ding,Weiqing Liu,Jiang Bian,Daoqiang Zhang,Tie-Yan Liu",,2018,,,
746,Stock Trading Bot Using Deep Reinforcement Learning,"This paper proposes automating swing trading using deep reinforcement learning. The deep deterministic policy gradient-based neural network model trains to choose an action to sell, buy, or hold the stocks to maximize the gain in asset value. The paper also acknowledges the need for a system that predicts the trend in stock value to work along with the reinforcement learning algorithm. We implement a sentiment analysis model using a recurrent convolutional neural network to predict the stock trend from the financial news. The objective of this paper is not to build a better trading bot, but to prove that reinforcement learning is capable of learning the tricks of stock trading.","Akhil Raj Azhikodan,Anvitha G. K. Bhat,Mamatha V. Jadhav,Mamatha V. Jadhav",,2019,,,
747,Co-explosivity in the cryptocurrency market,"Abstract   Most of the limited evidence on the exponential price spikes (i.e. price explosivity) in the cryptocurrency market mainly considers the case of Bitcoin, although other cryptocurrencies have gradually eroded Bitcoin's dominance. Importantly, none has been documented as to whether explosivity periods in cryptocurrencies are contemporaneously related. Accordingly, we date-stamp price explosivity in leading cryptocurrencies and reveal that all cryptocurrencies investigated herein were characterised by multiple explosivity. Then, we determine whether explosivity in one cryptocurrency can lead to explosivity in other cryptocurrencies. Results show evidence of a multidirectional co-explosivity behaviour that is not necessarily from bigger to smaller and younger markets.","Elie Bouri,Syed Jawad Hussain Shahzad,David Roubaud",Finance Research Letters,2019,,,
748,Predicting the direction of stock market prices using tree-based classifiers,"Predicting returns in the stock market is usually posed as a forecasting problem where prices are predicted. Intrinsic volatility in the stock market across the globe makes the task of prediction challenging. Consequently, forecasting and diffusion modeling undermines a diverse range of problems encountered in predicting trends in the stock market. Minimizing forecasting error would minimize investment risk. In the current work, we pose the problem as a direction-predicting exercise signifying gains and losses. We develop an experimental framework for the classification problem which predicts whether stock prices will increase or decrease with respect to the price prevailing n days earlier. Two algorithms, random forests, and gradient boosted decisio‘n trees (using XGBoost) facilitate this connection by using ensembles of decision trees. We test our approach and report the accuracies for a variety of companies as improvement over existing predictions. A novelty of the current work is about the selection of technical indicators and their use as features, with high accuracy for medium to long-run prediction of stock price direction.","Suryoday Basak,Suryoday Basak,Saibal Kar,Snehanshu Saha,Luckyson Khaidem,Sudeepa Roy Dey",The North American Journal of Economics and Finance,2019,,,
749,Automated trading systems statistical and machine learning methods and hardware implementation: a survey,"Automated trading, which is also known as algorithmic trading, is a method of using a predesigned computer program to submit a large number of trading orders to an exchange. It is substantially a r ...","Boming Huang,Yuxiang Huan,Li Da Xu,Li-Rong Zheng,Lirong Zheng,Zhuo Zou",Enterprise Information Systems,2019,,,
750,A Deep Joint Network for Session-based News Recommendations with Contextual Augmentation,"Session-based recommendations have drawn more and more attention in many recommendation settings of modern online services. Unlike many other domains such as books and music, news recommendations suffer from new challenges of fast updating rate and recency issues of news articles and lack of user profiles. In this paper, we proposed a method that combines user click events within session and news contextual features to predict next click behavior of a user. The model consists of two different kinds of hierarchical neutral networks to learn article contextual properties and temporal sequential patterns in streams of clicks. Character-level embedding over input features is adopted to allow integrating different types of data and reduce engineering computation. Besides, we also introduced a time-decay method to compute the freshness of news articles within a time slide. Experimental results on two real-world datasets show significant improvements over several baselines and state-of-the-art methods on session-based neural networks.","Lemei Zhang,Peng Liu,Jon Atle Gulla",,2018,,,
751,Sequence-Aware Recommender Systems,"Recommender systems are one of the most successful applications of data mining and machine-learning technology in practice. Academic research in the field is historically often based on the matrix completion problem formulation, where for each user-item-pair only one interaction (e.g., a rating) is considered. In many application domains, however, multiple user-item interactions of different types can be recorded over time. And, a number of recent works have shown that this information can be used to build richer individual user models and to discover additional behavioral patterns that can be leveraged in the recommendation process.

In this work, we review existing works that consider information from such sequentially ordered user-item interaction logs in the recommendation process. Based on this review, we propose a categorization of the corresponding recommendation tasks and goals, summarize existing algorithmic solutions, discuss methodological approaches when benchmarking what we call sequence-aware recommender systems, and outline open challenges in the area.","Massimo Quadrana,Paolo Cremonesi,Dietmar Jannach",ACM Computing Surveys,2018,,,
752,Performance of the Average Directional Index as a market timing tool for the most actively traded USD based currency pairs,"The aim of this study is to test a trading system based on the average directional index, which is complemented with the parabolic stop and reverse indicator. The trend-based system is tested onto the most actively traded USD based foreign currency pairs, using both monthly and weekly data set over 2000–2018. Sharpe and Sortino measures are used to track the performance of the currency pairs, based on total risk and downside risk assumptions. Results are robust tested by decomposing the data into pre and post 2008 financial crisis. Using an investment horizon over 18 years, the reliance upon the monthly model produced lower maximum drawdowns and lesser trades than the weekly model. While Swiss Franc had the best (worse) performance in the monthly (weekly) based model, the Chinese Renminbi witnessed the worse (best) performance in the monthly (weekly) based model. Pre and post financial crisis decompositions suggest the weekly-based system is more reliable than the monthly one with relatively more trades and positive performance, where the Chinese Renminbi and Japanese Yen posted the highest Sharpe and Sortino values of 0.996 and 4.452 respectively in the post crisis period. Proportionately high level of negative returns coupled with relatively low positive Sharpe and Sortino values, however, suggest that a trading system relying on the average directional index and parabolic stop and reverse indicator to be further tested and analyzed at higher frequencies.",Ikhlaas Gurrib,Banks and Bank Systems,2018,,,
753,"Data Science Thinking: The Next Scientific, Technological and Economic Revolution",,"Longbing Cao,Longbing Cao",,2018,,,
754,BE-DTI’: Ensemble framework for drug target interaction prediction using dimensionality reduction and active learning,"Abstract   Background and objective  Drug-target interaction prediction plays an intrinsic role in the drug discovery process. Prediction of novel drugs and targets helps in identifying optimal drug therapies for various stringent diseases. Computational prediction of drug-target interactions can help to identify potential drug-target pairs and speed-up the process of drug repositioning. In our present, work we have focused on machine learning algorithms for predicting drug-target interactions from the pool of existing drug-target data. The key idea is to train the classifier using existing DTI so as to predict new or unknown DTI. However, there are various challenges such as class imbalance and high dimensional nature of data that need to be addressed before developing optimal drug-target interaction model.    Methods  In this paper, we propose a bagging based ensemble framework named BE-DTI’ for drug-target interaction prediction using dimensionality reduction and active learning to deal with class-imbalanced data. Active learning helps to improve under-sampling bagging based ensembles. Dimensionality reduction is used to deal with high dimensional data.    Results  Results show that the proposed technique outperforms the other five competing methods in 10-fold cross-validation experiments in terms of AUC=0.927, Sensitivity=0.886, Specificity=0.864, and G-mean=0.874.    Conclusion  Missing interactions and new interactions are predicted using the proposed framework. Some of the known interactions are removed from the original dataset and their interactions are recalculated to check the accuracy of the proposed framework. Moreover, validation of the proposed approach is performed using the external dataset. All these results show that structurally similar drugs tend to interact with similar targets.","Aman Sharma,Aman Sharma,Rinkle Rani",Computer Methods and Programs in Biomedicine,2018,,,
755,Keynote: Session-based Recommendation - Challenges and Recent Advances,"In many applications of recommender systems, the system’s suggestions cannot be based on individual long-term preference profiles, because a large fraction of the user population are either first-time users or returning users who are not logged in when they use the service. Instead, the recommendations have to be determined based on the observed short-term behavior of the users during an ongoing session. Due to the high practical relevance of such session-based recommendation scenarios, different proposals were made in recent years to deal with the particular challenges of the problem setting.",Dietmar Jannach,,2018,,,
756,Benchmarking Reinforcement Learning Algorithms on Real-World Robots,"Through many recent successes in simulation, model-free reinforcement learning has emerged as a promising approach to solving continuous control robotic tasks. The research community is now able to reproduce, analyze and build quickly on these results due to open source implementations of learning algorithms and simulated benchmark tasks. To carry forward these successes to real-world applications, it is crucial to withhold utilizing the unique advantages of simulations that do not transfer to the real world and experiment directly with physical robots. However, reinforcement learning research with physical robots faces substantial resistance due to the lack of benchmark tasks and supporting source code. In this work, we introduce several reinforcement learning tasks with multiple commercially available robots that present varying levels of learning difficulty, setup, and repeatability. On these tasks, we test the learning performance of off-the-shelf implementations of four reinforcement learning algorithms and analyze sensitivity to their hyper-parameters to determine their readiness for applications in various real-world tasks. Our results show that with a careful setup of the task interface and computations, some of these implementations can be readily applicable to physical robots. We find that state-of-the-art learning algorithms are highly sensitive to their hyper-parameters and their relative ordering does not transfer across tasks, indicating the necessity of re-tuning them for each task for best performance. On the other hand, the best hyper-parameter configuration from one task may often result in effective learning on held-out tasks even with different robots, providing a reasonable default. We make the benchmark tasks publicly available to enhance reproducibility in real-world reinforcement learning.","A. Rupam Mahmood,Dmytro Korenkevych,Gautham Vasan,Will Ma,William Ma,James Bergstra",,2018,,,
757,Deep Boosting for Image Denoising,"Boosting is a classic algorithm which has been successfully applied to diverse computer vision tasks. In the scenario of image denoising, however, the existing boosting algorithms are surpassed by the emerging learning-based models. In this paper, we propose a novel deep boosting framework (DBF) for denoising, which integrates several convolutional networks in a feed-forward fashion. Along with the integrated networks, however, the depth of the boosting framework is substantially increased, which brings difficulty to training. To solve this problem, we introduce the concept of dense connection that overcomes the vanishing of gradients during training. Furthermore, we propose a path-widening fusion scheme cooperated with the dilated convolution to derive a lightweight yet efficient convolutional network as the boosting unit, named Dilated Dense Fusion Network (DDFN). Comprehensive experiments demonstrate that our DBF outperforms existing methods on widely used benchmarks, in terms of different denoising tasks.","Chang Chen,Zhiwei Xiong,Xinmei Tian,Feng Wu",,2018,,,
758,CEM-RL: Combining evolutionary and gradient-based methods for policy search,"Deep neuroevolution and deep reinforcement learning (deep RL) algorithms are two popular approaches to policy search. The former is widely applicable and rather stable, but suffers from low sample efficiency. By contrast, the latter is more sample efficient, but the most sample efficient variants are also rather unstable and highly sensitive to hyper-parameter setting. So far, these families of methods have mostly been compared as competing tools. However, an emerging approach consists in combining them so as to get the best of both worlds. Two previously existing combinations use either an ad hoc evolutionary algorithm or a goal exploration process together with the Deep Deterministic Policy Gradient (DDPG) algorithm, a sample efficient off-policy deep RL algorithm. In this paper, we propose a different combination scheme using the simple cross-entropy method (CEM) and Twin Delayed Deep Deterministic policy gradient (td3), another off-policy deep RL algorithm which improves over ddpg. We evaluate the resulting method, cem-rl, on a set of benchmarks classically used in deep RL. We show that cem-rl benefits from several advantages over its competitors and offers a satisfactory trade-off between performance and sample efficiency.","Aloïs Pourchot,Olivier Sigaud",arXiv: Learning,2018,,,
759,AI and Blockchain: A Disruptive Integration,"AI and blockchain are among the most disruptive technologies and will fundamentally reshape how we live, work, and interact. The authors summarize existing efforts and discuss the promising future of their integration, seeking to answer the question: What can smart, decentralized, and secure systems do for our society?","Thang N. Dinh,My T. Thai",IEEE Computer,2018,,,
760,Deep Probabilistic Video Compression,"We propose a variational inference approach to deep probabilistic video compression. Our model uses advances in variational autoencoders (VAEs) for sequential data and combines it with recent work on neural image compression. The approach jointly learns to transform the original video into a lower-dimensional representation as well as to entropy code this representation according to a temporally-conditioned probabilistic model. We split the latent space into local (per frame) and global (per segment) variables, and show that training the VAE to utilize both representations leads to an improved rate-distortion performance. Evaluation on small videos from public data sets with varying complexity and diversity show that our model yields competitive results when trained on generic video content. Extreme compression performance is achieved for videos with specialized content if the model is trained on similar videos.","Jun Han,Salvator Lombardo,Salvator Lombardo,Christopher Schroers,Christopher Schroers,Stephan Mandt",arXiv: Computer Vision and Pattern Recognition,2018,,,
761,PPO-CMA: Proximal Policy Optimization with Covariance Matrix Adaptation,"Proximal Policy Optimization (PPO) is a highly popular model-free reinforcement learning (RL) approach. However, we observe that in a continuous action space, PPO can prematurely shrink the exploration variance, which leads to slow progress and may make the algorithm prone to getting stuck in local optima. Drawing inspiration from CMA-ES, a black-box evolutionary optimization method designed for robustness in similar situations, we propose PPO-CMA, a proximal policy optimization approach that adaptively expands the exploration variance to speed up progress. This can be considered as a form of action-space momentum. With only minor changes to PPO, our algorithm considerably improves performance in Roboschool continuous control benchmarks.","Perttu Hämäläinen,Amin Babadi,Xiaoxiao Ma,Jaakko Lehtinen",arXiv: Learning,2018,,,
762,VIVID: Virtual Environment for Visual Deep Learning,"Due to the advances in deep reinforcement learning and the demand of large training data, virtual-to-real learning has gained lots of attention from computer vision community recently. As state-of-the-art 3D engines can generate photo-realistic images suitable for training deep neural networks, researchers have been gradually applied 3D virtual environment to learn different tasks including autonomous driving, collision avoidance, and image segmentation, to name a few. Although there are already many open-source simulation environments readily available, most of them either provide small scenes or have limited interactions with objects in the environment. To facilitate visual recognition learning, we present a new Virtual Environment for Visual Deep Learning (VIVID), which offers large-scale diversified indoor and outdoor scenes. Moreover, VIVID leverages the advanced human skeleton system, which enables us to simulate numerous complex human actions. VIVID has a wide range of applications and can be used for learning indoor navigation, action recognition, event detection, etc. We also release several deep learning examples in Python to demonstrate the capabilities and advantages of our system.","Kuan-Ting Lai,Chia-Chih Lin,Chun-Yao Kang,Chun-Yao Kang,Mei-Enn Liao,Ming-Syan Chen",,2018,,,
763,Incorporating Corporation Relationship via Graph Convolutional Neural Networks for Stock Price Prediction,"In this paper, we propose to incorporate information of related corporations of a target company for its stock price prediction. We first construct a graph including all involved corporations based on investment facts from real market and learn a distributed representation for each corporation via node embedding methods applied on the graph. Two approaches are then explored to utilize information of related corporations based on a pipeline model and a joint model via graph convolutional neural networks respectively. Experiments on the data collected from stock market in Mainland China show that the representation learned from our model is able to capture relationships between corporations, and prediction models incorporating related corporations' information are able to make more accurate predictions on stock market.","Yingmei Chen,Zhongyu Wei,Xuanjing Huang",,2018,,,
764,Deep Learning- and Word Embedding-Based Heterogeneous Classifier Ensembles for Text Classification,"The use of ensemble learning, deep learning, and effective document representation methods is currently some of the most common trends to improve the overall accuracy of a text classification/categorization system. Ensemble learning is an approach to raise the overall accuracy of a classification system by utilizing multiple classifiers. Deep learning-based methods provide better results in many applications when compared with the other conventional machine learning algorithms. Word embeddings enable representation of words learned from a corpus as vectors that provide a mapping of words with similar meaning to have similar representation. In this study, we use different document representations with the benefit of word embeddings and an ensemble of base classifiers for text classification. The ensemble of base classifiers includes traditional machine learning algorithms such as naive Bayes, support vector machine, and random forest and a deep learning-based conventional network classifier. We analysed the classification accuracy of different document representations by employing an ensemble of classifiers on eight different datasets. Experimental results demonstrate that the usage of heterogeneous ensembles together with deep learning methods and word embeddings enhances the classification performance of texts.","Zeynep Hilal Kilimci,Selim Akyokus",Complexity,2018,,,
765,Parametrized Deep Q-Networks Learning: Reinforcement Learning with Discrete-Continuous Hybrid Action Space,"Most existing deep reinforcement learning (DRL) frameworks consider either discrete action space or continuous action space solely. Motivated by applications in computer games, we consider the scenario with discrete-continuous hybrid action space. To handle hybrid action space, previous works either approximate the hybrid space by discretization, or relax it into a continuous set. In this paper, we propose a parametrized deep Q-network (P- DQN) framework for the hybrid action space without approximation or relaxation. Our algorithm combines the spirits of both DQN (dealing with discrete action space) and DDPG (dealing with continuous action space) by seamlessly integrating them. Empirical results on a simulation example, scoring a goal in simulated RoboCup soccer and the solo mode in game King of Glory (KOG) validate the efficiency and effectiveness of our method.","Jiechao Xiong,Qing Wang,Qing Wang,Qing Wang,Zhuoran Yang,Peng Sun,Peng Sun,Peng Sun,Lei Han,Yang Zheng,Haobo Fu,Haobo Fu,Tong Zhang,Ji Liu,Ji Liu,Ji Liu,Han Liu,Han Liu",arXiv: Learning,2018,,,
766,A study on novel filtering and relationship between input-features and target-vectors in a deep learning model for stock price prediction,"From past to present, the prediction of stock price in stock market has been a knotty problem. Many researchers have made various attempts and studies to predict stock prices. The prediction of stock price in stock market has been of concern to researchers in many disciplines, including economics, mathematics, physics, and computer science. This study intends to learn fluctuation of stock prices in stock market by using recently spotlighted techniques of deep learning to predict future stock price. In previous studies, we have used price-based input-features to measure performance changes in deep learning models. Results of this studies have revealed that the performance of stock price models would change according to varied input-features configured based on stock price. Therefore, we have concluded that more novel input-feature in deep learning model is needed to predict patterns of stock price fluctuation more precisely. In this paper, for predicting stock price fluctuation, we design deep learning model using 715 novel input-features configured on the basis of technical analyses. The performance of the prediction model was then compared to another model that employed simple price-based input-features. Also, rather than taking randomly collected set of stocks, stocks of a similar pattern of price fluctuation were filtered to identify the influence of filtering technique on the deep learning model. Finally, we compared and analyzed the performances of several models using different configuration of input-features and target-vectors.","Yoojeong Song,Jae Won Lee,Jongwoo Lee",Applied Intelligence,2019,,,
767,Variational Recurrent Model for Session-based Recommendation,"Session-based recommendation performance has been significantly improved by Recurrent Neural Networks (RNN). However, existing RNN-based models do not expose the global knowledge of frequent click patterns or consider variability of sequential behaviors in sessions. In this paper, we propose a novel Variational Recurrent Model (VRM), which employs the stochastic latent variable to capture the knowledge of frequent click patterns and impose variability for the sequential behavior modeling. A stochastic generative process of session sequence is specified, where the latent variable modulates the generation of session sequences in RNN. We further extend VRM to a Conditional Variational Recurrent Model (CVRM) by considering additional information (e.g., focused category in sessions) as the generative condition. When evaluated on a public benchmark dataset, VRM and its extension clearly demonstrate their superiority over popular baselines and state-of-the-art models.","Zhitao Wang,Chengyao Chen,Ke Zhang,Yu Lei,Wenjie Li",,2018,,,
768,Evolution-Guided Policy Gradient in Reinforcement Learning,"Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer from high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA's ability of temporal credit assignment with a fitness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments in a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DRL and EA methods.","Shauharda Khadka,Kagan Tumer",,2018,,,
769,Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform,"In this paper we present Horizon, Facebook's open source applied reinforcement learning (RL) platform. Horizon is an end-to-end platform designed to solve industry applied RL problems where datasets are large (millions to billions of observations), the feedback loop is slow (vs. a simulator), and experiments must be done with care because they don't run in a simulator. Unlike other RL platforms, which are often designed for fast prototyping and experimentation, Horizon is designed with production use cases as top of mind. The platform contains workflows to train popular deep RL algorithms and includes data preprocessing, feature transformation, distributed training, counterfactual policy evaluation, optimized serving, and a model-based data understanding tool. We also showcase and describe real examples where reinforcement learning models trained with Horizon significantly outperformed and replaced supervised learning systems at Facebook.","Jason Gauci,Edoardo Conti,Edoardo Conti,Yitao Liang,Kittipat Virochsiri,Kittipat Virochsiri,Yuchen He,Yuchen He,Zachary Kaden,Vivek Narayanan,Xiaohui Ye,Zhengxing Chen",arXiv: Learning,2018,,,
770,Deep Reinforcement Learning in High Frequency Trading.,"Financial trading is at the forefront of time-series analysis, and has grown hand-in-hand with it. The advent of electronic trading has allowed complex machine learning solutions to enter the field of financial trading. Financial markets have both long term and short term signals and thus a good predictive model in financial trading should be able to incorporate them together. One of the most sought after forms of electronic trading is high-frequency trading (HFT), typically known for microsecond sensitive changes, which results in a tremendous amount of data. LSTMs are one of the most capable variants of the RNN family that can handle long-term dependencies, but even they are not equipped to handle such long sequences of the order of thousands of data points like in HFT. We propose very-long short term memory networks, or VLSTMs, to deal with such extreme length sequences. We explore the importance of VLSTMs in the context of HFT. We compare our model on publicly available dataset and got a 3.14\% increase in F1-score over the existing state-of-the-art time-series forecasting models. We also show that our model has great parallelization potential, which is essential for practical purposes when trading on such markets.","Prakhar Ganesh,Puneet Rakheja",,2018,,,
771,Exploration by Random Network Distillation.,"We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.","Yuri Burda,Harrison Edwards,Amos Storkey,Oleg Klimov",arXiv: Learning,2018,,,
772,Global optimization in machine learning: the design of a predictive analytics application,"Global optimization, especially Bayesian optimization, has become the tool of choice in hyperparameter tuning and algorithmic configuration to optimize the generalization capability of machine learning algorithms. The contribution of this paper was to extend this approach to a complex algorithmic pipeline for predictive analytics, based on time-series clustering and artificial neural networks. The software environment R has been used with mlrMBO, a comprehensive and flexible toolbox for sequential model-based optimization. Random forest has been adopted as surrogate model, due to the nature of decision variables (i.e., conditional and discrete hyperparameters) of the case studies considered. Two acquisition functions have been considered: Expected improvement and lower confidence bound, and results are compared. The computational results, on a benchmark and a real-world dataset, show that even in a complex search space, up to 80 dimensions related to integer, categorical, and conditional variables (i.e., hyperparameters), sequential model-based optimization is an effective solution, with lower confidence bound requiring a lower number of function evaluations than expected improvement to find the same optimal solution.","Antonio Candelieri,Francesco Archetti",,2019,,,
773,Deep learning-based feature engineering for stock price movement prediction,"Abstract   Stock price modeling and prediction have been challenging objectives for researchers and speculators because of noisy and non-stationary characteristics of samples. With the growth in deep learning, the task of feature learning can be performed more effectively by purposely designed network. In this paper, we propose a novel end-to-end model named multi-filters neural network (MFNN) specifically for feature extraction on financial time series samples and price movement prediction task. Both convolutional and recurrent neurons are integrated to build the multi-filters structure, so that the information from different feature spaces and market views can be obtained. We apply our MFNN for extreme market prediction and signal-based trading simulation tasks on Chinese stock market index CSI 300. Experimental results show that our network outperforms traditional machine learning models, statistical models, and single-structure(convolutional, recurrent, and LSTM) networks in terms of the accuracy, profitability, and stability.","Wen Long,Zhichen Lu,Lingxiao Cui",Knowledge Based Systems,2019,,,
774,BDLOB: Bayesian Deep Convolutional Neural Networks for Limit Order Books,"We showcase how dropout variational inference can be applied to a large-scale deep learning model that predicts price movements from limit order books (LOBs), the canonical data source representing trading and pricing movements. We demonstrate that uncertainty information derived from posterior predictive distributions can be utilised for position sizing, avoiding unnecessary trades and improving profits. Further, we test our models by using millions of observations across several instruments and markets from the London Stock Exchange. Our results suggest that those Bayesian techniques not only deliver uncertainty information that can be used for trading but also improve predictive performance as stochastic regularisers. To the best of our knowledge, we are the first to apply Bayesian networks to LOBs.","Zihao Zhang,Zihao Zhang,Zihao Zhang,Stefan Zohren,Stephen J. Roberts",arXiv: Computational Finance,2018,,,
775,Stacking-Based Deep Neural Network: Deep Analytic Network for Pattern Classification,"Stacking-based deep neural network (S-DNN) is aggregated with pluralities of basic learning modules, one after another, to synthesize a deep neural network (DNN) alternative for pattern classification. Contrary to the DNNs trained from end to end by backpropagation (BP), each S-DNN layer, that is, a self-learnable module, is to be trained decisively and independently without BP intervention. In this paper, a ridge regression-based S-DNN, dubbed deep analytic network (DAN), along with its kernelization (K-DAN), are devised for multilayer feature relearning from the pre-extracted baseline features and the structured features. Our theoretical formulation demonstrates that DAN/K-DAN relearn by perturbing the intra/interclass variations, apart from diminishing the prediction errors. We scrutinize the DAN/K-DAN performance for pattern classification on datasets of varying domains--faces, handwritten digits, generic objects, to name a few. Unlike the typical BP-optimized DNNs to be trained from gigantic datasets by GPU, we reveal that DAN/K-DAN are trainable using only CPU even for small-scale training sets. Our experimental results show that DAN/K-DAN outperform the present S-DNNs and also the BP-trained DNNs, including multiplayer perceptron, deep belief network, etc., without data augmentation applied.","Cheng-Yaw Low,Jaewoo Park,Jaewoo Park,Jaewoo Park,Andrew Beng Jin Teoh","IEEE Transactions on Systems, Man, and Cybernetics",2019,,,
776,Online Active Learning Paired Ensemble for Concept Drift and Class Imbalance,"Practical applications often require learning algorithms capable of addressing data streams with concept drift and class imbalance. This paper proposes an online active learning paired ensemble for drifting streams with class imbalance. The paired ensemble consists of a long-term stable classifier and a dynamic classifier to address both sudden concept drift and gradual concept drift. To select the most representative instances for learning, a hybrid labeling strategy which includes an uncertainty strategy and an imbalance strategy is proposed. The uncertainty strategy applies a margin-based uncertainty criterion and a dynamic adjustment threshold. Based on the categorical distribution of the last data block, the imbalance strategy prefers to learn instances of the minority category. In addition, it also incorporates the advantages of the traditional random strategy and helps to capture the drifts away from the decision boundary. Experiments on real datasets and synthetic datasets utilize prequential AUC as an evaluation index, comparing the classification performance of our method with semi-supervised and supervised learning methods. The results show that the proposed methods can obtain higher AUC values at an even lower labeling cost. Moreover, it is noteworthy that the labeling cost can be dynamically allocated according to the concept drift and imbalance ratio.","Hang Zhang,Weike Liu,Jicheng Shan,Qingbao Liu",IEEE Access,2018,,,
777,A Simple Convolutional Generative Network for Next Item Recommendation,"Convolutional Neural Networks (CNNs) have been recently introduced in the domain of session-based next item recommendation. An ordered collection of past items the user has interacted with in a session (or sequence) are embedded into a 2-dimensional latent matrix, and treated as an image. The convolution and pooling operations are then applied to the mapped item embeddings. In this paper, we first examine the typical session-based CNN recommender and show that both the generative model and network architecture are suboptimal when modeling long-range dependencies in the item sequence. To address the issues, we introduce a simple, but very effective generative model that is capable of learning high-level representation from both short- and long-range item dependencies. The network architecture of the proposed model is formed of a stack of holed convolutional layers, which can efficiently increase the receptive fields without relying on the pooling operation. Another contribution is the effective use of residual block structure in recommender systems, which can ease the optimization for much deeper networks. The proposed generative model attains state-of-the-art accuracy with less training time in the next item recommendation task. It accordingly can be used as a powerful recommendation baseline to beat in future, especially when there are long sequences of user feedback.","Fajie Yuan,Alexandros Karatzoglou,Ioannis Arapakis,Joemon M. Jose,Xiangnan He",,2019,,,
778,"A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.","The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.","David Silver,Thomas Hubert,Julian Schrittwieser,Ioannis Antonoglou,Matthew Lai,Arthur Guez,Marc Lanctot,Laurent Sifre,Dharshan Kumaran,Thore Graepel,Timothy Lillicrap,Karen Simonyan,Demis Hassabis",Science,2018,,,
779,Genetic Algorithms,,"Goldberg,David E. Goldberg,William Shakespeare",,2008,,,
780,Double Deep Q-Learning for Optimal Execution.,"Optimal trade execution is an important problem faced by essentially all traders. Much research into optimal execution uses stringent model assumptions and applies continuous time stochastic control to solve them. Here, we instead take a model free approach and develop a variation of Deep Q-Learning to estimate the optimal actions of a trader. The model is a fully connected Neural Network trained using Experience Replay and Double DQN with input features given by the current state of the limit order book, other trading signals, and available execution actions, while the output is the Q-value function estimating the future rewards under an arbitrary action. We apply our model to nine different stocks and find that it outperforms the standard benchmark approach on most stocks using the measures of (i) mean and median out-performance, (ii) probability of out-performance, and (iii) gain-loss ratios.","Brian Ning,Franco Ho Ting Ling,Sebastian Jaimungal",arXiv: Trading and Market Microstructure,2018,,,
781,Guidelines for reinforcement learning in healthcare,"In this Comment, we provide guidelines for reinforcement learning for decisions about patient treatment that we hope will accelerate the rate at which observational cohorts can inform healthcare practice in a safe, risk-conscious manner.","Omer Gottesman,Fredrik D. Johansson,Matthieu Komorowski,A. Aldo Faisal,David Sontag,Finale Doshi-Velez,Leo Anthony Celi",Nature Medicine,2019,,,
782,A novel cryptocurrency price trend forecasting model based on LightGBM,"Abstract   Forecasting cryptocurrency prices is crucial for investors. In this paper, we adopt a novel Gradient Boosting Decision Tree (GBDT) algorithm, Light Gradient Boosting Machine (LightGBM), to forecast the price trend (falling, or not falling) of cryptocurrency market. In order to utilize market information, we combine the daily data of 42 kinds of primary cryptocurrencies with key economic indicators. Results show that the robustness of the LightGBM model is better than the other methods, and the comprehensive strength of the cryptocurrencies impacts the forecasting performance. This can effectively guide investors in constructing an appropriate cryptocurrency portfolio and mitigaterisks.","Xiaolei Sun,Mingxi Liu,Mingxi Liu,Zeqian Sima",Finance Research Letters,2020,,,
783,A Comprehensive Survey on Graph Neural Networks,"Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.","Zonghan Wu,Shirui Pan,Shirui Pan,Fengwen Chen,Guodong Long,Chengqi Zhang,Philip S. Yu",arXiv: Learning,2019,,,
784,Generating Realistic Stock Market Order Streams,"We propose an approach to generate realistic and high-fidelity stock market data based on generative adversarial networks (GANs). Our Stock-GAN model employs a conditional Wasserstein GAN to capture history dependence of orders. The generator design includes specially crafted aspects including components that approximate the market's auction mechanism, augmenting the order history with order-book constructions to improve the generation task. We perform an ablation study to verify the usefulness of aspects of our network structure. We provide a mathematical characterization of distribution learned by the generator. We also propose statistics to measure the quality of generated orders. We test our approach with synthetic and actual market data, compare to many baseline generative models, and find the generated data to be close to real data.","Junyi Li,Xintong Wang,Yaoyang Lin,Arunesh Sinha,Arunesh Sinha,Arunesh Sinha,Michael P. Wellman",,2020,,,
785,Enhancing unsupervised neural networks based text summarization with word embedding and ensemble learning,"Abstract   The vast amounts of data being collected and analyzed have led to invaluable source of information, which needs to be easily handled by humans. Automatic Text Summarization (ATS) systems enable users to get the gist of information and knowledge in a short time in order to make critical decisions quickly. Deep neural networks have proven their ability to achieve excellent performance in many real-world Natural Language Processing and computer vision applications. However, it still lacks attention in ATS. The key problem of traditional applications is that they involve high dimensional and sparse data, which makes it difficult to capture relevant information. One technique for overcoming these problems is learning features via dimensionality reduction. On the other hand, word embedding is another neural network technique that generates a much more compact word representation than a traditional Bag-of-Words (BOW) approach. In this paper, we are seeking to enhance the quality of ATS by integrating unsupervised deep neural network techniques with word embedding approach. First, we develop a word embedding based text summarization, and we show that Word2Vec representation gives better results than traditional BOW representation. Second, we propose other models by combining word2vec and unsupervised feature learning methods in order to merge information from different sources. We show that unsupervised neural networks models trained on Word2Vec representation give better results than those trained on BOW representation. Third, we also propose three ensemble techniques. The first ensemble combines BOW and word2vec using a majority voting technique. The second ensemble aggregates the information provided by the BOW approach and unsupervised neural networks. The third ensemble aggregates the information provided by Word2Vec and unsupervised neural networks. We show that the ensemble methods improve the quality of ATS, in particular the ensemble based on word2vec approach gives better results. Finally, we perform different experiments to evaluate the performance of the investigated models. We use two kind of datasets that are publically available for evaluating ATS task. Results of statistical studies affirm that word embedding-based models outperform the summarization task compared to those based on BOW approach. In particular, ensemble learning technique with Word2Vec representation surpass all the investigated models.","Nabil Alami,Mohammed Meknassi,Mohammed Meknassi,Noureddine En-nahnahi,Noureddine En-Nahnahi,Noureddine En-Nahnahi,Noureddine En-Nahnahi",Expert Systems With Applications,2019,,,
786,Bitcoin price forecasting with neuro-fuzzy techniques,"Abstract   Cryptocurrencies, with Bitcoin being the most notable example, have attracted considerable attention in recent years, and they have experienced large fluctuations in their price. While a few studies employ conventional statistical and econometric approaches to reveal the driving factors of Bitcoin's prices, research on the development of forecasting models to be used as decision support tools in investment strategies is scarce. This study proposes a computational intelligence technique that uses a hybrid Neuro-Fuzzy controller, namely PATSOS, to forecast the direction in the change of the daily price of Bitcoin. The proposed methodology outperforms two other computational intelligence models, the first being developed with a simpler neuro-fuzzy approach, and the second being developed with artificial neural networks. Furthermore, the investment returns achieved by a trading simulation, based on the signals of the proposed model, are 71.21% higher than the ones achieved through a naive buy-and-hold strategy. The performance of the PATSOS system is robust to the use of other cryptocurrencies.","George S. Atsalakis,Ioanna G. Atsalaki,Fotios Pasiouras,Constantin Zopounidis",European Journal of Operational Research,2019,,,
787,Literature review: Machine learning techniques applied to financial market prediction,"Abstract   The search for models to predict the prices of financial markets is still a highly researched topic, despite major related challenges. The prices of financial assets are non-linear, dynamic, and chaotic; thus, they are financial time series that are difficult to predict. Among the latest techniques, machine learning models are some of the most researched, given their capabilities for recognizing complex patterns in various applications. With the high productivity in the machine learning area applied to the prediction of financial market prices, objective methods are required for a consistent analysis of the most relevant bibliography on the subject. This article proposes the use of bibliographic survey techniques that highlight the most important texts for an area of research. Specifically, these techniques are applied to the literature about machine learning for predicting financial market values, resulting in a bibliographical review of the most important studies about this topic. Fifty-seven texts were reviewed, and a classification was proposed for markets, assets, methods, and variables. Among the main results, of particular note is the greater number of studies that use data from the North American market. The most commonly used models for prediction involve support vector machines (SVMs) and neural networks. It was concluded that the research theme is still relevant and that the use of data from developing markets is a research opportunity.","Bruno Miranda Henrique,Vinicius Amorim Sobreiro,Herbert Kimura",Expert Systems With Applications,2019,,,
788,Probabilistic recursive reasoning for multi-agent reinforcement learning,"Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs. In this paper, we start from level-1 recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the best response and then improve their own policy. We develop decentralized-training-decentralized-execution algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.","Ying Wen,Yaodong Yang,Rui Luo,Jun Wang,Jun Wang,Jun Wang,Wei Pan",,2019,,,
789,Clusterer ensemble,"Ensemble methods that train multiple learners and then combine their predictions have been shown to be very effective in supervised learning. This paper explores ensemble methods for unsupervised learning. Here, an ensemble comprises multiple clusterers, each of which is trained by k-means algorithm with different initial points. The clusters discovered by different clusterers are aligned, i.e. similar clusters are assigned with the same label, by counting their overlapped data items. Then, four methods are developed to combine the aligned clusterers. Experiments show that clustering performance could be significantly improved by ensemble methods, where utilizing mutual information to select a subset of clusterers for weighted voting is a nice choice. Since the proposed methods work by analyzing the clustering results instead of the internal mechanisms of the component clusterers, they are applicable to diverse kinds of clustering algorithms.","Zhi-Hua Zhou,Wei Tang,Wei Tang,Wei Tang",Knowledge Based Systems,2006,,,
790,HiBsteR: Hierarchical Boosted Deep Metric Learning for Image Retrieval,"When the number of categories is growing into thousands, large-scale image retrieval becomes an increasingly hard task. Retrieval accuracy can be improved by learning distance metric methods that separate categories in a transformed embedding space. Unlike most methods that utilize a single embedding to learn a distance metric, we build on the idea of boosted metric learning, where an embedding is split into a boosted ensemble of embeddings. While in general metric learning is directly applied on fine labels to learn embeddings, we take this one step further and incorporate hierarchical label information into the boosting framework and show how to properly adapt loss functions for this purpose. We show that by introducing several sub-embeddings which focus on specific hierarchical classes, the retrieval accuracy can be improved compared to standard flat label embeddings. The proposed method is especially suitable for exploiting hierarchical datasets or when additional labels can be retrieved without much effort. Our approach improves R@1 over state-of-the-art methods on the biggest available retrieval dataset (Stanford Online Products) and sets new reference baselines for hierarchical metric learning on several other datasets (CUB-200-2011, VegFru, FruitVeg-81). We show that the clustering quality in terms of NMI score is superior to previous works.","Georg Waltner,Michael Opitz,Horst Possegger,Horst Bischof",,2019,,,
791,Exhaustive Testing of Trader-agents in Realistically Dynamic Continuous Double Auction Markets: AA Does Not Dominate,"We analyse results from over 3.4million detailed market-trading simulation sessions which collectively confirm an unexpected result: in markets with dynamically varying supply and demand, the best-performing automated adaptive auction-market trading-agent currently known in the AI/Agents literature, i.e. Vytelingum’s Adaptive-Aggressive (AA) strategy, can be routinely out-performed by simpler trading strategies. AA is the most recent in a series of AI trading-agent strategies proposed by various researchers over the past twenty years: research papers contributing major steps in this evolution of strategies have been published at IJCAI, in the Artificial Intelligence journal, and at AAMAS. The innovative step taken here is to brute-force exhaustively evaluate AA in market environments that are in various ways more realistic, closer to real-world financial markets, than the simple constrained abstract experimental evaluations routinely used in the prior academic AI/Agents research literature. We conclude that AA can indeed appear dominant when tested only against other AI-based trading agents in the highly simplified market scenarios that have become the methodological norm in the trading-agents academic research literature, but much of that success seems to be because AA was designed with exactly those simplified experimental markets in mind. As soon as we put AA in scenarios closer to real-world markets, modify it to fit those markets accordingly, and exhaustively test it against simpler trading agents, AA’s dominance simply disappears.",Dave Cliff,,2019,,,
792,Hierarchical Temporal Convolutional Networks for Dynamic Recommender Systems,"Recommender systems that can learn from cross-session data to dynamically predict the next item a user will choose are crucial for online platforms. However, existing approaches often use out-of-the-box sequence models which are limited by speed and memory consumption, are often infeasible for production environments, and usually do not incorporate cross-session information, which is crucial for effective recommendations. Here we propose Hierarchical Temporal Convolutional Networks (HierTCN), a hierarchical deep learning architecture that makes dynamic recommendations based on users' sequential multi-session interactions with items. HierTCN is designed for web-scale systems with billions of items and hundreds of millions of users. It consists of two levels of models: The high-level model uses Recurrent Neural Networks (RNN) to aggregate users' evolving long-term interests across different sessions, while the low-level model is implemented with Temporal Convolutional Networks (TCN), utilizing both the long-term interests and the short-term interactions within sessions to predict the next interaction. We conduct extensive experiments on a public XING dataset and a large-scale Pinterest dataset that contains 6 million users with 1.6 billion interactions. We show that HierTCN is 2.5x faster than RNN-based models and uses 90% less data memory compared to TCN-based models. We further develop an effective data caching scheme and a queue-based mini-batch generator, enabling our model to be trained within 24 hours on a single GPU. Our model consistently outperforms state-of-the-art dynamic recommendation methods, with up to 18% improvement in recall and 10% in mean reciprocal rank.","Jiaxuan You,Yichen Wang,Aditya Pal,Pong Eksombatchai,Chuck Rosenburg,Jure Leskovec",,2019,,,
793,Random Forests,"Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, aaa, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",Leo Breiman,,2001,,,
794,Review on Natural Language Processing (NLP) and Its Toolkits for Opinion Mining and Sentiment Analysis,"As the majority of online networking on the Internet, opinion mining has turned into a fundamental way to deal with investigating such huge numbers of information. Different applications show up in an extensive variety of modern areas. In the interim, opinions have various pronunciations which bring along investigate challenges. The research challenges make opinion mining a dynamic research region recently. In this paper, Natural Language Processing (NLP) techniques for opinion mining and sentiment analysis are reviewed. Initially NLP is reviewed then briefed about its common and useful preprocessing steps also. In this paper opinion mining for various levels are analyzed and reviewed. At the end issues are identified and some recommendation are suggested for opinion mining and-sentiment-analysis.","Yasir Ali Solangi,Zulfiqar Ali Solangi,Samreen Aarain,Amna Abro,Ghulam Ali Mallah,Asadullah Shah",,2018,,,
795,Fuzzy sets,,Lotfi A. Zadeh,,1996,,,
796,Bagging predictors,"Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.",Leo Breiman,,1996,,,
797,Novel Hybrid Integration Approach of Bagging-Based Fisher’s Linear Discriminant Function for Groundwater Potential Analysis,"Groundwater is a vital water source in the rural and urban areas of developing and developed nations. In this study, a novel hybrid integration approach of Fisher’s linear discriminant function (FLDA) with rotation forest (RFLDA) and bagging (BFLDA) ensembles was used for groundwater potential assessment at the Ningtiaota area in Shaanxi, China. A spatial database with 66 groundwater spring locations and 14 groundwater spring contributing factors was prepared; these factors were elevation, aspect, slope, plan and profile curvatures, sediment transport index, stream power index, topographic wetness index, distance to roads and streams, land use, lithology, soil and normalized difference vegetation index. The classifier attribute evaluation method based on the FLDA model was implemented to test the predictive competence of the mentioned contributing factors. The area under curve, confidence interval at 95%, standard error, Friedman test and Wilcoxon signed-rank test were used to compare and validate the success and prediction competence of the three applied models. According to the achieved results, the BFLDA model showed the most prediction competence, followed by the RFLDA and FLDA models, respectively. The resulting groundwater spring potential maps can be used for groundwater development plans and land use planning.","Wei Chen,Wei Chen,Wei Chen,Wei Chen,Biswajeet Pradhan,Shaojun Li,Shaojun Li,Himan Shahabi,Hossein Mojaddadi Rizeei,Enke Hou,Shengquan Wang,Shengquan Wang",Natural resources research,2019,,,
798,Stochastic discrimination,A general method is introduced for separating points in multidimensional spaces through the use of stochastic processes. This technique is called stochastic discrimination.,E. M. Kleinberg,,1990,,,
799,Towards Neural Mixture Recommender for Long Range Dependent User Sequences,"Understanding temporal dynamics has proved to be highly valuable for accurate recommendation. Sequential recommenders have been successful in modeling the dynamics of users and items over time. However, while different model architectures excel at capturing various temporal ranges or dynamics, distinct application contexts require adapting to diverse behaviors.    In this paper we examine how to build a model that can make use of different temporal ranges and dynamics depending on the request context. We begin with the analysis of an anonymized Youtube dataset comprising millions of user sequences. We quantify the degree of long-range dependence in these sequences and demonstrate that both short-term and long-term dependent behavioral patterns co-exist. We then propose a neural Multi-temporal-range Mixture Model (M3) as a tailored solution to deal with both short-term and long-term dependencies. Our approach employs a mixture of models, each with a different temporal range. These models are combined by a learned gating mechanism capable of exerting different model combinations given different contextual information. In empirical evaluations on a public dataset and our own anonymized YouTube dataset, M3 consistently outperforms state-of-the-art sequential recommendation methods.","Jiaxi Tang,Francois Belletti,Sagar Jain,Minmin Chen,Alex Beutel,Can Xu,Can Xu,Can Xu,Can Xu,Ed H. Chi",,2019,,,
800,An energy persistent Range-dependent Regulated Transmission Communication model for vehicular network applications,"Abstract   Multi Point Relays (MPRs) are engaged for supporting road-side communication by sharing traffic information, road conditions and additional notifications. Vehicular Networks (VNs) gather round information from these relay points for ease of information access and monitoring purpose. Managing the operation hours of relay sensors becomes vital as they cannot be frequently recharged but has to meet the vehicle requirements over a protracted time. MPR is a tiny sensor mote with built-in power source that is placed along the roadways for the allotment of useful information. The operation hours of the sensors depends on its battery power for which maintenance is important. To assurance uninterrupted communication and information sharing from MPR sensors, we propose a Range-dependent Regulated Transmission (RRT) with doze formulation. In a RRT, the assortment and power of the sensor nodes are modifiable with respect to vehicular density and type of information. The type of information is pre-informed by the nearest Road-Side Unit (RSU) from which the transmit power level of the MPR is decided. Similarly, the MPRs remain in sleep state when vehicle density is nil. The MPRs switch to operation state when it listens to a Vehicle Request (VR) packet from withers RSU or On-Board unit (OBU). RRT with doze formulation improves the communication time and rate of the road side relay units but adjusting the communication factors with respect to the vehicle requirements.","S. Baskar,S. Periyanayagi,S. Periyanayagi,S. Periyanayagi,P. Mohamed Shakeel,V. R. Sarma Dhulipala",Computer Networks,2019,,,
801,Stacked regressions,,Leo Breiman,,1996,,,
802,Research on Key Technologies of Smart Campus Teaching Platform Based on 5G Network,"With the development of mobile Internet technology and the popularity of intelligent mobile terminals, the data traffic load of mobile client users on the smart campus network platform has surged. How to reduce the data traffic of the smart campus network platform is an urgent problem to be solved. First, this paper discussed the key technologies of smart campus network teaching platform under the background of the 5G network, expounded the critical technologies of the transport layer of the Internet of Things (IoT) technology, and analyzed from the development perspective of the IoT platform. Second, by investigating the online classroom data of five types of colleges and universities in China and comparing the advantages and disadvantages of online classroom teaching and traditional classroom teaching, it is found that the number of online courses in colleges and universities has exploded in the second half of 2017. Next, this paper analyzed the demand of smart campus online teaching platform under the background of the 5G network, thus established an online teaching platform based on the four initiatives operation model (government-led, college sponsor, teacher subject, and academic director). Finally, this paper adopted the improved VIRE localization algorithm to obtain the specific location information of the student users in the classroom and, then, compared with the error obtained by the VIRE algorithm, and the error of the improved VIRE algorithm is smaller. In the process of obtaining information, the 5G network technology is used for data transmission, which can shorten the check-in time and can improve the positioning accuracy.","Xin Xu,Dan Li,Mengyao Sun,Shichao Yang,Shujiang Yu,Gunasekaran Manogaran,George Mastorakis,Constandinos X. Mavromoustakis",IEEE Access,2019,,,
803,Dynamic Replication and Hedging: A Reinforcement Learning Approach,"The authors of this article address the problem of how to optimally hedge an options book in a practical setting, where trading decisions are discrete and trading costs can be nonlinear and difficult to model. Based on reinforcement learning (RL), a well-established machine learning technique, the authors propose a model that is flexible, accurate and very promising for real-world applications. A key strength of the RL approach is that it does not make any assumptions about the form of trading cost. RL learns the minimum variance hedge subject to whatever transaction cost function one provides. All that it needs is a good simulator, in which transaction costs and options prices are simulated accurately.","Petter N. Kolm,Petter N. Kolm,Gordon Ritter,Gordon Ritter",,2019,,,
804,A New Forecasting Framework for Bitcoin Price with LSTM,"Long short-term memory (LSTM) networks are a state-of-the-art sequence learning in deep learning for time series forecasting. However, less study applied to financial time series forecasting especially in cryptocurrency prediction. Therefore, we propose a new forecasting framework with LSTM model to forecasting bitcoin daily price with two various LSTM models (conventional LSTM model and LSTM with AR(2) model). The performance of the proposed models are evaluated using daily bitcoin price data during 2018/1/1 to 2018/7/28 in total 208 records. The results confirmed the excellent forecasting accuracy of the proposed model with AR(2). The test mean squared error (MSE), root mean square error (RMSE), mean absolute percentage error (MAPE), and mean absolute error (MAE) for bitcoin price prediction, respectively. The our proposed LSTM with AR(2) model outperformed than conventional LSTM model. The contribution of this study is providing a new forecasting framework for bitcoin price prediction can overcome and improve the problem of input variables selection in LSTM without strict assumptions of data assumption. The results revealed its possible applicability in various cryptocurrencies prediction, industry instances such as medical data or financial time-series data.","Chih-Hung Wu,Chih-Hung Wu,Chih-Hung Wu,Chih-Chiang Lu,Yu-Feng Ma,Ruei-Shan Lu",,2018,,,
805,Variational Session-based Recommendation Using Normalizing Flows,"We present a novel generative Session-Based Recommendation (SBR) framework, called VAriational SEssion-based Recommendation (VASER) - a non-linear probabilistic methodology allowing Bayesian inference for flexible parameter estimation of sequential recommendations. Instead of directly applying extended Variational AutoEncoders (VAE) to SBR, the proposed method introduces normalizing flows to estimate the probabilistic posterior, which is more effective than the agnostic presumed prior approximation used in existing deep generative recommendation approaches. VASER explores soft attention mechanism to upweight the important clicks in a session. We empirically demonstrate that the proposed model significantly outperforms several state-of-the-art baselines, including the recently-proposed RNN/VAE-based approaches on real-world datasets.","Fan Zhou,Zijing Wen,Kunpeng Zhang,Goce Trajcevski,Ting Zhong",,2019,,,
806,Deep learning,"Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.","Yann LeCun,Yoshua Bengio,Geoffrey E. Hinton",Nature,2015,,,
807,Model-Based Reinforcement Learning for Atari,"Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of $100$K interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.","Lukasz Kaiser,Mohammad Babaeizadeh,Piotr Milos,Blazej Osinski,Roy H. Campbell,Konrad Czechowski,Dumitru Erhan,Chelsea Finn,Piotr Kozakowski,Sergey Levine,Afroz Mohiuddin,Ryan Sepassi,George Tucker,Henryk Michalewski",arXiv: Learning,2019,,,
808,Graph Theory (on Demand Printing Of 02787),,"Frank Harary,Frank Harary",,2018,,,
809,High Frequency Trading and Its Impact on Market Quality,,Peiteng Zhai,,2018,,,
810,CNNpred: CNN-based stock market prediction using a diverse set of variables,"Abstract   Feature extraction from financial data is one of the most important problems in market prediction domain for which many approaches have been suggested. Among other modern tools, convolutional neural networks (CNN) have recently been applied for automatic feature selection and market prediction. However, in experiments reported so far, less attention has been paid to the correlation among different markets as a possible source of information for extracting features. In this paper, we suggest a CNN-based framework, that can be applied on a collection of data from a variety of sources, including different markets, in order to extract features for predicting the future of those markets. The suggested framework has been applied for predicting the next day’s direction of movement for the indices of S&P 500, NASDAQ, DJI, NYSE, and RUSSELL based on various sets of initial variables. The evaluations show a significant improvement in prediction’s performance compared to the state of the art baseline algorithms.","Ehsan Hoseinzade,Ehsan Hoseinzade,Saman Haratizadeh",Expert Systems With Applications,2019,,,
811,An evolutionary approach to build ensembles of multi-label classifiers,"Abstract   In recent years, the multi-label classification task has gained the attention of the scientific community given its ability to solve problems where each of the instances of the dataset may be associated with several class labels at the same time instead of just one. The main problems to deal with in multi-label classification are the imbalance, the relationships among the labels, and the high complexity of the output space. A large number of methods for multi-label classification has been proposed, but although they aimed to deal with one or many of these problems, most of them did not take into account these characteristics of the data in their building phase. In this paper we present an evolutionary algorithm for automatic generation of ensembles of multi-label classifiers by tackling the three previously mentioned problems, called Evolutionary Multi-label Ensemble (EME). Each multi-label classifier is focused on a small subset of the labels, still considering the relationships among them but avoiding the high complexity of the output space. Further, the algorithm automatically designs the ensemble evaluating both its predictive performance and the number of times that each label appears in the ensemble, so that in imbalanced datasets infrequent labels are not ignored. For this purpose, we also proposed a novel mutation operator that considers the relationship among labels, looking for individuals where the labels are more related. EME was compared to other state-of-the-art algorithms for multi-label classification over a set of fourteen multi-label datasets and using five evaluation measures. The experimental study was carried out in two parts, first comparing EME to classic multi-label classification methods, and second comparing EME to other ensemble-based methods in multi-label classification. EME performed significantly better than the rest of classic methods in three out of five evaluation measures. On the other hand, EME performed the best in one measure in the second experiment and it was the only one that did not perform significantly worse than the control algorithm in any measure. These results showed that EME achieved a better and more consistent performance than the rest of the state-of-the-art methods in MLC.","Jose M. Moyano,Eva Gibaja,Krzysztof J. Cios,Sebastián Ventura",Information Fusion,2019,,,
812,A constrained portfolio trading system using particle swarm algorithm and recurrent reinforcement learning,"Abstract   This study extends a recurrent reinforcement portfolio allocation and rebalancing management system with complex portfolio constraints using particle swarm algorithms. In particular, we propose to use a combination of recurrent reinforcement learning (RRL) and particle swarm algorithm (PSO) with Calmar ratio for both asset allocation and constraint optimization. Using S&P100 index stocks, we show such a system with a Calmar ratio based objective function yields a better efficient frontier than the Sharpe ratio and mean-variance based portfolios. By comparing with multiple PSO based long only constrained portfolios, we propose an optimal portfolio trading system that is capable of generating both long and short signals and handling the common portfolio constraints. We further develop an adaptive RRL-PSO portfolio rebalancing decision system with a market condition stop-loss retraining mechanism, and we show that the proposed portfolio trading system outperforms the benchmarks consistently especially under high transaction cost conditions.","Saud Almahdi,Steve Y. Yang",Expert Systems With Applications,2019,,,
813,Do news and sentiment play a role in stock price prediction,"Despite continuous improvement in the range and quality of machine learning techniques, accurately predicting stock prices still remains as elusive as ever. We approach this problem using a modern autoregressive neural network architecture and incorporate sentiment predictors, which are becoming increasingly available due to advances in text mining techniques. We find that the inclusion of predictors based on counts of the number of news articles and twitter posts can significantly improve the quality of stock price predictions.","Bruce J Vanstone,Adrian Gepp,Geoff Harris",Applied Intelligence,2019,,,
814,Ensemble of CNN for multi-focus image fusion,"Abstract   The Convolutional Neural Networks (CNNs) based multi-focus image fusion methods have recently attracted enormous attention. They greatly enhanced the constructed decision map compared with the previous state of the art methods that have been done in the spatial and transform domains. Nevertheless, these methods have not reached to the satisfactory initial decision map, and they need to undergo vast post-processing algorithms to achieve a satisfactory decision map. In this paper, a novel CNNs based method with the help of the ensemble learning is proposed. It is very reasonable to use various models and datasets rather than just one. The ensemble learning based methods intend to pursue increasing diversity among the models and datasets in order to decrease the problem of the overfitting on the training dataset. It is obvious that the results of an ensemble of CNNs are better than just one single CNNs. Also, the proposed method introduces a new simple type of multi-focus images dataset. It simply changes the arranging of the patches of the multi-focus datasets, which is very useful for obtaining the better accuracy. With this new type arrangement of datasets, the three different datasets including the original and the Gradient in directions of vertical and horizontal patches are generated from the COCO dataset. Therefore, the proposed method introduces a new network that three CNNs models which have been trained on three different created datasets to construct the initial segmented decision map. These ideas greatly improve the initial segmented decision map of the proposed method which is similar, or even better than, the other final decision map of CNNs based methods obtained after applying many post-processing algorithms. Many real multi-focus test images are used in our experiments, and the results are compared with quantitative and qualitative criteria. The obtained experimental results indicate that the proposed CNNs based network is more accurate and have the better decision map without post-processing algorithms than the other existing state of the art multi-focus fusion methods which used many post-processing algorithms.","Mostafa Amin-Naji,Ali Aghagolzadeh,Mehdi Ezoji",Information Fusion,2019,,,
815,Review of Deep Learning Algorithms and Architectures,"Deep learning (DL) is playing an increasingly important role in our lives. It has already made a huge impact in areas, such as cancer diagnosis, precision medicine, self-driving cars, predictive forecasting, and speech recognition. The painstakingly handcrafted feature extractors used in traditional learning, classification, and pattern recognition systems are not scalable for large-sized data sets. In many cases, depending on the problem complexity, DL can also overcome the limitations of earlier shallow networks that prevented efficient training and abstractions of hierarchical representations of multi-dimensional training data. Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures. This paper reviews several optimization methods to improve the accuracy of the training and to reduce training time. We delve into the math behind training algorithms used in recent deep networks. We describe current shortcomings, enhancements, and implementations. The review also covers different types of deep architectures, such as deep convolution networks, deep residual networks, recurrent neural networks, reinforcement learning, variational autoencoders, and others.","Ajay Shrestha,Ausif Mahmood",IEEE Access,2019,,,
816,Deep Reinforcement Learning for Optimizing Finance Portfolio Management,"Deep reinforcement learning (DRL) is an emerging artificial intelligence (AI) research field which combines deep learning (DL) for policy optimization and reinforcement learning (RL) for goal-oriented self-learning without human intervention. We address major research issues of policy optimization for finance portfolio management. First, we explore one of the deep recurrent neural network (RNN) models, GRUs, to decide the influences of earlier states and actions on policy optimization in non-Markov decision processes. Then, we craft for a viable risk-adjusted reward function to evaluate the expected total rewards for policy. Third, we empower the integration of RL and DL to leverage their respective capabilities to discover an optimal policy. Fourth, we investigate each type of RL approaches for integrating with the DL method while solving the policy optimization problem.","Yuh-Jong Hu,Shang-Jen Lin",,2019,,,
817,Self Learning in Flexible Manufacturing Units: A Reinforcement Learning Approach,"This paper presents a novel approach for self-learning as well as plug-and-play control of highly flexible, modular manufacturing units. The approach is inspired by recent encouraging results of reinforcement learning (RL) in computer science. However, instead of learning the entire control behavior which results in long training times and requires huge data sets, we restrict the learning process to the supervisor level by defining appropriate parameter from the basic control level (BCL) to be learned by learning agents. To this end, we define a set of interface parameter to the BCL programmed by IEC61131 compatible code, which will be used for learning. Typical control parameters include switching thresholds, timing parameters and transition conditions. We apply the approach to a laboratory testbed consisting of different production modules which underlines the efficiency improvements for manufacturing units. In addition, plug-and-produce control is enabled by the approach as different configuration of production modules can efficiently be put in operation by re-learning the parameter sets.","Dorothea Schwung,Jan Niklas Reimann,Andreas Schwung,Steven X. Ding",,2018,,,
818,Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement Learning,"Author(s): Haarnoja, Tuomas | Advisor(s): Levine, Sergey; Abbeel, Pieter | Abstract: In this thesis, we study how maximum entropy framework can provide efficient deep reinforcement learning (deep RL) algorithms that solve tasks consistently and sample efficiently. This framework has several intriguing properties. First, the optimal policies are stochastic, improving exploration and preventing convergence to local optima, particularly when the objective is multimodal. Second, the entropy term provides regularization, resulting in more consistent and robust learning when compared to deterministic methods. Third, maximum entropy policies are composable, that is, two or more policies can be combined, and the resulting policy can be shown to be approximately optimal for the sum of the constituent task rewards. And fourth, the view of maximum entropy RL as probabilistic inference provides a foundation for building hierarchical policies that can solve complex and sparse reward tasks. In the first part, we will devise new algorithms based on this framework, starting from soft Q-learning that learns expressive energy-based policies, to soft actor-critic that provides simplicity and convenience of actor-critic methods, and ending with automatic temperature adjustment scheme that practically eliminates the need for hyperparameter tuning, which is a crucial feature for real-world applications where tuning of hyperparameters can be prohibitively expensive. In the second part, we will discuss extensions enabled by the inherent stochasticity of maximum entropy polices, including compositionality and hierarchical learning. We will demonstrate the effectiveness of the proposed algorithms on both simulated and real-world robotic manipulation and locomotion tasks.",Tuomas Haarnoja,,2018,,,
819,A Deep Reinforcement Learning Approach for Automated Cryptocurrency Trading,"Nowadays, Artificial Intelligence (AI) is changing our daily life in many application fields. Automatic trading has inspired a large number of field experts and scientists in developing innovative techniques and deploying cutting-edge technologies to trade different markets. In this context, cryptocurrency has given new interest in the application of AI techniques for predicting the future price of a financial asset. In this work Deep Reinforcement Learning is applied to trade bitcoin. More precisely, Double and Dueling Double Deep Q-learning Networks are compared over a period of almost four years. Two reward functions are also tested: Sharpe ratio and profit reward functions. The Double Deep Q-learning trading system based on Sharpe ratio reward function demonstrated to be the most profitable approach for trading bitcoin.","Giorgio Lucarelli,Matteo Borrotti,Matteo Borrotti",,2019,,,
820,A novel deep stacking least squares support vector machine for rolling bearing fault diagnosis,"Abstract   The traditional intelligent diagnosis methods for rolling bearing based on least squares support vector machine (LS-SVM) need to manually extract sensitive features with professional engineering experience. To eliminate the impact of manual feature extraction on the capability of LS-SVM, a novel method called deep stacking LS-SVM (DSLS-SVM) with the concept of stacking-based representation learning (S-RL) is proposed in this paper. DSLS-SVM assembles learnable modules of multiply LS-SVMs in the structure of deep stacking with the output of the current module combined with raw input and the outputs of all lower models as the input of next module. The rolling bearing fault signals from Case Western Reserve University (CWRU) and our laboratory are used to analysis the performance of DSLS-SVM. The analysis results indicate that DSLS-SVM is effective to extract intrinsic fault feature from measured vibration signal, and it has good effectiveness and applicability in rolling bearing fault diagnosis.","Xin Li,Yu Yang,Haiyang Pan,Haiyang Pan,Jian Cheng,Jian Cheng,Jian Cheng,Jian Cheng,Junsheng Cheng",Computers in Industry,2019,,,
821,Usage of Machine Learning for Strategic Decision Making at Higher Educational Institutions,"Decisions made at the strategic level of Higher Educational Institutions (HEIs) affect policies, strategies, and actions that the institutions make as a whole. Decision's structures at HEIs are depicted in this paper and their effectiveness in supporting the institutions' governance. The disengagement of the stakeholders and the lack of using efficient computational algorithms lead to 1) the decision process takes longer; 2) the “whole picture” is not involved along with all data necessary; and 3) small academic impact is produced by the decision, among others. Machine learning is an emerging field of artificial intelligence that using various algorithms analyzes information and provides a richer understanding of the data contained in a specific context. Based on the author's previous works, we focus on supporting decision-making at a strategic level, being deans' concerns the preeminent mission to bolster. In this paper, three supervised classification algorithms are deployed to predict graduation rates from real data about undergraduate engineering students in South America. The analysis of receiver operating characteristic (ROC) curve and accuracy are executed as measures of effectiveness to compare and evaluate decision tree, logistic regression, and random forest, where this last one demonstrates the best outcomes.","Yuri Vanessa Nieto,Vicente Gacía-Díaz,Carlos Enrique Montenegro Marín,Carlos Enrique Montenegro,Claudio Camilo González,Rubén González Crespo",IEEE Access,2019,,,
822,Android malware detection through hybrid features fusion and ensemble classifiers: The AndroPyTool framework and the OmniDroid dataset,"Abstract   Cybersecurity has become a major concern for society, mainly motivated by the increasing number of cyber attacks and the wide range of targeted objectives. Due to the popularity of smartphones and tablets, Android devices are considered an entry point in many attack vectors. Malware applications are among the most used tactics and tools to perpetrate a cyber attack, so it is critical to study new ways of detecting them. In these detection mechanisms, machine learning has been used to build classifiers that are effective in discerning if an application is malware or benignware. However, training such classifiers require big amounts of labelled data which, in this context, consist of categorised malware and benignware Android applications represented by a set of features able to describe their behaviour. For that purpose, in this paper we present OmniDroid, a large and comprehensive dataset of features extracted from 22,000 real malware and goodware samples, aiming to help anti-malware tools creators and researchers when improving, or developing, new mechanisms and tools for Android malware detection. Furthermore, the characteristics of the dataset make it suitable to be used as a benchmark dataset to test classification and clustering algorithms or new representation techniques, among others. The dataset has been released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License and was built using AndroPyTool, our automated framework for dynamic and static analysis of Android applications. Finally, we test a set of ensemble classifiers over this dataset and propose a malware detection approach based on the fusion of static and dynamic features through the combination of ensemble classifiers. The experimental results show the feasibility and potential usability (for the machine learning, soft computing and cyber security communities) of our automated framework and the publicly available dataset.","Alejandro Martín,Raúl Lara-Cabrera,David Camacho",Information Fusion,2019,,,
823,Unsupervised Feature Learning with K-means and An Ensemble of Deep Convolutional Neural Networks for Medical Image Classification.,"Medical image analysis using supervised deep learning methods remains problematic because of the reliance of deep learning methods on large amounts of labelled training data. Although medical imaging data repositories continue to expand there has not been a commensurate increase in the amount of annotated data. Hence, we propose a new unsupervised feature learning method that learns feature representations to then differentiate dissimilar medical images using an ensemble of different convolutional neural networks (CNNs) and K-means clustering. It jointly learns feature representations and clustering assignments in an end-to-end fashion. We tested our approach on a public medical dataset and show its accuracy was better than state-of-the-art unsupervised feature learning methods and comparable to state-of-the-art supervised CNNs. Our findings suggest that our method could be used to tackle the issue of the large volume of unlabelled data in medical imaging repositories.","Euijoon Ahn,Ashnil Kumar,David Dagan Feng,Dagan Feng,Michael J. Fulham,Jinman Kim",arXiv: Computer Vision and Pattern Recognition,2019,,,
824,Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning,"Many real-world problems, such as network packet routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) problems. However, existing multi-agent RL methods typically scale poorly in the problem size. Therefore, a key challenge is to translate the success of deep learning on single-agent RL to the multi-agent setting. A major stumbling block is that independent Q-learning, the most popular multi-agent RL method, introduces nonstationarity that makes it incompatible with the experience replay memory on which deep Q-learning relies. This paper proposes two methods that address this problem: 1) using a multi-agent variant of importance sampling to naturally decay obsolete data and 2) conditioning each agent's value function on a fingerprint that disambiguates the age of the data sampled from the replay memory. Results on a challenging decentralised variant of StarCraft unit micromanagement confirm that these methods enable the successful combination of experience replay with multi-agent RL.","Jakob Foerster,Nantas Nardelli,Gregory Farquhar,Triantafyllos Afouras,Philip H. S. Torr,Pushmeet Kohli,Shimon Whiteson",arXiv: Artificial Intelligence,2017,,,
825,Reinforcement Learning with Deep Energy-Based Policies,"We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.","Tuomas Haarnoja,Haoran Tang,Pieter Abbeel,Sergey Levine",arXiv: Learning,2017,,,
826,Recurrent Latent Variable Networks for Session-Based Recommendation,"In this work, we attempt to ameliorate the impact of data sparsity in the context of session-based recommendation. Specifically, we seek to devise a machine learning mechanism capable of extracting subtle and complex underlying temporal dynamics in the observed session data, so as to inform the recommendation algorithm. To this end, we improve upon systems that utilize deep learning techniques with recurrently connected units; we do so by adopting concepts from the field of Bayesian statistics, namely variational inference. Our proposed approach consists in treating the network recurrent units as stochastic latent variables with a prior distribution imposed over them. On this basis, we proceed to infer corresponding posteriors; these can be used for prediction and recommendation generation, in a way that accounts for the uncertainty in the available sparse training data. To allow for our approach to easily scale to large real-world datasets, we perform inference under an approximate amortized variational inference (AVI) setup, whereby the learned posteriors are parameterized via (conventional) neural networks. We perform an extensive experimental evaluation of our approach using challenging benchmark datasets, and illustrate its superiority over existing state-of-the-art techniques.","Sotirios P. Chatzis,Panayiotis Christodoulou,Andreas S. Andreou",arXiv: Information Retrieval,2017,,,
827,Trust Region Policy Optimization,"We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.","John Schulman,Sergey Levine,Philipp Moritz,Michael I. Jordan,Pieter Abbeel",arXiv: Learning,2015,,,
828,Deep Forest: Towards An Alternative to Deep Neural Networks,"In this paper, we propose gcForest, a decision tree ensemble approach with performance highly competitive to deep neural networks. In contrast to deep neural networks which require great effort in hyper-parameter tuning, gcForest is much easier to train. Actually, even when gcForest is applied to different data from different domains, excellent performance can be achieved by almost same settings of hyper-parameters. The training process of gcForest is efficient and scalable. In our experiments its training time running on a PC is comparable to that of deep neural networks running with GPU facilities, and the efficiency advantage may be more apparent because gcForest is naturally apt to parallel implementation. Furthermore, in contrast to deep neural networks which require large-scale training data, gcForest can work well even when there are only small-scale training data. Moreover, as a tree-based approach, gcForest should be easier for theoretical analysis than deep neural networks.","Zhi-Hua Zhou,Ji Feng",arXiv: Learning,2017,,,
829,Contextual Sequence Modeling for Recommendation with Recurrent Neural Networks,"Recommendations can greatly benefit from good representations of the user state at recommendation time. Recent approaches that leverage Recurrent Neural Networks (RNNs) for session-based recommendations have shown that Deep Learning models can provide useful user representations for recommendation. However, current RNN modeling approaches summarize the user state by only taking into account the sequence of items that the user has interacted with in the past, without taking into account other essential types of context information such as the associated types of user-item interactions, the time gaps between events and the time of day for each interaction. To address this, we propose a new class of Contextual Recurrent Neural Networks for Recommendation (CRNNs) that can take into account the contextual information both in the input and output layers and modifying the behavior of the RNN by combining the context embedding with the item embedding and more explicitly, in the model dynamics, by parametrizing the hidden unit transitions as a function of context information. We compare our CRNNs approach with RNNs and non-sequential baselines and show good improvements on the next event prediction task.","Elena Smirnova,Flavian Vasile,Flavian Vasile",arXiv: Information Retrieval,2017,,,
830,Distributed Representations of Words and Phrases and their Compositionality,"The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of ""Canada"" and ""Air"" cannot be easily combined to obtain ""Air Canada"". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.","Tomas Mikolov,Ilya Sutskever,Kai Chen,Kai Chen,Greg S. Corrado,Jeffrey Dean",arXiv: Computation and Language,2013,,,
831,Benchmarking Reinforcement Learning Algorithms on Real-World Robots,"Through many recent successes in simulation, model-free reinforcement learning has emerged as a promising approach to solving continuous control robotic tasks. The research community is now able to reproduce, analyze and build quickly on these results due to open source implementations of learning algorithms and simulated benchmark tasks. To carry forward these successes to real-world applications, it is crucial to withhold utilizing the unique advantages of simulations that do not transfer to the real world and experiment directly with physical robots. However, reinforcement learning research with physical robots faces substantial resistance due to the lack of benchmark tasks and supporting source code. In this work, we introduce several reinforcement learning tasks with multiple commercially available robots that present varying levels of learning difficulty, setup, and repeatability. On these tasks, we test the learning performance of off-the-shelf implementations of four reinforcement learning algorithms and analyze sensitivity to their hyper-parameters to determine their readiness for applications in various real-world tasks. Our results show that with a careful setup of the task interface and computations, some of these implementations can be readily applicable to physical robots. We find that state-of-the-art learning algorithms are highly sensitive to their hyper-parameters and their relative ordering does not transfer across tasks, indicating the necessity of re-tuning them for each task for best performance. On the other hand, the best hyper-parameter configuration from one task may often result in effective learning on held-out tasks even with different robots, providing a reasonable default. We make the benchmark tasks publicly available to enhance reproducibility in real-world reinforcement learning.","A. Rupam Mahmood,Dmytro Korenkevych,Gautham Vasan,Will Ma,William Ma,James Bergstra",arXiv: Learning,2018,,,
832,Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning,"Humans are capable of attributing latent mental contents such as beliefs or intentions to others. The social skill is critical in daily life for reasoning about the potential consequences of others' behaviors so as to plan ahead. It is known that humans use such reasoning ability recursively by considering what others believe about their own beliefs. In this paper, we start from level-$1$ recursion and introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policies, to which each agent finds the best response and then improve their own policies. We develop decentralized-training-decentralized-execution algorithms, namely PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenarios when there exists one Nash equilibrium. Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.","Ying Wen,Yaodong Yang,Rui Luo,Jun Wang,Jun Wang,Jun Wang,Wei Pan",arXiv: Learning,2019,,,
833,Efficient Estimation of Word Representations in Vector Space,"We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.","Tomas Mikolov,Kai Chen,Kai Chen,Greg S. Corrado,Jeffrey Dean",arXiv: Computation and Language,2013,,,
834,Can we learn to beat the best stock,"A novel algorithm for actively trading stocks is presented. While traditional expert advice and ""universal"" algorithms (as well as standard technical trading heuristics) attempt to predict winners or trends, our approach relies on predictable statistical relations between all pairs of stocks in the market. Our empirical results on historical markets provide strong evidence that this type of technical trading can ""beat the market"" and moreover, can beat the best stock in the market. In doing so we utilize a new idea for smoothing critical parameters in the context of expert learning.","Allan Borodin,Ran El-Yaniv,Vincent Gogan",Journal of Artificial Intelligence Research,2004,,,
835,BPR: Bayesian Personalized Ranking from Implicit Feedback,"Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive knearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.","Steffen Rendle,Christoph Freudenthaler,Zeno Gantner,Lars Schmidt-Thieme",arXiv: Information Retrieval,2012,,,
836,Extending Deep Learning Models for Limit Order Books to Quantile Regression,"We showcase how Quantile Regression (QR) can be applied to forecast financial returns using Limit Order Books (LOBs), the canonical data source of high-frequency financial time-series. We develop a deep learning architecture that simultaneously models the return quantiles for both buy and sell positions. We test our model over millions of LOB updates across multiple different instruments on the London Stock Exchange. Our results suggest that the proposed network not only delivers excellent performance but also provides improved prediction robustness by combining quantile estimates.","Zihao Zhang,Zihao Zhang,Zihao Zhang,Stefan Zohren,Stephen J. Roberts",arXiv: Trading and Market Microstructure,2019,,,
837,Streaming Session-based Recommendation,"Session-based Recommendation (SR) is the task of recommending the next item based on previously recorded user interactions. In this work, we study SR in a practical streaming scenario, namely Streaming Session-based Recommendation (SSR), which is a more challenging task due to (1) the uncertainty of user behaviors, and (2) the continuous, large-volume, high-velocity nature of the session data. Recent studies address (1) by exploiting the attention mechanism in Recurrent Neural Network (RNN) to better model the user's current intent, which leads to promising improvements. However, the proposed attention models are based solely on the current session. Moreover, existing studies only perform SR under static offline settings and none of them explore (2). In this work, we target SSR and propose a Streaming Session-based Recommendation Machine (SSRM) to tackle these two challenges. Specifically, to better understand the uncertainty of user behaviors, we propose a Matrix Factorization (MF) based attention model, which improves the commonly used attention mechanism by leveraging the user's historical interactions. To deal with the large-volume and high-velocity challenge, we introduce a reservoir-based streaming model where an active sampling strategy is proposed to improve the efficiency of model updating. We conduct extensive experiments on two real-world datasets. The experimental results demonstrate the superiority of the SSRM method compared to several state-of-the-art methods in terms of MRR and Recall.","Lei Guo,Hongzhi Yin,Qinyong Wang,Tong Chen,Alexander Zhou,Quoc Viet Hung Nguyen,Nguyen Quoc Viet Hung",,2019,,,
838,Evolution-Guided Policy Gradient in Reinforcement Learning,"Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real-world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer from high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA's ability of temporal credit assignment with a fitness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments in a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DRL and EA methods.","Shauharda Khadka,Kagan Tumer",arXiv: Learning,2018,,,
839,Temporal Ensembling for Semi-Supervised Learning,"In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.","Samuli Laine,Timo Aila",arXiv: Neural and Evolutionary Computing,2016,,,
840,The StarCraft Multi-Agent Challenge,"In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-the-art algorithms. We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: this https URL.","Mikayel Samvelyan,Tabish Rashid,Christian Schroeder de Witt,Gregory Farquhar,Nantas Nardelli,Tim G. J. Rudner,Chia-Man Hung,Philip H. S. Torr,Jakob Foerster,Shimon Whiteson",arXiv: Learning,2019,,,
841,A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction,"The Nonlinear autoregressive exogenous (NARX) model, which predicts the current value of a time series based upon its previous values as well as the current and past values of multiple driving (exogenous) series, has been studied for decades. Despite the fact that various NARX models have been developed, few of them can capture the long-term temporal dependencies appropriately and select the relevant driving series to make predictions. In this paper, we propose a dual-stage attention-based recurrent neural network (DA-RNN) to address these two issues. In the first stage, we introduce an input attention mechanism to adaptively extract relevant driving series (a.k.a., input features) at each time step by referring to the previous encoder hidden state. In the second stage, we use a temporal attention mechanism to select relevant encoder hidden states across all time steps. With this dual-stage attention scheme, our model can not only make predictions effectively, but can also be easily interpreted. Thorough empirical studies based upon the SML 2010 dataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can outperform state-of-the-art methods for time series prediction.","Yao Qin,Dongjin Song,Haifeng Chen,Haifeng Chen,Wei Cheng,Guofei Jiang,Garrison W. Cottrell",arXiv: Learning,2017,,,
842,Learning with Symmetric Label Noise: The Importance of Being Unhinged,"Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2010] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2010] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the SLN-robustness of the unhinged loss.","Brendan van Rooyen,Aditya Krishna Menon,Robert C. Williamson",arXiv: Learning,2015,,,
843,Locally Weighted Ensemble Clustering,"Due to its ability to combine multiple base clusterings into a probably better and more robust clustering, the ensemble clustering technique has been attracting increasing attention in recent years. Despite the significant success, one limitation to most of the existing ensemble clustering methods is that they generally treat all base clusterings equally regardless of their reliability, which makes them vulnerable to low-quality base clusterings. Although some efforts have been made to (globally) evaluate and weight the base clusterings, yet these methods tend to view each base clustering as an individual and neglect the local diversity of clusters inside the same base clustering. It remains an open problem how to evaluate the reliability of clusters and exploit the local diversity in the ensemble to enhance the consensus performance, especially in the case when there is no access to data features or specific assumptions on data distribution. To address this, in this paper, we propose a novel ensemble clustering approach based on ensemble-driven cluster uncertainty estimation and local weighting strategy. In particular, the uncertainty of each cluster is estimated by considering the cluster labels in the entire ensemble via an entropic criterion. A novel ensemble-driven cluster validity measure is introduced, and a locally weighted co-association matrix is presented to serve as a summary for the ensemble of diverse clusters. With the local diversity in ensembles exploited, two novel consensus functions are further proposed. Extensive experiments on a variety of real-world datasets demonstrate the superiority of the proposed approach over the state-of-the-art.","Dong Huang,Chang-Dong Wang,Jian-Huang Lai,Jian-Huang Lai",arXiv: Learning,2016,,,
844,Investment Behaviors Can Tell What Inside: Exploring Stock Intrinsic Properties for Stock Trend Prediction,"Stock trend prediction, aiming at predicting future price trend of stocks, plays a key role in seeking maximized profit from the stock investment. Recent years have witnessed increasing efforts in applying machine learning techniques, especially deep learning, to pursue more promising stock prediction. While deep learning has given rise to significant improvement, human investors still retain the leading position due to their understanding on stock intrinsic properties, which can imply invaluable principles for stock prediction. In this paper, we propose to extract and explore stock intrinsic properties to enhance stock trend prediction. Fortunately, we discover that the repositories of investment behaviors within mutual fund portfolio data form up a gold mine to extract latent representations of stock properties, since such collective investment behaviors can reflect the professional fund managers' common beliefs on stock intrinsic properties. Powered by extracted stock properties, we further propose to model the dynamic market state and trend using stock representations so as to generate the dynamic correlation between the stock and the market, and then we aggregate such correlation with dynamic stock indicators to achieve more accurate stock prediction. Extensive experiments on real-world stock market data demonstrate the effectiveness of stock properties extracted from collective investment behaviors in the task of stock prediction.","Chi Chen,Li Zhao,Jiang Bian,Chunxiao Xing,Chunxiao Xing,Chunxiao Xing,Tie-Yan Liu",,2019,,,
845,Individualized Indicator for All: Stock-wise Technical Indicator Optimization with Stock Embedding,"As one of the most important investing approaches, technical analysis attempts to forecast stock movement by interpreting the inner rules from historic price and volume data. To address the vital noisy nature of financial market, generic technical analysis develops technical trading indicators, as mathematical summarization of historic price and volume data, to form up the foundation for robust and profitable investment strategies. However, an observation reveals that stocks with different properties have different affinities over technical indicators, which discloses a big challenge for the indicator-oriented stock selection and investment. To address this problem, in this paper, we design a Technical Trading Indicator Optimization(TTIO) framework that manages to optimize the original technical indicator by leveraging stock-wise properties. To obtain effective representations of stock properties, we propose a Skip-gram architecture to learn stock embedding inspired by a valuable knowledge repository formed by fund manager's collective investment behaviors. Based on the learned stock representations, TTIO further learns a re-scaling network to optimize the indicator's performance. Extensive experiments on real-world stock market data demonstrate that our method can obtain the very stock representations that are invaluable for technical indicator optimization since the optimized indicators can result in strong investing signals than original ones.","Zhige Li,Derek Yang,Derek Yang,Li Zhao,Jiang Bian,Tao Qin,Tie-Yan Liu",,2019,,,
846,"Deep Learning-based Sequential Recommender Systems: Concepts, Algorithms, and Evaluations","In the field of sequential recommendation, deep learning methods have received a lot of attention in the past few years and surpassed traditional models such as Markov chain-based and factorization-based ones. However, DL-based methods also have some critical drawbacks, such as insufficient modeling of user representation and ignoring to distinguish the different types of interactions (i.e., user behavior) among users and items. In this view, this survey focuses on DL-based sequential recommender systems by taking the aforementioned issues into consideration. Specifically, we illustrate the concept of sequential recommendation, propose a categorization of existing algorithms in terms of three types of behavioral sequence, summarize the key factors affecting the performance of DL-based models, and conduct corresponding evaluations to demonstrate the effects of these factors. We conclude this survey by systematically outlining future directions and challenges in this field.","Hui Fang,Danning Zhang,Yiheng Shu,Guibing Guo",arXiv: Information Retrieval,2019,,,
847,Real-World Image Denoising with Deep Boosting,"We propose a Deep Boosting Framework (DBF) for real-world image denoising by integrating the deep learning technique into the boosting algorithm. The DBF replaces conventional handcrafted boosting units by elaborate convolutional neural networks, which brings notable advantages in terms of both performance and speed. We design a lightweight Dense Dilated Fusion Network (DDFN) as an embodiment of the boosting unit, which addresses the vanishing of gradients during training due to the cascading of networks while promoting the efficiency of limited parameters. The capabilities of the proposed method are first validated on several representative simulation tasks including non-blind and blind Gaussian denoising and JPEG image deblocking. We then focus on a practical scenario to tackle with the complex and challenging real-world noise. To facilitate leaning-based methods including ours, we build a new Real-world Image Denoising (RID) dataset, which contains 200 pairs of high-resolution images with diverse scene content under various shooting conditions. Moreover, we conduct comprehensive analysis on the domain shift issue for real-world denoising and propose an effective one-shot domain transfer scheme to address this issue. Comprehensive experiments on widely used benchmarks demonstrate that the proposed method significantly surpasses existing methods on the task of real-world image denoising.","Chang Chen,Zhiwei Xiong,Xinmei Tian,Zheng-Jun Zha,Feng Wu",IEEE Transactions on Pattern Analysis and Machine Intelligence,2019,,,
848,Hierarchical Temporal Convolutional Networks for Dynamic Recommender Systems,"Recommender systems that can learn from cross-session data to dynamically predict the next item a user will choose are crucial for online platforms. However, existing approaches often use out-of-the-box sequence models which are limited by speed and memory consumption, are often infeasible for production environments, and usually do not incorporate cross-session information, which is crucial for effective recommendations. Here we propose Hierarchical Temporal Convolutional Networks (HierTCN), a hierarchical deep learning architecture that makes dynamic recommendations based on users' sequential multi-session interactions with items. HierTCN is designed for web-scale systems with billions of items and hundreds of millions of users. It consists of two levels of models: The high-level model uses Recurrent Neural Networks (RNN) to aggregate users' evolving long-term interests across different sessions, while the low-level model is implemented with Temporal Convolutional Networks (TCN), utilizing both the long-term interests and the short-term interactions within sessions to predict the next interaction. We conduct extensive experiments on a public XING dataset and a large-scale Pinterest dataset that contains 6 million users with 1.6 billion interactions. We show that HierTCN is 2.5x faster than RNN-based models and uses 90% less data memory compared to TCN-based models. We further develop an effective data caching scheme and a queue-based mini-batch generator, enabling our model to be trained within 24 hours on a single GPU. Our model consistently outperforms state-of-the-art dynamic recommendation methods, with up to 18% improvement in recall and 10% in mean reciprocal rank.","Jiaxuan You,Yichen Wang,Aditya Pal,Pong Eksombatchai,Charles J. Rosenberg,Chuck Rosenberg,Jure Leskovec",arXiv: Social and Information Networks,2019,,,
849,UESTS: An Unsupervised Ensemble Semantic Textual Similarity Method,"Semantic textual similarity (STS) is the task of assessing the degree of similarity between two texts in terms of meaning. Several approaches have been proposed in the literature to determine the semantic similarity between texts. The most promising work recently presented in the literature was supervised approaches. Unsupervised STS approaches are characterized by the fact that they do not require learning data, but they still suffer from some limitations. Word alignment has been widely used in the state-of-the-art approaches. From this point, this paper has three contributions. First, a new synset-oriented word aligner is presented, which relies on a huge multilingual semantic network named BabelNet. Second, three unsupervised STS approaches are proposed: string kernel-based (SK), alignment-based (AL), and weighted alignment-based (WAL). Third, some limitations of the state-of-the-art approaches are tackled, and different similarity methods are demonstrated to be complementary with each other by proposing an unsupervised ensemble STS (UESTS) approach. The UESTS incorporates the merits of four similarity measures: proposed alignment-based, surface-based, corpus-based, and enhanced edit distance. The experimental results proved that the participation of the proposed aligner in STS is effective. Over all the evaluation data sets, the proposed UESTS outperforms the state-of-the-art unsupervised approaches, which is a promising result.","Basma Hassan,Samir E. AbdelRahman,Reem Bahgat,Ibrahim Farag",IEEE Access,2019,,,
850,A Collaborative Session-based Recommendation Approach with Parallel Memory Modules,"Session-based recommendation is the task of predicting the next item to recommend when the only available information consists of anonymous behavior sequences. Previous methods for session-based recommendation focus mostly on the current session, ignoring collaborative information in so-called neighborhood sessions, sessions that have been generated previously by other users and reflect similar user intents as the current session. We hypothesize that the collaborative information contained in such neighborhood sessions may help to improve recommendation performance for the current session.   We propose a Collaborative Session-based Recommendation Machine (CSRM), a novel hybrid framework to apply collaborative neighborhood information to session-based recommendations. CSRM consists of two parallel modules: an Inner Memory Encoder (IME) and an Outer Memory Encoder (OME). The IME models a user's own information in the current session with the help of Recurrent Neural Networks (RNNs) and an attention mechanism. The OME exploits collaborative information to better predict the intent of current sessions by investigating neighborhood sessions. Then, a fusion gating mechanism is used to selectively combine information from the IME and OME so as to obtain the final representation of the current session. Finally, CSRM obtains a recommendation score for each candidate item by computing a bilinear match with the final representation.   Experimental results on three public datasets demonstrate the effectiveness of CSRM compared to state-of-the-art session-based recommender systems. Our analysis of CSRM's recommendation process shows when and how collaborative neighborhood information and the fusion gating mechanism positively impact the performance of session-based recommendations.","Meirui Wang,Pengjie Ren,Lei Mei,Zhumin Chen,Jun Ma,Maarten de Rijke",,2019,,,
851,Sequence and Time Aware Neighborhood for Session-based Recommendations: STAN,"Recent advances in sequence-aware approaches for session-based recommendation, such as those based on recurrent neural networks, highlight the importance of leveraging sequential information from a session while making recommendations. Further, a session based k-nearest-neighbors approach (SKNN) has proven to be a strong baseline for session-based recommendations. However, SKNN does not take into account the readily available sequential and temporal information from sessions. In this work, we propose Sequence and Time Aware Neighborhood (STAN), with vanilla SKNN as its special case. STAN takes into account the following factors for making recommendations: i) position of an item in the current session, ii) recency of a past session w.r.t. to the current session, and iii) position of a recommendable item in a neighboring session. The importance of above factors for a specific application can be adjusted via controllable decay factors. Despite being simple, intuitive and easy to implement, empirical evaluation on three real-world datasets shows that STAN significantly improves over SKNN, and is even comparable to the recently proposed state-of-the-art deep learning approaches. Our results suggest that STAN can be considered as a strong baseline for evaluating session-based recommendation algorithms in future.","Diksha Garg,Priyanka Gupta,Pankaj Malhotra,Lovekesh Vig,Gautam Shroff,Gautam Shroff",,2019,,,
852,Random Vector Functional Link Neural Network based Ensemble Deep Learning,"In this paper, we propose a deep learning framework based on randomized neural network. In particular, inspired by the principles of Random Vector Functional Link (RVFL) network, we present a deep RVFL network (dRVFL) with stacked layers. The parameters of the hidden layers of the dRVFL are randomly generated within a suitable range and kept fixed while the output weights are computed using the closed form solution as in a standard RVFL network. We also propose an ensemble deep network (edRVFL) that can be regarded as a marriage of ensemble learning with deep learning. Unlike traditional ensembling approaches that require training several models independently from scratch, edRVFL is obtained by training a single dRVFL network once. Both dRVFL and edRVFL frameworks are generic and can be used with any RVFL variant. To illustrate this, we integrate the deep learning networks with a recently proposed sparse-pretrained RVFL (SP-RVFL). Extensive experiments on benchmark datasets from diverse domains show the superior performance of our proposed deep RVFL networks.","Rakesh Katuwal,Ponnuthurai Nagaratnam Suganthan,M. Tanveer",arXiv: Computer Vision and Pattern Recognition,2019,,,
853,Multi-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis.,"Liquidation is the process of selling a large number of shares of one stock sequentially within a given time frame, taking into consideration the costs arising from market impact and a trader's risk aversion. The main challenge in optimizing liquidation is to find an appropriate modeling system that can incorporate the complexities of the stock market and generate practical trading strategies. In this paper, we propose to use multi-agent deep reinforcement learning model, which better captures high-level complexities comparing to various machine learning methods, such that agents can learn how to make the best selling decisions. First, we theoretically analyze the Almgren and Chriss model and extend its fundamental mechanism so it can be used as the multi-agent trading environment. Our work builds the foundation for future multi-agent environment trading analysis. Secondly, we analyze the cooperative and competitive behaviours between agents by adjusting the reward functions for each agent, which overcomes the limitation of single-agent reinforcement learning algorithms. Finally, we simulate trading and develop an optimal trading strategy with practical constraints by using a reinforcement learning method, which shows the capabilities of reinforcement learning methods in solving realistic liquidation problems.","Wenhang Bao,Wenhang Bao,Xiao-Yang Liu,Xiao-Yang Liu,Xiao-Yang Liu,Xiao-yang Liu",arXiv: Trading and Market Microstructure,2019,,,
854,Trends and Applications of Machine Learning in Quantitative Finance,"Recent advances in machine learning are finding commercial applications across many industries, not least the finance industry. This paper focuses on applications in one of the core functions of finance, the investment process. This includes return forecasting, risk modelling and portfolio construction. The study evaluates the current state of the art through an extensive review of recent literature. Themes and technologies are identified and classified, and the key use cases highlighted. Quantitative investing, traditionally a leading field in adopting new techniques is found to be the most common source of use cases in the emerging literature.","Sophie Emerson,Ruairí Kennedy,Luke O'Shea,John O'Brien,John O'Brien,John O'Brien,Jack F. O'Brien",,2019,,,
855,How to Evaluate Trading Strategies: Single Agent Market Replay or Multiple Agent Interactive Simulation?,"We show how a multi-agent simulator can support two important but distinct methods for assessing a trading strategy: Market Replay and Interactive Agent-Based Simulation (IABS). Our solution is important because each method offers strengths and weaknesses that expose or conceal flaws in the subject strategy. A key weakness of Market Replay is that the simulated market does not substantially adapt to or respond to the presence of the experimental strategy. IABS methods provide an artificial market for the experimental strategy using a population of background trading agents. Because the background agents attend to market conditions and current price as part of their strategy, the overall market is responsive to the presence of the experimental strategy. Even so, IABS methods have their own weaknesses, primarily that it is unclear if the market environment they provide is realistic. We describe our approach in detail, and illustrate its use in an example application: The evaluation of market impact for various size orders.","Tucker Hybinette Balch,Tucker Balch,Tucker Hybinette Balch,Mahmoud Mahfouz,Joshua Lockhart,Joshua Lockhart,Maria Hybinette,David Byrd",arXiv: Trading and Market Microstructure,2019,,,
856,DEEP LEARNING FOR STOCK MARKET TRADING: A SUPERIOR TRADING STRATEGY?,,"Dušan Fister,Johnathan C. Mun,Johnathan Mun,Vita Jagric,Timotej Jagric",Neural Network World,2019,,,
857,Bayesian regression and Bitcoin,"In this paper, we discuss the method of Bayesian regression and its efficacy for predicting price variation of Bitcoin, a recently popularized virtual, cryptographic currency. Bayesian regression refers to utilizing empirical data as proxy to perform Bayesian inference. We utilize Bayesian regression for the so-called “latent source model”. The Bayesian regression for “latent source model” was introduced and discussed by Chen, Nikolov and Shah [1] and Bresler, Chen and Shah [2] for the purpose of binary classification. They established theoretical as well as empirical efficacy of the method for the setting of binary classification. In this paper, instead we utilize it for predicting real-valued quantity, the price of Bitcoin. Based on this price prediction method, we devise a simple strategy for trading Bitcoin. The strategy is able to nearly double the investment in less than 60 day period when run against real data trace.","Devavrat Shah,Kang Zhang",,2014,,,
858,Very Deep Convolutional Networks for Large-Scale Image Recognition,"Abstract: In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.","Karen Simonyan,Karen Simonyan,Andrew Zisserman",,2015,,,
859,Visualizing the Loss Landscape of Neural Nets,"Neural network training relies on our ability to find ````````""good"" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple ``""filter normalization"" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture effects the loss landscape, and how training parameters affect the shape of minimizers.","Hao Li,Zheng Xu,Gavin Taylor,Tom Goldstein,Christoph Studer",,2018,,,
860,Counterfactual Multi−Agent Policy Gradients,"Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.","Jakob Foerster,Gregory Farquhar,Triantafyllos Afouras,Nantas Nardelli,Shimon Whiteson",,2018,,,
861,Bootstrap Model Aggregation for Distributed Statistical Learning,"In distributed, or privacy-preserving learning, we are often given a set of probabilistic models estimated from different local repositories, and asked to combine them into a single model that gives efficient statistical estimation. A simple method is to linearly average the parameters of the local models, which, however, tends to be degenerate or not applicable on non-convex models, or models with different parameter dimensions. One more practical strategy is to generate bootstrap samples from the local models, and then learn a joint model based on the combined bootstrap set. Unfortunately, the bootstrap procedure introduces additional noise and can significantly deteriorate the performance. In this work, we propose two variance reduction methods to correct the bootstrap noise, including a weighted M-estimator that is both statistically efficient and practically powerful. Both theoretical and empirical analysis is provided to demonstrate our methods.","Jun Han,Qiang Liu,Qiang Liu",,2016,,,
862,Mean Field Multi-Agent Reinforcement Learning,"Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of user interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent's optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution. Experiments on resource allocation, Ising model estimation, and battle game tasks verify the learning effectiveness of our mean field approaches in handling many-agent interactions in population.","Yaodong Yang,Rui Luo,Minne Li,Ming Zhou,Jun Wang,Ming Zhou,Ming Zhou,Ming Zhou,Weinan Zhang,Jun Wang,Jun Wang",,2018,,,
863,Meta-Reinforcement Learning of Structured Exploration Strategies,"Exploration is a fundamental challenge in reinforcement learning (RL). Many current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we study how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise (MAESN) -- to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.","Abhishek Gupta,Abhishek Gupta,Abhishek Gupta,Russell Mendonca,YuXuan Liu,Pieter Abbeel,Sergey Levine,Sergey Levine",,2018,,,
864,OptLayer - Practical Constrained Optimization for Deep Reinforcement Learning in the Real World,"While deep reinforcement learning techniques have recently produced considerable achievements on many decision-making problems, their use in robotics has largely been limited to simulated worlds or restricted motions, since unconstrained trial-and-error interactions in the real world can have undesirable consequences for the robot or its environment. To overcome such limitations, we propose a novel reinforcement learning architecture, OptLayer, that takes as inputs possibly unsafe actions predicted by a neural network and outputs the closest actions that satisfy chosen constraints. While learning control policies often requires carefully crafted rewards and penalties while exploring the range of possible actions, OptLayer ensures that only safe actions are actually executed and unsafe predictions are penalized during training. We demonstrate the effectiveness of our approach on robot reaching tasks, both simulated and in the real world.","Tu-Hoa Pham,Giovanni De Magistris,Ryuki Tachibana",,2018,,,
865,Learning to Reweight Examples for Robust Deep Learning,"Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.","Mengye Ren,Wenyuan Zeng,Bin Yang,Raquel Urtasun",,2018,,,
866,Improved techniques for training GANs,"We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.","Tim Salimans,Ian Goodfellow,Wojciech Zaremba,Vicki Cheung,Alec Radford,Xi Chen,Xi Chen,Xi Chen",,2016,,,
867,Residual networks behave like ensembles of relatively shallow networks,"In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.","Andreas Veit,Michael J. Wilber,Serge Belongie",,2016,,,
868,Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs,"We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures (typically used at the coarse search stage of the most proximity graph techniques). Hierarchical NSW incrementally builds a multi-layer structure consisting of a hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting the search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.","Yu. A. Malkov,Yu. A. Malkov,D. A. Yashunin",IEEE Transactions on Pattern Analysis and Machine Intelligence,2020,,,
869,Prioritized Experience Replay,"Abstract: Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.","Tom Schaul,John Quan,Ioannis Antonoglou,David Silver",,2016,,,
870,QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation,"In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.","Dmitry Kalashnikov,Alex Irpan,Peter Pastor,Julian Ibarz,Alexander Herzog,Eric Jang,Deirdre Quillen,Ethan Holly,Mrinal Kalakrishnan,Vincent Vanhoucke,Sergey Levine",,2018,,,
871,Gradual DropIn of Layers to Train Very Deep Neural Networks,"We introduce the concept of dynamically growing a neural network during training. In particular, an untrainable deep network starts as a trainable shallow network and newly added layers are slowly, organically added during training, thereby increasing the network's depth. This is accomplished by a new layer, which we call DropIn. The DropIn layer starts by passing the output from a previous layer (effectively skipping over the newly added layers), then increasingly including units from the new layers for both feedforward and backpropagation. We show that deep networks, which are untrainable with conventional methods, will converge with DropIn layers interspersed in the architecture. In addition, we demonstrate that DropIn provides regularization during training in an analogous way as dropout. Experiments are described with the MNIST dataset and various expanded LeNet architectures, CIFAR-10 dataset with its architecture expanded from 3 to 11 layers, and on the ImageNet dataset with the AlexNet architecture expanded to 13 layers and the VGG 16-layer architecture.","Leslie N. Smith,Emily M. Hand,Timothy Doster",,2016,,,
872,SVM-Based Deep Stacking Networks,"The deep network model, with the majority built on neural networks, has been proved to be a powerful framework to represent complex data for high performance machine learning. In recent years, more and more studies turn to nonneural network approaches to build diverse deep structures, and the Deep Stacking Network (DSN) model is one of such approaches that uses stacked easy-to-learn blocks to build a parameter-training-parallelizable deep network. In this paper, we propose a novel SVM-based Deep Stacking Network (SVM-DSN), which uses the DSN architecture to organize linear SVM classifiers for deep learning. A BP-like layer tuning scheme is also proposed to ensure holistic and local optimizations of stacked SVMs simultaneously. Some good math properties of SVM, such as the convex optimization, is introduced into the DSN framework by our model. From a global view, SVM-DSN can iteratively extract data representations layer by layer as a deep neural network but with parallelizability, and from a local view, each stacked SVM can converge to its optimal solution and obtain the support vectors, which compared with neural networks could lead to interesting improvements in anti-saturation and interpretability. Experimental results on both image and text data sets demonstrate the excellent performances of SVM-DSN compared with some competitive benchmark models.","Jingyuan Wang,Kai Feng,Kai Feng,Kai Feng,Kai Feng,Junjie Wu,Junjie Wu",,2019,,,
873,Volatility estimation under one-sided errors with applications to limit order books,"For a semi-martingale $X_{t}$, which forms a stochastic boundary, a rate-optimal estimator for its quadratic variation $\langle X,X\rangle_{t}$ is constructed based on observations in the vicinity of $X_{t}$. The problem is embedded in a Poisson point process framework, which reveals an interesting connection to the theory of Brownian excursion areas. We derive $n^{-1/3}$ as optimal convergence rate in a high-frequency framework with $n$ observations (in mean). We discuss a potential application for the estimation of the integrated squared volatility of an efficient price process $X_{t}$ from intra-day order book quotes.","Markus Bibinger,Moritz Jirak,Markus Reiß",Annals of Applied Probability,2016,,,
874,GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning.,"Most Deep Reinforcement Learning (Deep RL) algorithms require a prohibitively large number of training samples for learning complex tasks. Many recent works on speeding up Deep RL have focused on distributed training and simulation. While distributed training is often done on the GPU, simulation is not. In this work, we propose using GPU-accelerated RL simulations as an alternative to CPU ones. Using NVIDIA Flex, a GPU-based physics engine, we show promising speed-ups of learning various continuous-control, locomotion tasks. With one GPU and CPU core, we are able to train the Humanoid running task in less than 20 minutes, using 10-1000x fewer CPU cores than previous works. We also demonstrate the scalability of our simulator to multi-GPU settings to train more challenging locomotion tasks.","Jacky Liang,Viktor Makoviychuk,Ankur Handa,Nuttapong Chentanez,Miles Macklin,Dieter Fox,Dieter Fox",,2018,,,
875,Emergent Complexity via Multi-Agent Competition,"Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty. This work introduces several competitive multi-agent environments where agents compete in a 3D world with simulated physics. The trained agents learn a wide variety of complex and interesting skills, even though the environment themselves are relatively simple. The skills include behaviors such as running, blocking, ducking, tackling, fooling opponents, kicking, and defending using both arms and legs. A highlight of the learned behaviors can be found here: https://goo.gl/eR7fbX","Trapit Bansal,Jakub Pachocki,Szymon Sidor,Ilya Sutskever,Igor Mordatch",,2018,,,
876,VBPR: visual Bayesian Personalized Ranking from implicit feedback,"Modern recommender systems model people and items by discovering or 'teasing apart' the underlying dimensions that encode the properties of items and users' preferences toward them. Critically, such dimensions are uncovered based on user feedback, often in implicit form (such as purchase histories, browsing logs, etc.); in addition, some recommender systems make use of side information, such as product attributes, temporal information, or review text. However one important feature that is typically ignored by existing personalized recommendation and ranking methods is the visual appearance of the items being considered. In this paper we propose a scalable factorization model to incorporate visual signals into predictors of people's opinions, which we apply to a selection of large, real-world datasets. We make use of visual features extracted from product images using (pre-trained) deep networks, on top of which we learn an additional layer that uncovers the visual dimensions that best explain the variation in people's feedback. This not only leads to significantly more accurate personalized ranking methods, but also helps to alleviate cold start issues, and qualitatively to analyze the visual dimensions that influence people's opinions.","Ruining He,Julian McAuley",,2016,,,
877,Multiagent cooperation and competition with deep reinforcement learning,"Evolution of cooperation and competition can appear when multiple adaptive agents share a biological, social, or technological niche. In the present work we study how cooperation and competition emerge between autonomous agents that learn by reinforcement while using only their raw visual input as the state representation. In particular, we extend the Deep Q-Learning framework to multiagent environments to investigate the interaction between two learning agents in the well-known video game Pong. By manipulating the classical rewarding scheme of Pong we show how competitive and collaborative behaviors emerge. We also describe the progression from competitive to collaborative behavior when the incentive to cooperate is increased. Finally we show how learning by playing against another adaptive agent, instead of against a hard-wired algorithm, results in more robust strategies. The present work shows that Deep Q-Networks can become a useful tool for studying decentralized learning of multiagent systems coping with high-dimensional environments.","Ardi Tampuu,Tambet Matiisen,Dorian Kodelja,Dorian Kodelja,Ilya Kuzovkin,Kristjan Korjus,Juhan Aru,Jaan Aru,Raul Vicente",PLOS ONE,2017,,,
878,RepeatNet: A Repeat Aware Neural Recommendation Machine for Session-Based Recommendation,"Recurrent neural networks for session-based recommendation have attracted a lot of attention recently because of their promising performance. repeat consumption is a common phenomenon in many recommendation scenarios (e.g., e-commerce, music, and TV program recommendations), where the same item is re-consumed repeatedly over time. However, no previous studies have emphasized repeat consumption with neural networks. An effective neural approach is needed to decide when to perform repeat recommendation. In this paper, we incorporate a repeat-explore mechanism into neural networks and propose a new model, called RepeatNet, with an encoder-decoder structure. RepeatNet integrates a regular neural recommendation approach in the decoder with a new repeat recommendation mechanism that can choose items from a user’s history and recommends them at the right time. We report on extensive experiments on three benchmark datasets. RepeatNet outperforms state-of-the-art baselines on all three datasets in terms of MRR and Recall. Furthermore, as the dataset size and the repeat ratio increase, the improvements of RepeatNet over the baselines also increase, which demonstrates its advantage in handling repeat recommendation scenarios.","Pengjie Ren,Zhumin Chen,Zhumin Chen,Zhumin Chen,Jing Li,Zhaochun Ren,Jun Ma,Maarten de Rijke",,2019,,,
879,Latent Space Policies for Hierarchical Reinforcement Learning,"We address the problem of learning hierarchical deep neural network policies for reinforcement learning. In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective. Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of that layer. The maximum entropy objective causes these latent variables to be incorporated into the layer's policy, and the higher level layer can directly control the behavior of the lower layer through this latent space. Furthermore, by constraining the mapping from latent variables to actions to be invertible, higher layers retain full expressivity: neither the higher layers nor the lower layers are constrained in their behavior. Our experimental evaluation demonstrates that we can improve on the performance of single-layer policies on standard benchmark tasks simply by adding additional layers, and that our method can solve more complex sparse-reward tasks by learning higher-level policies on top of high-entropy skills optimized for simple low-level objectives.","Tuomas Haarnoja,Kristian Hartikainen,Pieter Abbeel,Sergey Levine",,2018,,,
880,Variance-constrained actor-critic algorithms for discounted and average reward MDPs,"In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in rewards in addition to maximizing a standard criterion. Variance related risk measures are among the most common risk-sensitive criteria in finance and operations research. However, optimizing many such criteria is known to be a hard problem. In this paper, we consider both discounted and average reward Markov decision processes. For each formulation, we first define a measure of variability for a policy, which in turn gives us a set of risk-sensitive criteria to optimize. For each of these criteria, we derive a formula for computing its gradient. We then devise actor-critic algorithms that operate on three timescales--a TD critic on the fastest timescale, a policy gradient (actor) on the intermediate timescale, and a dual ascent for Lagrange multipliers on the slowest timescale. In the discounted setting, we point out the difficulty in estimating the gradient of the variance of the return and incorporate simultaneous perturbation approaches to alleviate this. The average setting, on the other hand, allows for an actor update using compatible features to estimate the gradient of the variance. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in a traffic signal control application.","L A Prashanth,Mohammad Ghavamzadeh,Mohammad Ghavamzadeh",Machine Learning,2016,,,
881,Graph Attention Networks,"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).","Petar Veličković,Guillem Cucurull,Arantxa Casanova,Adriana Romero,Pietro Liò,Yoshua Bengio",,2018,,,
882,Interacting Attention-gated Recurrent Networks for Recommendation,"Capturing the temporal dynamics of user preferences over items is important for recommendation. Existing methods mainly assume that all time steps in user-item interaction history are equally relevant to recommendation, which however does not apply in real-world scenarios where user-item interactions can often happen accidentally. More importantly, they learn user and item dynamics separately, thus failing to capture their joint effects on user-item interactions. To better model user and item dynamics, we present the Interacting Attention-gated Recurrent Network (IARN) which adopts the attention model to measure the relevance of each time step. In particular, we propose a novel attention scheme to learn the attention scores of user and item history in an interacting way, thus to account for the dependencies between user and item dynamics in shaping user-item interactions. By doing so, IARN can selectively memorize different time steps of a user's history when predicting her preferences over different items. Our model can therefore provide meaningful interpretations for recommendation results, which could be further enhanced by auxiliary features. Extensive validation on real-world datasets shows that IARN consistently outperforms state-of-the-art methods.","Wenjie Pei,Jie Yang,Zhu Sun,Jie Zhang,Alessandro Bozzon,David M. J. Tax,David M. J. Tax,David M. J. Tax",,2017,,,
883,A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning,"There has been a resurgence of interest in multiagent reinforcement learning (MARL), due partly to the recent success of deep neural networks. The simplest form of MARL is independent reinforcement learning (InRL), where each agent treats all of its experience as part of its (non stationary) environment. In this paper, we first observe that policies learned using InRL can overfit to the other agents' policies during training, failing to sufficiently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe a meta-algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game theoretic analysis to compute meta-strategies for policy selection. The meta-algorithm generalizes previous algorithms such as InRL, iterated best response, double oracle, and fictitious play. Then, we propose a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in three partially observable settings: gridworld coordination problems, emergent language games, and poker.","Marc Lanctot,Vinicius Zambaldi,Audrunas Gruslys,Angeliki Lazaridou,Karl Tuyls,Julien Perolat,David Silver,Thore Graepel",,2017,,,
884,Sparse deep stacking network for image classification,"Sparse coding can learn good robust representation to noise and model more higher-order representation for image classification. However, the inference algorithm is computationally expensive even though the supervised signals are used to learn compact and discriminative dictionaries in sparse coding techniques. Luckily, a simplified neural network module (SNNM) has been proposed to directly learn the discriminative dictionaries for avoiding the expensive inference. But the SNNM module ignores the sparse representations. Therefore, we propose a sparse SNNM module by adding the mixed-norm regularization (l1/l2 norm). The sparse SNNM modules are further stacked to build a sparse deep stacking network (S-DSN). In the experiments, we evaluate S-DSN with four databases, including Extended YaleB, AR, 15 scene and Caltech101. Experimental results show that our model outperforms related classification methods with only a linear classifier. It is worth noting that we reach 98.8% recognition accuracy on 15 scene.","Jun Li,Heyou Chang,Heyou Chang,Jian Yang,Jian Yang",,2015,,,
885,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,"In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.","Lasse Espeholt,Hubert Soyer,Rémi Munos,Karen Simonyan,Volodymyr Mnih,Vlad Mnih,Tom Ward,Yotam Doron,Yotam Doron,Vlad Firoiu,Tim Harley,Iain Dunning,Shane Legg,Shane Legg,Koray Kavukcuoglu",,2018,,,
886,CatBoost: unbiased boosting with categorical features,"This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.","Liudmila Ostroumova Prokhorenkova,Gleb Gusev,Aleksandr Vorobev,Anna Veronika Dorogush,Andrey Gulin",,2018,,,
887,Neural Attentive Session-based Recommendation,"Given e-commerce scenarios that user profiles are invisible, session-based recommendation is proposed to generate recommendation results from short sessions. Previous work only considers the user's sequential behavior in the current session, whereas the user's main purpose in the current session is not emphasized. In this paper, we propose a novel neural networks framework, i.e., Neural Attentive Recommendation Machine (NARM), to tackle this problem. Specifically, we explore a hybrid encoder with an attention mechanism to model the user's sequential behavior and capture the user's main purpose in the current session, which are combined as a unified session representation later. We then compute the recommendation scores for each candidate item with a bi-linear matching scheme based on this unified session representation. We train NARM by jointly learning the item and session representations as well as their matchings. We carried out extensive experiments on two benchmark datasets. Our experimental results show that NARM outperforms state-of-the-art baselines on both datasets. Furthermore, we also find that NARM achieves a significant improvement on long sessions, which demonstrates its advantages in modeling the user's sequential behavior and main purpose simultaneously.","Jing Li,Pengjie Ren,Zhumin Chen,Zhaochun Ren,Tao Lian,Jun Ma",,2017,,,
888,Exploration by random network distillation,"We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.","Yuri Burda,Harrison Edwards,Amos Storkey,Oleg Klimov",,2019,,,
889,AdaNet: adaptive structural learning of artificial neural networks,"We present new algorithms for adaptively learning artificial neural networks. Our algorithms (ADA NET) adaptively learn both the structure of the network and its weights. They are based on a solid theoretical analysis, including data-dependent generalization guarantees that we prove and discuss in detail. We report the results of large-scale experiments with one of our algorithms on several binary classification tasks extracted from the CIFAR-10 dataset and on the Criteo dataset. The results demonstrate that our algorithm can automatically learn network structures with very competitive performance accuracies when compared with those achieved by neural networks found by standard approaches.","Corinna Cortes,Xavi Gonzalvo,Xavi Gonzalvo,Vitaly Kuznetsov,Mehryar Mohri,Scott Yang",,2017,,,
890,Adam: A Method for Stochastic Optimization,"Abstract: We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.","Diederik P. Kingma,Jimmy Ba",,2015,,,
891,End-to-end training of deep visuomotor policies,"Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.","Sergey Levine,Chelsea Finn,Trevor Darrell,Pieter Abbeel",Journal of Machine Learning Research,2016,,,
892,Incremental Boosting Convolutional Neural Network for Facial Action Unit Recognition,"Recognizing facial action units (AUs) from spontaneous facial expressions is still a challenging problem. Most recently, CNNs have shown promise on facial AU recognition. However, the learned CNNs are often overfitted and do not generalize well to unseen subjects due to limited AU-coded training images. We proposed a novel Incremental Boosting CNN (IB-CNN) to integrate boosting into the CNN via an incremental boosting layer that selects discriminative neurons from the lower layer and is incrementally updated on successive mini-batches. In addition, a novel loss function that accounts for errors from both the incremental boosted classifier and individual weak classifiers was proposed to fine-tune the IB-CNN. Experimental results on four benchmark AU databases have demonstrated that the IB-CNN yields significant improvement over the traditional CNN and the boosting CNN without incremental learning, as well as outperforming the state-of-the-art CNN-based methods in AU recognition. The improvement is more impressive for the AUs that have the lowest frequencies in the databases.","Shizhong Han,Zibo Meng,Ahmed Shehab Khan,Yan Tong",,2016,,,
893,On the Properties of Neural Machine Translation: Encoder--Decoder Approaches,"Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder‐Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.","Kyunghyun Cho,Bart van Merriënboer,Dzmitry Bahdanau,Yoshua Bengio",,2014,,,
894,Evolved Policy Gradients,"We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent's experience. Because this loss is highly flexible in its ability to take into account the agent's history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG's learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.","Rein Houthooft,Richard Y. Chen,Yuhua Chen,Phillip Isola,Bradly C. Stadie,Filip Wolski,Jonathan Ho,OpenAI Jonathan Ho,Pieter Abbeel",,2018,,,
895,Adaboost-based security level classification of mobile intelligent terminals,"With the rapid development of Internet of Things, massive mobile intelligent terminals are ready to access edge servers for real-time data calculation and interaction. However, the risk of private data leakage follows simultaneously. As the administrator of all intelligent terminals in a region, the edge server needs to clarify the ability of the managed intelligent terminals to defend against malicious attacks. Therefore, the security level classification for mobile intelligent terminals before accessing the network is indispensable. In this paper, we firstly propose a safety assessment method to detect the weakness of mobile intelligent terminals. Secondly, we match the evaluation results to the security level. Finally, a scheme of security level classification for mobile intelligent terminals based on Adaboost algorithm is proposed. The experimental results demonstrate that compared to a baseline that statistically calculates the security level, the proposed method can complete the security level classification with lower latency and high accuracy when massive mobile intelligent terminals access the network at the same time.","Feng Wang,Dingde Jiang,Hong Wen,Houbing Song",The Journal of Supercomputing,2019,,,
896,Learning to Detect and Track Visible and Occluded Body Joints in a Virtual World,"Multi-People Tracking in an open-world setting requires a special effort in precise detection. Moreover, temporal continuity in the detection phase gains more importance when scene cluttering introduces the challenging problems of occluded targets. For the purpose, we propose a deep network architecture that jointly extracts people body parts and associates them across short temporal spans. Our model explicitly deals with occluded body parts, by hallucinating plausible solutions of not visible joints. We propose a new end-to-end architecture composed by four branches (visible heatmaps, occluded heatmaps, part affinity fields and temporal affinity fields) fed by a time linker feature extractor. To overcome the lack of surveillance data with tracking, body part and occlusion annotations we created the vastest Computer Graphics dataset for people tracking in urban scenarios by exploiting a photorealistic videogame. It is up to now the vastest dataset (about 500.000 frames, almost 10 million body poses) of human body parts for people tracking in urban scenarios. Our architecture trained on virtual data exhibits good generalization capabilities also on public real tracking benchmarks, when image resolution and sharpness are high enough, producing reliable tracklets useful for further batch data association or re-id modules.","Matteo Fabbri,Matteo Fabbri,Fabio Lanzi,Simone Calderara,Andrea Palazzi,Roberto Vezzani,Rita Cucchiara",,2018,,,
897,Fusing Similarity Models with Markov Chains for Sparse Sequential Recommendation,"Predicting personalized sequential behavior is a key task for recommender systems. In order to predict user actions such as the next product to purchase, movie to watch, or place to visit, it is essential to take into account both long-term user preferences and sequential patterns (i.e., short-term dynamics). Matrix Factorization and Markov Chain methods have emerged as two separate but powerful paradigms for modeling the two respectively. Combining these ideas has led to unified methods that accommodate long-and short-term dynamics simultaneously by modeling pairwise user-item and item-item interactions. In spite of the success of such methods for tackling dense data, they are challenged by sparsity issues, which are prevalent in real-world datasets. In recent years, similarity-based methods have been proposed for (sequentially-unaware) item recommendation with promising results on sparse datasets. In this paper, we propose to fuse such methods with Markov Chains to make personalized sequential recommendations. We evaluate our method, Fossil, on a variety of large, real-world datasets. We show quantitatively that Fossil outperforms alternative algorithms, especially on sparse datasets, and qualitatively that it captures personalized dynamics and is able to make meaningful recommendations.","Ruining He,Julian McAuley",,2016,,,
898,Session-based Recommendations with Recurrent Neural Networks,"Abstract: We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.","Balázs Hidasi,Alexandros Karatzoglou,Linas Baltrunas,Domonkos Tikk",,2016,,,
899,Learning Deep ResNet Blocks Sequentially using Boosting Theory,"We prove a multiclass boosting theory for the ResNet architectures which simultaneously creates a new technique for multiclass boosting and provides a new algorithm for ResNet-style architectures. Our proposed training algorithm, BoostResNet, is particularly suitable in non-differentiable architectures. Our method only requires the relatively inexpensive sequential training of T ""shallow ResNets"". We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline. In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition. A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l_1 norm bounded weights.","Furong Huang,Jordan T. Ash,John Langford,Robert E. Schapire",,2018,,,
900,Ray RLlib: A Composable and Scalable Reinforcement Learning Library,"Reinforcement learning (RL) algorithms involve the deep nesting of distinct components, where each component typically exhibits opportunities for distributed computation. Current RL libraries offer parallelism at the level of the entire program, coupling all the components together and making existing implementations difficult to extend, combine, and reuse. We argue for building composable RL components by encapsulating parallelism and resource requirements within individual components, which can be achieved by building on top of a flexible task-based programming model. We demonstrate this principle by building Ray RLLib on top of Ray and show that we can implement a wide range of state-of-the-art algorithms by composing and reusing a handful of standard components. This composability does not come at the cost of performance --- in our experiments, RLLib matches or exceeds the performance of highly optimized reference implementations. Ray RLLib is available as part of Ray at this https URL","Eric Liang,Richard Liaw,Robert Nishihara,Philipp Moritz,Roy Fox,Joseph E. Gonzalez,Ken Goldberg,Ion Stoica",,2017,,,
901,Stein Variational Adaptive Importance Sampling.,"We propose a novel adaptive importance sampling algorithm which incorporates Stein variational gradient decent algorithm (SVGD) with importance sampling (IS). Our algorithm leverages the nonparametric transforms in SVGD to iteratively decrease the KL divergence between our importance proposal and the target distribution. The advantages of this algorithm are twofold: first, our algorithm turns SVGD into a standard IS algorithm, allowing us to use standard diagnostic and analytic tools of IS to evaluate and interpret the results; second, we do not restrict the choice of our importance proposal to predefined distribution families like traditional (adaptive) IS methods. Empirical experiments demonstrate that our algorithm performs well on evaluating partition functions of restricted Boltzmann machines and testing likelihood of variational auto-encoders.","Jun Han,Qiang Liu,Qiang Liu",,2017,,,
902,Correlation-sensitive next-basket recommendation,"Items adopted by a user over time are indicative of the underlying preferences. We are concerned with learning such preferences from observed sequences of adoptions for recommendation. As multiple items are commonly adopted concurrently, e.g., a basket of grocery items or a sitting of media consumption, we deal with a sequence of baskets as input, and seek to recommend the next basket. Intuitively, a basket tends to contain groups of related items that support particular needs. Instead of recommending items independently for the next basket, we hypothesize that incorporating information on pairwise correlations among items would help to arrive at more coherent basket recommendations. Towards this objective, we develop a hierarchical network architecture codenamed Beacon to model basket sequences. Each basket is encoded taking into account the relative importance of items and correlations among item pairs. This encoding is utilized to infer sequential associations along the basket sequence. Extensive experiments on three public real-life datasets showcase the effectiveness of our approach for the next-basket recommendation problem.","Duc-Trong Le,Hady W. Lauw,Yuan Fang",,2019,,,
903,Modeling multi-purpose sessions for next-item recommendations via mixture-channel purpose routing networks,"A session-based recommender system (SBRS) suggests the next item by modeling the dependencies between items in a session. Most of existing SBRSs assume the items inside a session are associated with one (implicit) purpose. However, this may not always be true in reality, and a session may often consist of multiple subsets of items for different purposes (e.g., breakfast and decoration). Specifically, items (e.g., bread and milk) in a subsethave strong purpose-specific dependencies whereas items (e.g., bread and vase) from different subsets have much weaker or even no dependencies due to the difference of purposes. Therefore, we propose a mixture-channel model to accommodate the multi-purpose item subsets for more precisely representing a session. Filling gaps in existing SBRSs, this model recommends more diverse items to satisfy different purposes. Accordingly, we design effective mixture-channel purpose routing networks (MCPRN) with a purpose routing network to detect the purposes of each item and assign it into the corresponding channels. Moreover, a purpose specific recurrent network is devised to model the dependencies between items within each channel for a specific purpose. The experimental results show the superiority of MCPRN over the state-of-the-art methods in terms of both recommendation accuracy and diversity.","Shoujin Wang,Liang Hu,Yan Wang,Quan Z. Sheng,Mehmet A. Orgun,Longbing Cao",,2019,,,
904,A Survey of Reinforcement Learning Informed by Natural Language,"To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.","Jelena Luketina,Nantas Nardelli,Gregory Farquhar,Jakob Foerster,Jacob Andreas,Edward Grefenstette,Shimon Whiteson,Tim Rocktäschel",,2019,,,
905,Graph contextualized self-attention network for session-based recommendation,"Session-based recommendation, which aims to predict the user's immediate next action based on anonymous sessions, is a key task in many online services (e.g., e-commerce, media streaming). Recently, Self-Attention Network (SAN) has achieved significant success in various sequence modeling tasks without using either recurrent or convolutional network. However, SAN lacks local dependencies that exist over adjacent items and limits its capacity for learning contextualized representations of items in sequences. In this paper, we propose a graph contextualized self-attention model (GC-SAN), which utilizes both graph neural network and self-attention mechanism, for session-based recommendation. In GC-SAN, we dynamically construct a graph structure for session sequences and capture rich local dependencies via graph neural network (GNN). Then each session learns long-range dependencies by applying the self-attention mechanism. Finally, each session is represented as a linear combination of the global preference and the current interest of that session. Extensive experiments on two real-world datasets show that GC-SAN outperforms state-of-the-art methods consistently.","Chengfeng Xu,Pengpeng Zhao,Yanchi Liu,Victor S. Sheng,Jiajie Xu,Fuzhen Zhuang,Junhua Fang,Junhua Fang,Xiaofang Zhou",,2019,,,
906,Deep Stacked Hierarchical Multi-Patch Network for Image Deblurring,"Despite deep end-to-end learning methods have shown their superiority in removing non-uniform motion blur, there still exist major challenges with the current multi-scale and scale-recurrent models: 1) Deconvolution/upsampling operations in the coarse-to-fine scheme result in expensive runtime; 2) Simply increasing the model depth with finer-scale levels cannot improve the quality of deblurring. To tackle the above problems, we present a deep {hierarchical multi-patch network} inspired by Spatial Pyramid Matching to deal with blurry images via a fine-to-coarse hierarchical representation. To deal with the performance saturation w.r.t. depth, we propose a stacked version of our multi-patch model. Our proposed basic multi-patch model achieves the state-of-the-art performance on the GoPro dataset while enjoying a 40$\times$ faster runtime compared to current multi-scale methods. With 30ms to process an image at 1280$\times$720 resolution, it is the first real-time deep motion deblurring model for 720p images at 30fps. For stacked networks, significant improvements (over 1.2dB) are achieved on the GoPro dataset by increasing the network depth. Moreover, by varying the depth of the stacked model, one can adapt the performance and runtime of the same network for different application scenarios.","Hongguang Zhang,Yuchao Dai,Hongdong Li,Hongdong Li,Piotr Koniusz",,2019,,,
907,"Sequential Recommender Systems: Challenges, Progress and Prospects","The emerging topic of sequential recommender systems has attracted increasing attention in recent years.Different from the conventional recommender systems including collaborative filtering and content-based filtering, SRSs try to understand and model the sequential user behaviors, the interactions between users and items, and the evolution of users preferences and item popularity over time. SRSs involve the above aspects for more precise characterization of user contexts, intent and goals, and item consumption trend, leading to more accurate, customized and dynamic recommendations.In this paper, we provide a systematic review on SRSs.We first present the characteristics of SRSs, and then summarize and categorize the key challenges in this research area, followed by the corresponding research progress consisting of the most recent and representative developments on this topic.Finally, we discuss the important research directions in this vibrant area.","Shoujin Wang,Liang Hu,Yan Wang,Longbing Cao,Quan Z. Sheng,Mehmet A. Orgun",,2019,,,
908,Ensemble-Based Deep Reinforcement Learning for Chatbots,"Abstract   Trainable chatbots that exhibit fluent and human-like conversations remain a big challenge in artificial intelligence. Deep Reinforcement Learning (DRL) is promising for addressing this challenge, but its successful application remains an open question. This article describes a novel ensemble-based approach applied to value-based DRL chatbots, which use finite action sets as a form of meaning representation. In our approach, while dialogue actions are derived from sentence clustering, the training datasets in our ensemble are derived from dialogue clustering. The latter aim to induce specialised agents that learn to interact in a particular style. In order to facilitate neural chatbot training using our proposed approach, we assume dialogue data in raw text only – without any manually-labelled data. Experimental results using chitchat data reveal that (1) near human-like dialogue policies can be induced, (2) generalisation to unseen data is a difficult problem, and (3) training an ensemble of chatbot agents is essential for improved performance over using a single agent. In addition to evaluations using held-out data, our results are further supported by a human evaluation that rated dialogues in terms of fluency, engagingness and consistency – which revealed that our proposed dialogue rewards strongly correlate with human judgements.","Heriberto Cuayáhuitl,Donghyeon Lee,Donghyeon Lee,Seonghan Ryu,Yongjin Cho,Sungja Choi,Satish Reddy Indurthi,Seunghak Yu,Hyung-tak Choi,In-Chul Hwang,Jihie Kim,Jihie Kim",Neurocomputing,2019,,,
909,Reinforcement Learning in Financial Markets,"Recently there has been an exponential increase in the use of artificial intelligence for trading in financial markets such as stock and forex. Reinforcement learning has become of particular interest to financial traders ever since the program AlphaGo defeated the strongest human contemporary Go board game player Lee Sedol in 2016. We systematically reviewed all recent stock/forex prediction or trading articles that used reinforcement learning as their primary machine learning method. All reviewed articles had some unrealistic assumptions such as no transaction costs, no liquidity issues and no bid or ask spread issues. Transaction costs had significant impacts on the profitability of the reinforcement learning algorithms compared with the baseline algorithms tested. Despite showing statistically significant profitability when reinforcement learning was used in comparison with baseline models in many studies, some showed no meaningful level of profitability, in particular with large changes in the price pattern between the system training and testing data. Furthermore, few performance comparisons between reinforcement learning and other sophisticated machine/deep learning models were provided. The impact of transaction costs, including the bid/ask spread on profitability has also been assessed. In conclusion, reinforcement learning in stock/forex trading is still in its early development and further research is needed to make it a reliable method in this domain.","Terry Lingze Meng,Matloob Khushi",,2019,,,
910,Deep Robust Reinforcement Learning for Practical Algorithmic Trading,"In algorithmic trading, feature extraction and trading strategy design are two prominent challenges to acquire long-term profits. However, the previously proposed methods rely heavily on domain knowledge to extract handcrafted features and lack an effective way to dynamically adjust the trading strategy. With the recent breakthroughs of deep reinforcement learning (DRL), sequential real-world problems can be modeled and solved with a more human-like approach. In this paper, we propose a novel trading agent, based on deep reinforcement learning, to autonomously make trading decisions and gain profits in the dynamic financial markets. We extend the value-based deep Q-network (DQN) and the asynchronous advantage actor-critic (A3C) for better adapting to the trading market. Specifically, in order to automatically extract robust market representations and resolve the financial time series dependence, we utilize the stacked denoising autoencoders (SDAEs) and the long short-term memory (LSTM) as parts of the function approximator, respectively. Furthermore, we design several elaborate mechanisms to make the trading agent more practical to the real trading environment, such as position-controlled action and n-step reward. The experimental results show that our trading agent outperforms the baselines and achieves stable risk-adjusted returns in both the stock and the futures markets.","Yang Li,Yang Li,Wanshan Zheng,Wanshan Zheng,Wanshan Zheng,Zibin Zheng,Zibin Zheng,Zibin Zheng",IEEE Access,2019,,,
911,Resource Constrained Deep Reinforcement Learning,"In urban environments, resources have to be constantly matched to the “right” locations where customer demand is present. For instance, ambulances have to be matched to base stations regularly so as to reduce response time for emergency incidents in ERS (Emergency Response Systems); vehicles (cars, bikes among others) have to be matched to docking stations to reduce lost demand in shared mobility systems. Such problems are challenging owing to the demand uncertainty, combinatorial action spaces and constraints on allocation of resources (e.g., total resources, minimum and maximum number of resources at locations and regions).Existing systems typically employ myopic and greedy optimization approaches to optimize resource allocation. Such approaches typically are unable to handle surges or variances in demand patterns well. Recent work has demonstrated the ability of Deep RL methods in adapting well to highly uncertain environments. However, existing Deep RL methods are unable to handle combinatorial action spaces and constraints on allocation of resources. To that end, we have developed three approaches on top of the well known actor-critic approach, DDPG (Deep Deterministic Policy Gradient) that are able to handle constraints on resource allocation. We also demonstrate that they are able to outperform leading approaches on simulators validated on semi-real and real data sets.","Abhinav Bhatia,Pradeep Varakantham,Akshat Kumar",,2019,,,
912,CLVSA: a convolutional LSTM based variational sequence-to-sequence model with attention for predicting trends of financial markets,"Financial markets are a complex dynamical system. The complexity comes from the interaction between a market and its participants, in other words, the integrated outcome of activities of the entire participants determines the markets trend, while the markets trend affects activities of participants. These interwoven interactions make financial markets keep evolving. Inspired by stochastic recurrent models that successfully capture variability observed in natural sequential data such as speech and video, we propose CLVSA, a hybrid model that consists of stochastic recurrent networks, the sequence-to-sequence architecture, the self- and inter-attention mechanism, and convolutional LSTM units to capture variationally underlying features in raw financial trading data. Our model outperforms basic models, such as convolutional neural network, vanilla LSTM network, and sequence-to-sequence model with attention, based on backtesting results of six futures from January 2010 to December 2017. Our experimental results show that, by introducing an approximate posterior, CLVSA takes advantage of an extra regularizer based on the Kullback-Leibler divergence to prevent itself from overfitting traps.","Jia Wang,Tong Sun,Benyuan Liu,Yu Cao,Hongwei Zhu",,2019,,,
913,Evaluation of algorithms for Multi-Modality Whole Heart Segmentation: An open-access grand challenge,"Abstract   Knowledge of whole heart anatomy is a prerequisite for many clinical applications. Whole heart segmentation (WHS), which delineates substructures of the heart, can be very valuable for modeling and analysis of the anatomy and functions of the heart. However, automating this segmentation can be challenging due to the large variation of the heart shape, and different image qualities of the clinical data. To achieve this goal, an initial set of training data is generally needed for constructing priors or for training. Furthermore, it is difficult to perform comparisons between different methods, largely due to differences in the datasets and evaluation metrics used. This manuscript presents the methodologies and evaluation results for the WHS algorithms selected from the submissions to the Multi-Modality Whole Heart Segmentation (MM-WHS) challenge, in conjunction with MICCAI 2017. The challenge provided 120 three-dimensional cardiac images covering the whole heart, including 60 CT and 60 MRI volumes, all acquired in clinical environments with manual delineation. Ten algorithms for CT data and eleven algorithms for MRI data, submitted from twelve groups, have been evaluated. The results showed that the performance of CT WHS was generally better than that of MRI WHS. The segmentation of the substructures for different categories of patients could present different levels of challenge due to the difference in imaging and variations of heart shapes. The deep learning (DL)-based methods demonstrated great potential, though several of them reported poor results in the blinded evaluation. Their performance could vary greatly across different network structures and training strategies. The conventional algorithms, mainly based on multi-atlas segmentation, demonstrated good performance, though the accuracy and computational efficiency could be limited. The challenge, including provision of the annotated training data and the blinded evaluation for submitted algorithms on the test data, continues as an ongoing benchmarking resource via its homepage ( www.sdspeople.fudan.edu.cn/zhuangxiahai/0/mmwhs/ ).","Xiahai Zhuang,Lei Li,Christian Payer,Darko Štern,Martin Urschler,Mattias P. Heinrich,Julien Oster,Chunliang Wang,Örjan Smedby,Cheng Bian,Cheng Bian,Xin Yang,Xin Yang,Xin Yang,Pheng-Ann Heng,Aliasghar Mortazi,Ulas Bagci,Guanyu Yang,Chenchen Sun,Gaetan Galisot,Jean-Yves Ramel,Thierry Brouard,Qianqian Tong,Weixin Si,Xiangyun Liao,Xiangyun Liao,Guodong Zeng,Guodong Zeng,Zenglin Shi,Guoyan Zheng,Chengjia Wang,Tom MacGillivray,David E. Newby,Kawal Rhode,Sebastien Ourselin,Raad H. Mohiaddin,Raad Mohiaddin,Jennifer Keegan,David N. Firmin,David N. Firmin,David N. Firmin,Guang Yang,Guang Yang",Medical Image Analysis,2019,,,
914,Risk Management via Anomaly Circumvent: Mnemonic Deep Learning for Midterm Stock Prediction,"Midterm stock price prediction is crucial for value investments in the stock market. However, most deep learning models are essentially short-term and applying them to midterm predictions encounters large cumulative errors because they cannot avoid anomalies. In this paper, we propose a novel deep neural network Mid-LSTM for midterm stock prediction, which incorporates the market trend as hidden states. First, based on the autoregressive moving average model (ARMA), a midterm ARMA is formulated by taking into consideration both hidden states and the capital asset pricing model. Then, a midterm LSTM-based deep neural network is designed, which consists of three components: LSTM, hidden Markov model and linear regression networks. The proposed Mid-LSTM can avoid anomalies to reduce large prediction errors, and has good explanatory effects on the factors affecting stock prices. Extensive experiments on S&P 500 stocks show that (i) the proposed Mid-LSTM achieves 2-4% improvement in prediction accuracy, and (ii) in portfolio allocation investment, we achieve up to 120.16% annual return and 2.99 average Sharpe ratio.","Xinyi Li,Yinchuan Li,Yin-Chuan Li,Xiao-Yang Liu,Xiao-Yang Liu,Christina Dan Wang,Christina Dan Wang",arXiv: Statistical Finance,2019,,,
915,Residual Reinforcement Learning for Robot Control,"Conventional feedback control methods can solve various types of robot control problems very efficiently by capturing the structure with explicit models, such as rigid body equations of motion. However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment. Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts. In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects.","Tobias Johannink,Shikhar Bahl,Ashvin Nair,Jianlan Luo,Avinash Kumar,Avinash Kumar,Avinash Kumar,Avinash Kumar,Matthias Loskyll,Juan Aparicio Ojea,Eugen Solowjow,Sergey Levine,Sergey Levine",,2019,,,
916,FinBrain: when finance meets AI 2.0,"Artificial intelligence (AI) is the core technology of technological revolution and industrial transformation. As one of the new intelligent needs in the AI 2.0 era, financial intelligence has elicited much attention from the academia and industry. In our current dynamic capital market, financial intelligence demonstrates a fast and accurate machine learning capability to handle complex data and has gradually acquired the potential to become a “financial brain.” In this paper, we survey existing studies on financial intelligence. First, we describe the concept of financial intelligence and elaborate on its position in the financial technology field. Second, we introduce the development of financial intelligence and review state-of-the-art techniques in wealth management, risk management, financial security, financial consulting, and blockchain. Finally, we propose a research framework called FinBrain and summarize four open issues, namely, explainable financial agents and causality, perception and prediction under uncertainty, risk-sensitive and robust decision-making, and multi-agent game and mechanism design. We believe that these research directions can lay the foundation for the development of AI 2.0 in the finance field.","Xiaolin Zheng,Mengying Zhu,Mengying Zhu,Qibing Li,Chaochao Chen,Yanchao Tan",Journal of Zhejiang University Science C,2019,,,
917,Advances in Financial Machine Learning,"Machine learning (ML) is changing virtually every aspect of our lives. Today ML algorithms accomplish tasks that until recently only expert humans could perform. As it relates to finance, this is the most exciting time to adopt a disruptive technology that will transform how everyone invests for generations. Readers will learn how to structure Big data in a way that is amenable to ML algorithms; how to conduct research with ML algorithms on that data; how to use supercomputing methods; how to backtest your discoveries while avoiding false positives. The book addresses real-life problems faced by practitioners on a daily basis, and explains scientifically sound solutions using math, supported by code and examples. Readers become active users who can test the proposed solutions in their particular setting. Written by a recognized expert and portfolio manager, this book will equip investment professionals with the groundbreaking tools needed to succeed in modern finance.",Marcos Lopez de Prado,,2018,,,
918,Semi-Supervised Deep Coupled Ensemble Learning With Classification Landmark Exploration,"Using an ensemble of neural networks with consistency regularization is effective for improving performance and stability of deep learning, compared to the case of a single network. In this paper, we present a semi-supervised Deep Coupled Ensemble (DCE) model, which contributes to ensemble learning and classification landmark exploration for better locating the final decision boundaries in the learnt latent space. First, multiple complementary consistency regularizations are integrated into our DCE model to enable the ensemble members to learn from each other and themselves, such that training experience from different sources can be shared and utilized during training. Second, in view of the possibility of producing incorrect predictions on a number of difficult instances, we adopt class-wise mean feature matching to explore important unlabeled instances as classification landmarks, on which the model predictions are more reliable. Minimizing the weighted conditional entropy on unlabeled data is able to force the final decision boundaries to move away from important training data points, which facilitates semi-supervised learning. Ensemble members could eventually have similar performance due to consistency regularization, and thus only one of these members is needed during the test stage, such that the efficiency of our model is the same as the non-ensemble case. Extensive experimental results demonstrate the superiority of our proposed DCE model over existing state-of-the-art semi-supervised learning methods.","Jichang Li,Si Wu,Cheng Liu,Cheng Liu,Cheng Liu,Cheng Liu,Zhiwen Yu,Hau-San Wong",IEEE Transactions on Image Processing,2020,,,
919,Deep Execution - Value and Policy Based Reinforcement Learning for Trading and Beating Market Benchmarks,"In this article we introduce the term ""Deep Execution"" that utilize deep reinforcement learning (DRL) for optimal execution. We demonstrate two different approaches to solve for the optimal execution: (1) the deep double Q-network (DDQN), a value-based approach and (2) the proximal policy optimization (PPO) a policy-based approach, for trading and beating market benchmarks, such as the time-weighted average price (TWAP). We show that, firstly, the DRL can reach the theoretically derived optimum by acting on the environment directly. Secondly, the DRL agents can learn to capitalize on price trends (alpha signals) without directly observing the price. Finally, the DRL can take advantage of the available information to create dynamic strategies as an informed trader and thus outperform static benchmark strategies such as the TWAP.","Kevin Dabérius,Elvin Granat,Patrik Karlsson,Patrik Karlsson",,2019,,,
920,Performance comparison of neural and non-neural approaches to session-based recommendation,"The benefits of neural approaches are undisputed in many application areas. However, today's research practice in applied machine learning---where researchers often use a variety of baselines, datasets, and evaluation procedures---can make it difficult to understand how much progress is actually achieved through novel technical approaches. In this work, we focus on the fast-developing area of session-based recommendation and aim to contribute to a better understanding of what represents the state-of-the-art.   To that purpose, we have conducted an extensive set of experiments, using a variety of datasets, in which we benchmarked four neural approaches that were published in the last three years against each other and against a set of simpler baseline techniques, e.g., based on nearest neighbors. The evaluation of the algorithms under the exact same conditions revealed that the benefits of applying today's neural approaches to session-based recommendations are still limited. In the majority of the cases, and in particular when precision and recall are used, it turned out that simple techniques in most cases outperform recent neural approaches. Our findings therefore point to certain major limitations of today's research practice. By sharing our evaluation framework publicly, we hope that some of these limitations can be overcome in the future.","Malte Ludewig,Noemi Mauro,Sara Latifi,Dietmar Jannach",,2019,,,
921,Predictability limits in session-based next item recommendation,"Session-based recommendations are based on the user's recent actions, for example, the items they have viewed during the current browsing session or the sightseeing places they have just visited. Closely related is sequence-aware recommendation, where the choice of the next item should follow from the sequence of previous actions.   We study seven benchmarks for session-based recommendation, covering retail, music and news domains to investigate how accurately user behavior can be predicted from the session histories. We measure the entropy rate of the data and estimate the limit of predictability to be between 44% and 73% in the included datasets.   We establish some algorithm-specific limits on prediction accuracy for Markov chains, association rules and k-nearest neighbors methods. With most of the analyzed methods, the algorithm design limits their performance with sparse training data. The session based k-nearest neighbors are least restricted in comparison and have room for improvement across all of the analyzed datasets.",Priit Järv,,2019,,,
922,Stock closing price prediction based on sentiment analysis and LSTM,"Stock market prediction has been identified as a very important practical problem in the economic field. However, the timely prediction of the market is generally regarded as one of the most challenging problems due to the stock market’s characteristics of noise and volatility. To address these challenges, we propose a deep learning-based stock market prediction model that considers investors’ emotional tendency. First, we propose to involve investors’ sentiment for stock prediction, which can effectively improve the model prediction accuracy. Second, the stock pricing sequence is a complex time sequence with different scales of fluctuations, making the accurate prediction very challenging. We propose to gradually decompose the complex sequence of stock price by adopting empirical modal decomposition (EMD), which yields better prediction accuracy. Third, we adopt LSTM due to its advantages of analyzing relationships among time-series data through its memory function. We further revised it by adopting attention mechanism to focus more on the more critical information. Experiment results show that the revised LSTM model can not only improve prediction accuracy, but also reduce time delay. It is confirmed that investors’ emotional tendency is effective to improve the predicted results; the introduction of EMD can improve the predictability of inventory sequences; and the attention mechanism can help LSTM to efficiently extract specific information and current mission objectives from the information ocean.","Zhigang Jin,Zhigang Jin,Zhigang Jin,Yang Yang,Yuhong Liu",Neural Computing and Applications,2019,,,
923,Nonlinear Regression via Deep Negative Correlation Learning,"Nonlinear regression has been extensively employed in many computer vision problems (e.g., crowd counting, age estimation, affective computing). Under the umbrella of deep learning, two common solutions exist i) transforming nonlinear regression to a robust loss function which is jointly optimizable with the deep convolutional network, and ii) utilizing ensemble of deep networks. Although some improved performance is achieved, the former may be lacking due to the intrinsic limitation of choosing a single hypothesis and the latter usually suffers from much larger computational complexity. To cope with those issues, we propose to regress via an efficient ""divide and conquer"" manner. The core of our approach is the generalization of negative correlation learning that has been shown, both theoretically and empirically, to work well for non-deep regression problems. Without extra parameters, the proposed method controls the bias-variance-covariance trade-off systematically and usually yields a deep regression ensemble where each base model is both ""accurate"" and ""diversified."" Moreover, we show that each sub-problem in the proposed method has less Rademacher Complexity and thus is easier to optimize. Extensive experiments on several diverse and challenging tasks including crowd counting, personality analysis, age estimation, and image super-resolution demonstrate the superiority over challenging baselines.","Le Zhang,Zenglin Shi,Ming-Ming Cheng,Yun Liu,Jia-Wang Bian,Joey Tianyi Zhou,Guoyan Zheng,Zeng Zeng,Zeng Zeng",IEEE Transactions on Pattern Analysis and Machine Intelligence,2019,,,
924,Residual Networks Behave Like Boosting Algorithms,"We show that Residual Networks (ResNet) is equivalent to boosting feature representation, without any modification to the underlying ResNet training algorithm. A regret bound based on Online Gradient Boosting theory is proved and suggests that ResNet could achieve Online Gradient Boosting regret bounds through neural network architectural changes with the addition of a shrinkage parameter in the identity skip-connections and using residual modules with max-norm bounds. Through this relation between ResNet and Online Boosting, novel feature representation boosting algorithms can be constructed based on altering residual modules. We demonstrate this through proposing decision tree residual modules to construct a new boosted decision tree algorithm and demonstrating generalization error bounds for both approaches; relaxing constraints within BoostResNet algorithm to allow it to be trained in an out-of-core manner. We evaluate convolution ResNet with and without shrinkage modifications to demonstrate its efficacy, and demonstrate that our online boosted decision tree algorithm is comparable to state-of-the-art offline boosted decision tree algorithms without the drawback of offline approaches.",Chapman Siu,arXiv: Machine Learning,2019,,,
925,"Explainable AI: A Brief Survey on History, Research Areas, Approaches and Challenges","Deep learning has made significant contribution to the recent progress in artificial intelligence. In comparison to traditional machine learning methods such as decision trees and support vector machines, deep learning methods have achieved substantial improvement in various prediction tasks. However, deep neural networks (DNNs) are comparably weak in explaining their inference processes and final results, and they are typically treated as a black-box by both developers and users. Some people even consider DNNs (deep neural networks) in the current stage rather as alchemy, than as real science. In many real-world applications such as business decision, process optimization, medical diagnosis and investment recommendation, explainability and transparency of our AI systems become particularly essential for their users, for the people who are affected by AI decisions, and furthermore, for the researchers and developers who create the AI solutions. In recent years, the explainability and explainable AI have received increasing attention by both research community and industry. This paper first introduces the history of Explainable AI, starting from expert systems and traditional machine learning approaches to the latest progress in the context of modern deep learning, and then describes the major research areas and the state-of-art approaches in recent years. The paper ends with a discussion on the challenges and future directions.","Feiyu Xu,Hans Uszkoreit,Hans Uszkoreit,Yangzhou Du,Yangzhou Du,Wei Fan,Dongyan Zhao,Jun Zhu,Jun Zhu,Jun Zhu",,2019,,,
926,Make more digital twins.,"Virtual models boost smart manufacturing by simulating decisions and optimization, from design to operations, explain Fei Tao and Qinglin Qi.  Virtual models boost smart manufacturing by simulating decisions and optimization, from design to operations, explain Fei Tao and Qinglin Qi.","Fei Tao,Fei Tao,Qinglin Qi,Qinglin Qi",Nature,2019,,,
927,Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning,"Hierarchical reinforcement learning has demonstrated significant success at solving difficult reinforcement learning (RL) tasks. Previous works have motivated the use of hierarchy by appealing to a number of intuitive benefits, including learning over temporally extended transitions, exploring over temporally extended periods, and training and exploring in a more semantically meaningful action space, among others. However, in fully observed, Markovian settings, it is not immediately clear why hierarchical RL should provide benefits over standard ""shallow"" RL architectures. In this work, we isolate and evaluate the claimed benefits of hierarchical RL on a suite of tasks encompassing locomotion, navigation, and manipulation. Surprisingly, we find that most of the observed benefits of hierarchy can be attributed to improved exploration, as opposed to easier policy learning or imposed hierarchical structures. Given this insight, we present exploration techniques inspired by hierarchy that achieve performance competitive with hierarchical RL while at the same time being much simpler to use and implement.","Ofir Nachum,Haoran Tang,Xingyu Lu,Shixiang Gu,Honglak Lee,Sergey Levine",arXiv: Learning,2019,,,
928,Bayesian Optimization and Data Science,,"Antonio Candelieri,Antonio Candelieri,Francesco Archetti",,2019,,,
929,ChainSplitter: Towards Blockchain-based Industrial IoT Architecture for Supporting Hierarchical Storage,"The fast developing Industrial Internet of Things (IIoT) technologies provide a promising opportunity to build large-scale systems to connect numerous heterogeneous devices into the Internet. Most existing IIoT infrastructures are based on a centralized architecture, which is easier for management but cannot effectively support immutable and verifiable services among multiple parties. Blockchain technology provides many desired features for large-scale IIoT infrastructures, such as decentralization, trustworthiness, trackability, and immutability. This paper presents a blockchain-based IIoT architecture to support immutable and verifiable services. However, when applying blockchain technology to the IIoT infrastructure, the required storage space posts a grant challenge to resource-constrained IIoT infrastructures. To address the storage issue, this paper proposes a hierarchical blockchain storage structure, ChainSplitter. Specially, the proposed architecture features a hierarchical storage structure where the majority of the blockchain is stored in the clouds, while the most recent blocks are stored in the overlay network of the individual IIoT networks. The proposed architecture seamlessly binds local IIoT networks, the blockchain overlay network, and the cloud infrastructure together through two connectors, the blockchain connector and the cloud connector, to construct the hierarchical blockchain storage. The blockchain connector in the overlay network builds blocks in blockchain from data generated in IIoT networks, and the cloud connector resolves the blockchain synchronization issues between the overlay network and the clouds. We also provide a case study to show the efficiency of the proposed hierarchical blockchain storage in a practical Industrial IoT case.","Gang Wang,Gang Wang,Gang Wang,Gang Wang,Zhijie Jerry Shi,Zhijie Shi,Mark Nixon,Mark J. Nixon,Mark Nixon,Mark J. Nixon,Mark Nixon,Song Han,Song Han,Song Han",arXiv: Cryptography and Security,2019,,,
930,Automatic Human Emotion Classification in Web Document Using Fuzzy Inference System (FIS): Human Emotion Classification,"Textual information mining deals with various information extraction methods that can be evolved from the rapid growth of textual information through human machine interface for analyzing emotions which are taken by a facial expression. The problem of emotions in text is concerned with the fast development of web 2.0 documents that are assigned by users with emotion labels, namely: sadness, surprise, happiness, empathy, anger, warmness, boredom, and amusement. Such emotions can give a new characteristic for document categorization. Textual information mining deals with various information extraction methods that can evolved from the rapid growth of textual information through a human machine interface for analyzing emotions, which are taken by a facial expression. The problem of emotions from text is concerned with the fast development of web 2.0 documents that are assigned by users with emotion labels. Such emotions can give a new characteristic for document categorization.","P. Mohamed Shakeel,S. Baskar",International Journal of Technology and Human Interaction,2020,,,
931,Exploring the effectiveness of STEAM design processes on middle school students’ creativity,"Creativity has an important role in many scientific processes which constitute a large and complex structure. It is difficult to identify and measure. Students’ creativity can be enhanced through specific education programs. The aim of this study was to develop a science, technology, engineering, art, mathematics (STEAM) design process program for teaching 7th grade middle school students to enhance their verbal and figural creativity. The study lasted 11 weeks. Pre-test/post-test quasi-experimental method with a nonequivalent control group was used. Study Group (n = 34) was presented a teaching approach focused on STEAM education, while the control group (n = 34) was taught based on the science curriculum and science textbook. Nine different STEAM design activities were developed. The data were collected with the Torrance Test of Creative Thinking. The SPSS Program was used in analyzing the data. At the end of the study, significant differences were determined in favor of the study group in both verbal and figural creativity. As a result of the study, recommendations for implementation of STEAM design processes were discussed.","Gulbin Ozkan,Unsal Umdu Topsakal",International Journal of Technology and Design Education,2019,,,
932,Unsupervised Ensemble Strategy for Retinal Vessel Segmentation,"Retinal vessel segmentation is a fundamental step in diagnosis for retinal image analysis. Though many segmentation methods are proposed, little research considers how to ensemble their results to fully exploit the advantages of each method. In this work, we propose a novel unsupervised ensemble strategy to automatically combine multiple segmentation results for an accurate result. There is a no-reference network that could assess the vessel segmentation quality without knowing the ground truth. We then optimize the weight of individual result to maximize this segmentation quality score to enhance the final result. Through extensive experiments, our method has shown superior performance over the state-of-the-art on the DRIVE, STARE, CHASE_DB1 datasets.","Bo Liu,Bo Liu,Bo Liu,Lin Gu,Feng Lu,Feng Lu",,2019,,,
933,Stacked autoencoder based deep random vector functional link neural network for classification,"Abstract   Extreme learning machine (ELM), which can be viewed as a variant of Random Vector Functional Link (RVFL) network without the input–output direct connections, has been extensively used to create multi-layer (deep) neural networks. Such networks employ randomization based autoencoders (AE) for unsupervised feature extraction followed by an ELM classifier for final decision making. Each randomization based AE acts as an independent feature extractor and a deep network is obtained by stacking several such AEs. Inspired by the better performance of RVFL over ELM, in this paper, we propose several deep RVFL variants by utilizing the framework of stacked autoencoders. Specifically, we introduce direct connections (feature reuse) from preceding layers to the fore layers of the network as in the original RVFL network. Such connections help to regularize the randomization and also reduce the model complexity. Furthermore, we also introduce denoising criterion, recovering clean inputs from their corrupted versions, in the autoencoders to achieve better higher level representations than the ordinary autoencoders. Extensive experiments on several classification datasets show that our proposed deep networks achieve overall better and faster generalization than the other relevant state-of-the-art deep neural networks.","Rakesh Katuwal,Ponnuthurai Nagaratnam Suganthan",Applied Soft Computing,2019,,,
934,Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning,"Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 6 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.","Tianhe Yu,Deirdre Quillen,Zhanpeng He,Ryan Julian,Avnish Narayan,Karol Hausman,Chelsea Finn,Hayden Shively,Sergey Levine,Adithya Bellathur",arXiv: Learning,2019,,,
935,Using Similarity Metrics on Real World Data and Patient Treatment Pathways to Recommend the Next Treatment.,"Non-small-cell lung cancer (NSCLC) is one of the most prevalent types of lung cancer and continues to have an ominous five year survival rate. Considerable work has been accomplished in analyzing the viability of the treatments offered to NSCLC patients; however, while many of these treatments have performed better over populations of diagnosed NSCLC patients, a specific treatment may not be the most effective therapy for a given patient. Coupling both patient similarity metrics using the Gower similarity metric and prior treatment knowledge, we were able to demonstrate how patient analytics can complement clinical efforts in recommending the next best treatment. Our retrospective and exploratory results indicate that a majority of patients are not recommended the best surviving therapy once they require a new therapy. This investigation lays the groundwork for treatment recommendation using analytics, but more investigation is required to analyze patient outcomes beyond survival.","Kyle Haas,Stuart Morton,Stuart Morton,Simone Gupta,Malika Mahoui",,2019,,,
936,Deep LSTM with Reinforcement Learning Layer for Financial Trend Prediction in FX High Frequency Trading Systems,"High-frequency trading is a method of intervention on the financial markets that uses sophisticated software tools, and sometimes also hardware, with which to implement high-frequency negotiations, guided by mathematical algorithms, that act on markets for shares, options, bonds, derivative instruments, commodities, and so on. HFT strategies have reached considerable volumes of commercial traffic, so much so that it is estimated that they are responsible for most of the transaction traffic of some stock exchanges, with percentages that, in some cases, exceed 70% of the total. One of the main issues of the HFT systems is the prediction of the medium-short term trend. For this reason, many algorithms have been proposed in literature. The author proposes in this work the use of an algorithm based both on supervised Deep Learning and on a Reinforcement Learning algorithm for forecasting the short-term trend in the currency FOREX (FOReign EXchange) market to maximize the return on investment in an HFT algorithm. With an average accuracy of about 85%, the proposed algorithm is able to predict the medium-short term trend of a currency cross based on the historical trend of this and by means of correlation data with other currency crosses using techniques known in the financial field with the term arbitrage. The final part of the proposed pipeline includes a grid trading engine which, based on the aforementioned trend predictions, will perform high frequency operations in order to maximize profit and minimize drawdown. The trading system has been validated over several financial years and on the EUR/USD cross confirming the high performance in terms of Return of Investment (98.23%) in addition to a reduced drawdown (15.97 %) which confirms its financial sustainability.","Francesco Rundo,Francesco Rundo",Applied Sciences,2019,,,
937,Empirical Analysis of Session-Based Recommendation Algorithms,"Recommender systems are tools that support online users by pointing them to potential items of interest in situations of information overload. In recent years, the class of session-based recommendation algorithms received more attention in the research literature. These algorithms base their recommendations solely on the observed interactions with the user in an ongoing session and do not require the existence of long-term preference profiles. Most recently, a number of deep learning based (""neural"") approaches to session-based recommendations were proposed. However, previous research indicates that today's complex neural recommendation methods are not always better than comparably simple algorithms in terms of prediction accuracy. 
With this work, our goal is to shed light on the state-of-the-art in the area of session-based recommendation and on the progress that is made with neural approaches. For this purpose, we compare twelve algorithmic approaches, among them six recent neural methods, under identical conditions on various datasets. We find that the progress in terms of prediction accuracy that is achieved with neural methods is still limited. In most cases, our experiments show that simple heuristic methods based on nearest-neighbors schemes are preferable over conceptually and computationally more complex methods. Observations from a user study furthermore indicate that recommendations based on heuristic methods were also well accepted by the study participants. To support future progress and reproducibility in this area, we publicly share the session-rec evaluation framework that was used in our research.","Malte Ludewig,Noemi Mauro,Sara Latifi,Dietmar Jannach",arXiv: Information Retrieval,2019,,,
938,Digital Dance Ethnography: Organizing Large Dance Collections,"Folk dances often reflect the socio-cultural influences prevailing in different periods and nations; each dance produces a meaning, a story with the help of music, costumes and dance moves. However, dances have no borders; they have been transmitted from generation to generation, along different countries, mainly due to movements of people carrying and disseminating their civilization. Studying the contextual correlation of dances along neighboring countries, unveils the evolution of this unique intangible heritage in time, and helps in understanding potential cultural similarities. In this work we present a method for contextually motion analysis that organizes dance data semantically, to form the first digital dance ethnography. Firstly, we break dance motion sequences into some narrow temporal overlapping feature descriptors, named motion and style words , and then cluster them in a high-dimensional features space to define motifs . The distribution of those motion and style motifs creates motion and style signatures , in the content of a bag-of-motifs representation, that implies for a succinct but descriptive portrayal of motions sequences. Signatures are time-scale and temporal-order invariant, capable of exploiting the contextual correlation between dances, and distinguishing fine-grained difference between semantically similar motions. We then use quartet -based analysis to organize dance data into a categorization tree , while inferred information from dance metadata descriptions are then used to set parent-child relationships. We illustrate a number of different organization trees, and portray the evolution of dances over time. The efficiency of our method is also demonstrated in retrieving contextually similar dances from a database.","Andreas Aristidou,Ariel Shamir,Yiorgos Chrysanthou",ACM Journal on Computing and Cultural Heritage,2020,,,
939,A Multi-Scale Temporal Feature Aggregation Convolutional Neural Network for Portfolio Management,"Financial portfolio management is the process of periodically reallocating a fund into different financial investment products, with the goal of achieving the maximum profits. While conventional financial machine learning methods try to predict the price trends, reinforcement learning based portfolio management methods makes trading decisions according to the price changes directly. However, existing reinforcement learning based methods are limited in extracting the price change information at single-scale level, which makes their performance still not satisfactory. In this paper, inspired by the Inception network that has achieved great success in computer vision and can extract multi-scale features simultaneously, we propose a novel Ensemble of Identical Independent Inception (EI$^3$) convolutional neural network, with the objective of addressing the limitation of existing reinforcement learning based portfolio management methods. With EI$^3$, multiple assets can be processed independently while sharing the same network parameters. Moreover, price movement information for each product can be extracted at multiple scales via wide network and then aggregated to make trading decision. Based on EI$^3$, we further propose a recurrent reinforcement learning framework to provide a deep machine learning solution for the portfolio management problem. Comprehensive experiments on the cryptocurrency datasets demonstrate the superiority of our method over existing competitors, in both upswing and downswing environments.","Si Shi,Jianjun Li,Jia Li,Guohui Li,Peng Pan",,2019,,,
940,A Dynamic Co-attention Network for Session-based Recommendation,"Session-based recommendation is the task of recommending the next item a user might be interested in given partially known session information, e.g., part of a session or recent historical sessions. An effective session-based recommender should be able to exploit a user's evolving preferences, which we assume to be a mixture of her short- and long-term interests. Existing session-based recommendation methods often embed a user's long-term preference into a static representation, which plays a fixed role when dealing with her current short-term interests. This is problematic because long-term preferences may be more or less important for predicting the next conversion depending on the user's short-term interests. We propose a DCN-SR. DCN-SR applies a co-attention network to capture the dynamic interactions between the user's long- and short-term interaction behavior and generates co-dependent representations of the user's long- and short-term interests. For modeling a user's short-term interaction behavior, we design a CGRU network to take actions like ""click'', ""collect'' and ""buy'' into account. Experiments on e-commerce datasets show significant improvements of DCN-SR over state-of-the-art session-based recommendation methods, with improvements of up to 2.58% on the Tmall dataset and 3.08% on the Tianchi dataset in terms of Recall@10. MRR@10 improvements are 3.78% and 4.05%, respectively. We also investigate the scalability and sensitivity of DCN-SR. The improvements of DCN-SR over state-of-the-art baselines are especially noticeable for short sessions and active users with many historical interactions.","Wanyu Chen,Fei Cai,Honghui Chen,Maarten de Rijke",,2019,,,
941,Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks,"Predicting a user's preference in a short anonymous interaction session instead of long-term history is a challenging problem in the real-life session-based recommendation, e.g., e-commerce and media stream. Recent research of the session-based recommender system mainly focuses on sequential patterns by utilizing the attention mechanism, which is straightforward for the session's natural sequence sorted by time. However, the user's preference is much more complicated than a solely consecutive time pattern in the transition of item choices. In this paper, therefore, we study the item transition pattern by constructing a session graph and propose a novel model which collaboratively considers the sequence order and the latent order in the session graph for a session-based recommender system. We formulate the next item recommendation within the session as a graph classification problem. Specifically, we propose a weighted attention graph layer and a Readout function to learn embeddings of items and sessions for the next item recommendation. Extensive experiments have been conducted on two benchmark E-commerce datasets, Yoochoose and Diginetica, and the experimental results show that our model outperforms other state-of-the-art methods.","Ruihong Qiu,Jingjing Li,Zi Huang,Hongzhi Yin",,2019,,,
942,DTCDR: A Framework for Dual-Target Cross-Domain Recommendation,"In order to address the data sparsity problem in recommender systems, in recent years, Cross-Domain Recommendation (CDR) leverages the relatively richer information from a source domain to improve the recommendation performance on a target domain with sparser information. However, each of the two domains may be relatively richer in certain types of information (e.g., ratings, reviews, user profiles, item details, and tags), and thus, if we can leverage such information well, it is possible to improve the recommendation performance on both domains simultaneously (i.e., dual-target CDR), rather than a single target domain only. To this end, in this paper, we propose a new framework, DTCDR, for Dual-Target Cross-Domain Recommendation. In DTCDR, we first extensively utilize rating and multi-source content information to generate rating and document embeddings of users and items. Then, based on Multi-Task Learning (MTL), we design an adaptable embedding-sharing strategy to combine and share the embeddings of common users across domains, with which DTCDR can improve the recommendation performance on both richer and sparser (i.e., dual-target) domains simultaneously. Extensive experiments conducted on real-world datasets demonstrate that DTCDR can significantly improve the recommendation accuracies on both richer and sparser domains and outperform the state-of-the-art single-domain and cross-domain approaches.","Feng Zhu,Chaochao Chen,Yan Wang,Guanfeng Liu,Xiaolin Zheng",,2019,,,
943,Deep Generative Video Compression.,"The usage of deep generative models for image compression has led to impressive performance gains over classical codecs while neural video compression is still in its infancy. Here, we propose an end-to-end, deep generative modeling approach to compress temporal sequences with a focus on video. Our approach builds upon variational autoencoder (VAE) models for sequential data and combines them with recent work on neural image compression. The approach jointly learns to transform the original sequence into a lower-dimensional representation as well as to discretize and entropy code this representation according to predictions of the sequential VAE. Rate-distortion evaluations on small videos from public data sets with varying complexity and diversity show that our model yields competitive results when trained on generic video content. Extreme compression performance is achieved when training the model on specialized content.","Jun Han,Salvator Lombardo,Salvator Lombardo,Christopher Schroers,Christopher Schroers,Stephan Mandt",arXiv: Computer Vision and Pattern Recognition,2019,,,
944,Session-based Recommendation with Hierarchical Memory Networks,"The task of session-based recommendation aims to predict users' future interests based on anonymous historical sessions. Recent works have shown that memory models, which capture user preference from previous interaction sequence with long short-term or short-term memory, can lead to encouraging results in this problem. However, most existing memory models tend to regard each item as a memory unit, which neglect n-gram features and are insufficient to learn the user's feature-level preferences. In this paper, we aim to leverage n-gram features and model users' feature-level preferences in an explicit and effective manner. To this end, we present a memory model with multi-scale feature memory for session-based recommendation. A densely connected convolutional neural network (CNN) with short-cut path between upstream and downstream convolutional blocks is applied to build multi-scale features from item representations, and features in the same scale are combined with memory mechanism to capture users' feature-level preferences. Furthermore, attention is used to adaptively select users' multi-scale feature-level preferences for recommendation. Extensive experiments conducted on two benchmark datasets demonstrate the effectiveness of the proposed model in comparison with competitive baselines.","Bo Song,Yi Cao,Weifeng Zhang,Congfu Xu",,2019,,,
945,Deep Reinforcement Learning in Cryptocurrency Market Making,"This paper sets forth a framework for deep reinforcement learning as applied to market making (DRLMM) for cryptocurrencies. Two advanced policy gradient-based algorithms were selected as agents to interact with an environment that represents the observation space through limit order book data, and order flow arrival statistics. Within the experiment, a forward-feed neural network is used as the function approximator and two reward functions are compared. The performance of each combination of agent and reward function is evaluated by daily and average trade returns. Using this DRLMM framework, this paper demonstrates the effectiveness of deep reinforcement learning in solving stochastic inventory control challenges market makers face.",Jonathan Sadighian,arXiv: Trading and Market Microstructure,2019,,,
946,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model","Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.","Julian Schrittwieser,Ioannis Antonoglou,Thomas Hubert,Karen Simonyan,Laurent Sifre,Simon Schmitt,Arthur Guez,Edward Lockhart,Demis Hassabis,Thore Graepel,Timothy Lillicrap,David Silver,David L. Silver",arXiv: Learning,2019,,,
947,Contextual Hybrid Session-Based News Recommendation With Recurrent Neural Networks,"Recommender systems help users deal with information overload by providing tailored item suggestions to them. The recommendation of news is often considered to be challenging, since the relevance of an article for a user can depend on a variety of factors, including the user's short-term reading interests, the reader's context, or the recency or popularity of an article. Previous work has shown that the use of Recurrent Neural Networks is promising for the next-in-session prediction task, but has certain limitations when only recorded item click sequences are used as input. In this work, we present a contextual hybrid, deep learning based approach for session-based news recommendation that is able to leverage a variety of information types. We evaluated our approach on two public datasets, using a temporal evaluation protocol that simulates the dynamics of a news portal in a realistic way. Our results confirm the benefits of considering additional types of information, including article popularity and recency, in the proposed way, resulting in significantly higher recommendation accuracy and catalog coverage than other session-based algorithms. Additional experiments show that the proposed parameterizable loss function used in our method also allows us to balance two usually conflicting quality factors, accuracy and novelty.","Gabriel de Souza Pereira Moreira,Dietmar Jannach,Adilson Marques da Cunha,Adilson Marques da Cunha",IEEE Access,2019,,,
948,EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning.,"Deep neural networks have been successfully applied to many real-world applications. However, these successes rely heavily on large amounts of labeled data, which is expensive to obtain. Recently, Auto-Encoding Transformation (AET) and MixMatch have been proposed and achieved state-of-the-art results for unsupervised and semi-supervised learning, respectively. In this study, we train an Ensemble of Auto-Encoding Transformations (EnAET) to learn from both labeled and unlabeled data based on the embedded representations by decoding both spatial and non-spatial transformations. This distinguishes EnAET from conventional semi-supervised methods that focus on improving prediction consistency and confidence by different models on both unlabeled and labeled examples. In contrast, we propose to explore the role of self-supervised representations in semi-supervised learning under a rich family of transformations. Experiment results on CIFAR-10, CIFAR-100, SVHN and STL10 demonstrate that the proposed EnAET outperforms the state-of-the-art semi-supervised methods by significant margins. In particular, we apply the proposed method to extremely challenging scenarios with only 10 images per class, and show that EnAET can achieve an error rate of 9.35% on CIFAR-10 and 16.92% on SVHN. In addition, EnAET achieves the best result when compared with fully supervised learning using all labeled data with the same network architecture. The performance on CIFAR-10, CIFAR-100 and SVHN with a smaller network is even more competitive than the state-of-the-art of supervised learning methods based on a larger network. We also set a new performance record with an error rate of 1.99% on CIFAR-10 and 4.52% on STL10. The code and experiment records are released at this https URL.","Xiao Wang,Xiao Wang,Xiao Wang,Xiao Wang,Daisuke Kihara,Jiebo Luo,Jiebo Luo,Guo-Jun Qi,Guo-Jun Qi",arXiv: Computer Vision and Pattern Recognition,2019,,,
949,Deep Generative Video Compression,"The usage of deep generative models for image compression has led to impressive performance gains over classical codecs while neural video compression is still in its infancy. Here, we propose an end-to-end, deep generative modeling approach to compress temporal sequences with a focus on video. Our approach builds upon variational autoencoder (VAE) models for sequential data and combines them with recent work on neural image compression. The approach jointly learns to transform the original sequence into a lower-dimensional representation as well as to discretize and entropy code this representation according to predictions of the sequential VAE. Rate-distortion evaluations on small videos from public data sets with varying complexity and diversity show that our model yields competitive results when trained on generic video content. Extreme compression performance is achieved when training the model on specialized content.","Salvator Lombardo,Salvator Lombardo,Jun Han,Christopher Schroers,Christopher Schroers,Stephan Mandt",,2019,,,
950,Adaptive early classification of temporal sequences using deep reinforcement learning,"Abstract   In this article, we address the problem of early classification (EC) of temporal sequences with adaptive prediction times. We frame EC as a sequential decision making problem and we define a partially observable Markov decision process (POMDP) fitting the competitive objectives of classification earliness and accuracy. We solve the POMDP by training an agent for EC with deep reinforcement learning (DRL). The agent learns to make adaptive decisions between classifying incomplete sequences now or delaying its prediction to gather more measurements. We adapt an existing DRL algorithm for batch and online learning of the agent’s action value function with a deep neural network. We propose strategies of prioritized sampling, prioritized storing and random episode initialization to address the fact that the agent’s memory is unbalanced due to (1): all but one of its actions terminate the process and thus (2): actions of classification are less frequent than the action of delay. In experiments, we show improvements in accuracy induced by our specific adaptation of the algorithm used for online learning of the agent’s action value function. Moreover, we compare two definitions of the POMDP based on delay reward shaping against reward discounting. Finally, we demonstrate that a static naive deep neural network, i.e. trained to classify at static times, is less efficient in terms of accuracy against speed than the equivalent network trained with adaptive decision making capabilities.","Coralie Martinez,Coralie Martinez,Emmanuel Ramasso,Guillaume Perrin,Michèle Rombaut",Knowledge Based Systems,2020,,,
951,Multimodal Deep Learning for Finance: Integrating and Forecasting International Stock Markets,"Stock prices are influenced by numerous factors. We present a method to combine these factors and we validate the method by taking the international stock market as a case study. In today's increasingly international economy, return and volatility spillover effects across international equity markets are major macroeconomic drivers of stock dynamics. Thus, foreign market information is one of the most important factors in forecasting domestic stock prices. However, the cross-correlation between domestic and foreign markets is so complex that it would be extremely difficult to express it explicitly with a dynamical equation. In this study, we develop stock return prediction models that can jointly consider international markets, using multimodal deep learning. Our contributions are three-fold: (1) we visualize the transfer information between South Korea and US stock markets using scatter plots; (2) we incorporate the information into stock prediction using multimodal deep learning; (3) we conclusively show that both early and late fusion models achieve a significant performance boost in comparison with single modality models. Our study indicates that considering international stock markets jointly can improve prediction accuracy, and deep neural networks are very effective for such tasks.","Sang Il Lee,Seong Joon Yoo",The Journal of Supercomputing,2019,,,
952,Stock Price Reactions on NASDAQ Stock Exchange for Special Dividend Announcements,"Announcing dividend pay-out policy by a company will signals market firm’s future prospects and changes its stock prices according to dividend signalling theory. By analysis the effect of special dividend announcements for 5 companies listed in NASDAQ for the period of 2014-2018, this study investigates the stock price reactions to special dividend announcement for 40 days around the event and challenges dividend signalling theory. The empirical results calculated both in discrete and logarithmic forms. Only few disordered significant abnormal returns and average abnormal returns occurred according to the t-test. The results show that shareholders do not gain value from announcement of special dividend in NASDAQ stock exchange market. That Results indicated from adjusted market model in this research do not support dividend-signalling theory Hence do not confirm that the announcement of dividend has significant effect on price of shares. In general the results consistent with the Miller and Modigliani (1961) dividend irrelevance hypothesis.",Arian Seyedimany,,2019,,,
953,Risk-Averse Trust Region Optimization for Reward-Volatility Reduction,"In real-world decision-making problems, for instance in the fields of finance, robotics or autonomous driving, keeping uncertainty under control is as important as maximizing expected returns. Risk aversion has been addressed in the reinforcement learning literature through risk measures related to the variance of returns. However, in many cases, the risk is measured not only on a long-term perspective, but also on the step-wise rewards (e.g., in trading, to ensure the stability of the investment bank, it is essential to monitor the risk of portfolio positions on a daily basis). In this paper, we define a novel measure of risk, which we call reward volatility, consisting of the variance of the rewards under the state-occupancy measure. We show that the reward volatility bounds the return variance so that reducing the former also constrains the latter. We derive a policy gradient theorem with a new objective function that exploits the mean-volatility relationship, and develop an actor-only algorithm. Furthermore, thanks to the linearity of the Bellman equations defined under the new objective function, it is possible to adapt the well-known policy gradient algorithms with monotonic improvement guarantees such as TRPO in a risk-averse manner. Finally, we test the proposed approach in two simulated financial environments.","Lorenzo Bisi,Luca Sabbioni,Edoardo Vittori,Matteo Papini,Marcello Restelli",arXiv: Learning,2019,,,
954,Machine learning for quantitative finance applications: A survey,"The analysis of financial data represents a challenge that researchers had to deal with. The rethinking of the basis of financial markets has led to an urgent demand for developing innovative models to understand financial assets. In the past few decades, researchers have proposed several systems based on traditional approaches, such as autoregressive integrated moving average (ARIMA) and the exponential smoothing model, in order to devise an accurate data representation. Despite their efficacy, the existing works face some drawbacks due to poor performance when managing a large amount of data with intrinsic complexity, high dimensionality and casual dynamicity. Furthermore, these approaches are not suitable for understanding hidden relationships (dependencies) between data. This paper proposes a review of some of the most significant works providing an exhaustive overview of recent machine learning (ML) techniques in the field of quantitative finance showing that these methods outperform traditional approaches. Finally, the paper also presents comparative studies about the effectiveness of several ML-based systems.","Francesco Rundo,Rundo,Francesca Trenta,Trenta,Agatino Luigi Di Stallo,di Stallo,Battiato,Sebastiano Battiato",Applied Sciences,2019,,,
955,Dream to Control: Learning Behaviors by Latent Imagination,"To select effective actions in complex environments, intelligent agents need to generalize from past experience. World models can represent knowledge about the environment to facilitate such generalization. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks purely by latent imagination. We efficiently learn behaviors by backpropagating analytic gradients of learned state values through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.","Danijar Hafner,Timothy Lillicrap,Jimmy Ba,Mohammad Norouzi",,2020,,,
956,SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference,"We present a modern scalable reinforcement learning agent called SEED (Scalable, Efficient Deep-RL). By effectively utilizing modern accelerators, we show that it is not only possible to train on millions of frames per second but also to lower the cost. of experiments compared to current methods. We achieve this with a simple architecture that features centralized inference and an optimized communication layer. SEED adopts two state-of-the-art distributed algorithms, IMPALA/V-trace (policy gradients) and R2D2 (Q-learning), and is evaluated on Atari-57, DeepMind Lab and Google Research Football. We improve the state of the art on Football and are able to reach state of the art on Atari-57 twice as fast in wall-time. For the scenarios we consider, a 40% to 80% cost reduction for running experiments is achieved. The implementation along with experiments is open-sourced so results can be reproduced and novel ideas tried out.","Lasse Espeholt,Raphaël Marinier,Piotr Stanczyk,Ke Wang,Marcin Michalski",,2020,,,
957,Maxmin Q-learning: Controlling the Estimation Bias of Q-learning,"Q-learning suffers from overestimation bias, because it approximates the maximum action value using the maximum estimated action value. Algorithms have been proposed to reduce overestimation bias, but we lack an understanding of how bias interacts with performance, and the extent to which existing algorithms mitigate bias. In this paper, we 1) highlight that the effect of overestimation bias on learning efficiency is environment-dependent; 2) propose a generalization of Q-learning, called \emph{Maxmin Q-learning}, which provides a parameter to flexibly control bias; 3) show theoretically that there exists a parameter choice for Maxmin Q-learning that leads to unbiased estimation with a lower approximation variance than Q-learning; and 4) prove the convergence of our algorithm in the tabular case, as well as convergence of several previous Q-learning variants, using a novel Generalized Q-learning framework. We empirically verify that our algorithm better controls estimation bias in toy environments, and that it achieves superior performance on several benchmark problems.","Qingfeng Lan,Yangchen Pan,Alona Fyshe,Martha White",,2020,,,
958,A Reputation Management Framework for Knowledge-Based and Probabilistic Blockchains,"Recently, leading research communities have been investigating the use of blockchains for Artificial Intelligence (AI) applications, where multiple participants, or agents, collaborate to make consensus decisions. To achieve this, the data in the blockchain storage have to be transformed into blockchain knowledge. We refer to these types of blockchains as knowledge-based blockchains. Knowledge-based blockchains are potentially useful in building efficient risk assessment applications. An earlier work introduced probabilistic blockchain which facilitates knowledge-based blockchains. This paper proposes an extension for the probabilistic blockchain concept. The design of a reputation management framework, suitable for such blockchains, is proposed. The framework has been developed to suit the requirements of a wide range of applications. In particular, we apply it to the detection of malicious nodes and reduce their effect on the probabilistic blockchains' consensus process. We evaluate the framework by comparing it to a baseline using several adversarial strategies. Further, we analyze the collaborative decisions with and without the malicious node detection. Both results show a sustainable performance, where the proposed work outperforms others and achieves excellent results.","Tara Salman,Raj Jain,Lav Gupta",,2019,,,
959,Development of neuro-fuzzy and neuro-bee predictive models for prediction of the safety factor of eco-protection slopes,"Abstract   This study is aimed to investigate the surface eco-protection techniques for cohesive soil slopes along the selected Guthrie Corridor Expressway (GCE) stretch by way of analysing a new set of intelligence techniques namely neuro-bee, artificial neural network (ANN) and neuro-fuzzy. Soil erosion and mass movement which induce landslides have become one of the disasters faced in Selangor, Malaysia causing enormous loss affecting human lives, destruction of property and the environment. Establishing and maintaining slope stability using mechanical structures are costly. Hence, biotechnical slope protection offers an alternative which is not only cost effective but also aesthetically pleasing. To reach the aim of the current study, a field investigations and numerical studies were conducted and a suitable database was prepared and established. By preparing factor of safety (FOS) as a single output parameter and a combination of the most important parameters on that, the desired models have been designed based on training and test patterns. In order to evaluate and compare the prediction precision of the developed models, a series of statistical indices, such as root mean squared error (RMSE), determination coefficient (R-square) and variance account for (VAF) are calculated. Many intelligence models with the most effective parameters on the mentioned models were developed to predict FOS. Based on the simulation results and the measured indices, it was found that the proposed neuro-fuzzy model with the lowest system error and highest R-square performs better as compared to other proposed ANN and neuro-bee models. Therefore, the neuro-fuzzy can provide a new applicable model to effectively predict the FOS of the slopes due to the fact that it is able to combine the advantages of the ANN and fuzzy inference system to indicate a high prediction capacity in solving problem of slope stability.","Maryam Safa,Puteri Azura Sari,Mahdi Shariati,Mahdi Shariati,Mahdi Shariati,Meldi Suhatril,Trung Nguyen-Thoi,Nguyen Thoi Trung,Karzan Wakil,Majid Khorami",Physica A-statistical Mechanics and Its Applications,2020,,,
960,Pseudo Dyna-Q: A Reinforcement Learning Framework for Interactive Recommendation,"Applying reinforcement learning (RL) in recommender systems is attractive but costly due to the constraint of the interaction with real customers, where performing online policy learning through interacting with real customers usually harms customer experiences. A practical alternative is to build a recommender agent offline from logged data, whereas directly using logged data offline leads to the problem of selection bias between logging policy and the recommendation policy. The existing direct offline learning algorithms, such as Monte Carlo methods and temporal difference methods are either computationally expensive or unstable on convergence. To address these issues, we propose Pseudo Dyna-Q (PDQ). In PDQ, instead of interacting with real customers, we resort to a customer simulator, referred to as the World Model, which is designed to simulate the environment and handle the selection bias of logged data. During policy improvement, the World Model is constantly updated and optimized adaptively, according to the current recommendation policy. This way, the proposed PDQ not only avoids the instability of convergence and high computation cost of existing approaches but also provides unlimited interactions without involving real customers. Moreover, a proved upper bound of empirical error of reward function guarantees that the learned offline policy has lower bias and variance. Extensive experiments demonstrated the advantages of PDQ on two real-world datasets against state-of-the-arts methods.","Lixin Zou,Long Xia,Zhuo Zhang,Pan Du,Pan Du,Zhuo Zhang,Ting Bai,Ting Bai,Weidong Liu,Jian-Yun Nie,Dawei Yin",,2020,,,
961,Snapshot boosting: a fast ensemble framework for deep neural networks,"Boosting has been proven to be effective in improving the generalization of machine learning models in many fields. It is capable of getting high-diversity base learners and getting an accurate ensemble model by combining a sufficient number of weak learners. However, it is rarely used in deep learning due to the high training budget of the neural network. Another method named snapshot ensemble can significantly reduce the training budget, but it is hard to balance the tradeoff between training costs and diversity. Inspired by the ideas of snapshot ensemble and boosting, we propose a method named snapshot boosting. A series of operations are performed to get many base models with high diversity and accuracy, such as the use of the validation set, the boosting-based training framework, and the effective ensemble strategy. Last, we evaluate our method on the computer vision (CV) and the natural language processing (NLP) tasks, and the results show that snapshot boosting can get a more balanced trade-off between training expenses and ensemble accuracy than other well-known ensemble methods.","Wentao Zhang,Jiawei Jiang,Yingxia Shao,Bin Cui",Science in China Series F: Information Sciences,2020,,,
962,TrustChain: Trust Management in Blockchain and IoT Supported Supply Chains,"Traceability and integrity are major challenges for the increasingly complex supply chains of today's world. Although blockchain technology has the potential to address these challenges through providing a tamper-proof audit trail of supply chain events and data associated with a product life-cycle, it does not solve the trust problem associated with the data itself. Reputation systems are an effective approach to solve this trust problem. However, current reputation systems are not suited to the blockchain based supply chain applications as they are based on limited observations, they lack granularity and automation, and their overhead has not been explored. In this work, we propose TrustChain, as a three-layered trust management framework which uses a consortium blockchain to track interactions among supply chain participants and to dynamically assign trust and reputation scores based on these interactions. The novelty of Trustchain stems from: (a) the reputation model that evaluates the quality of commodities, and the trustworthiness of entities based on multiple observations of supply chain events, (b) its support for reputation scores that separate between a supply chain participant and products, enabling the assignment of product-specific reputations for the same participant, (c) the use of smart contracts for transparent, efficient, secure, and automated calculation of reputation scores, and (d) its minimal overhead in terms of latency and throughput when compared to a simple blockchain based supply chain model.","Sidra Malik,Volkan Dedeoglu,Salil S. Kanhere,Raja Jurdak",,2019,,,
963,CoDAG: An Efficient and Compacted DAG-Based Blockchain Protocol,"Blockchain is seen as a promising technology to provide reliable and secure services due to its decentralized characteristic. However, because of the limited throughput, current blockchain platforms can not meet the transaction demand in practical use. Though researchers proposed many new solutions, they suffered either decentralization or security issues. In this paper, using Directed Acyclic Graph (DAG) structure, we improve the linear structure of traditional blockchain protocol. In the new structure, blocks are organized in levels and width, which will generate into a compacted DAG structure (CoDAG). To make CoDAG more efficient and secure, we design algorithms and protocols to place the new-generated blocks appropriately. Compared with traditional blockchain protocols, CoDAG improves the security and transaction verification time, and enjoys the consistency and liveness properties of blockchain. Taking adversary parties into consideration, two possible attack strategies are presented in this paper, and we further prove that CoDAG is a secure and robust protocol to resist them. The experimental results show that CoDAG can achieve 394 transactions per second, which is 56 times of Bitcoin's throughput and 26 times of Ethereum's.","Shu Yang,Ziteng Chen,Laizhong Cui,Mingwei Xu,Zhong Ming,Zhongxing Ming,Ke Xu,Ke Xu",,2019,,,
964,Intention Nets: Psychology-Inspired User Choice Behavior Modeling for Next-Basket Prediction,"Human behaviors are complex, which are often observed as a sequence of heterogeneous actions. In this paper, we take user choices for shopping baskets as a typical case to study the complexity of user behaviors. Most of existing approaches often model user behaviors in a mechanical way, namely treating a user action sequence as homogeneous sequential data, such as hourly temperatures, which fails to consider the complexity in user behaviors. In fact, users' choices are driven by certain underlying intentions (e.g., feeding the baby or relieving pain) according to Psychological theories. Moreover, the durations of intentions to drive user actions are quite different; some of them may be persistent while others may be transient. According to Psychological theories, we develop a hierarchical framework to describe the goal, intentions and action sequences, based on which, we design Intention Nets (IntNet). In IntNet, multiple Action Chain Nets are constructed to model the user actions driven by different intentions, and a specially designed Persistent-Transient Intention Unit models the different intention durations. We apply the IntNet to next-basket prediction, a recent challenging task in recommender systems. Extensive experiments on real-world datasets show the superiority of our Psychology-inspired model IntNet over the state-of-the-art approaches.","Shoujin Wang,Liang Hu,Yan Wang,Quan Z. Sheng,Quan Z. Sheng,Mehmet A. Orgun,Longbing Cao",,2020,,,
965,Alpha Discovery Neural Network based on Prior Knowledge.,"In financial automatic feature construction task, genetic programming is the state-of-the-art-technic. It uses reverse polish expression to represent features and then uses genetic programming to simulate the evolution process. With the development of deep learning, there are more powerful feature extractors for option. And we think that comprehending the relationship between different feature extractors and data shall be the key. In this work, we put prior knowledge into alpha discovery neural network, combined with different kinds of feature extractors to do this task. We find that in the same type of network, simple network structure can produce more informative features than sophisticated network structure, and it costs less training time. However, complex network is good at providing more diversified features. In both experiment and real business environment, fully-connected network and recurrent network are good at extracting information from financial time series, but convolution network structure can not effectively extract this information.","Jie Fang,Zhikang Xia,Xiang Liu,Shu-Tao Xia,Shutao Xia,Shutao Xia,Yong Jiang,Yong Jiang,Jianwu Lin,Yong Jiang",arXiv: Statistical Finance,2019,,,
966,Minimax and Biobjective Portfolio Selection Based on Collaborative Neurodynamic Optimization,"Portfolio selection is one of the important issues in financial investments. This article is concerned with portfolio selection based on collaborative neurodynamic optimization. The classic Markowitz mean-variance (MV) framework and its variant mean conditional value-at-risk (CVaR) are formulated as minimax and biobjective portfolio selection problems. Neurodynamic approaches are then applied for solving these optimization problems. For each of the problems, multiple neural networks work collaboratively to characterize the efficient frontier by means of particle swarm optimization (PSO)-based weight optimization. Experimental results with stock data from four major markets show the performance and characteristics of the collaborative neurodynamic approaches to the portfolio optimization problems.","Man-Fai Leung,Man-Fai Leung,Jun Wang,Jun Wang,Jun Wang,Jun Wang",IEEE Transactions on Neural Networks,2020,,,
967,Adaptive-Aggressive Traders Don’t Dominate,"For more than a decade Vytelingum’s Adaptive-Aggressive (AA) algorithm has been recognized as the best-performing automated auction-market trading-agent strategy currently known in the AI/Agents literature; in this paper, we demonstrate that it is in fact routinely outperformed by another algorithm when exhaustively tested across a sufficiently wide range of market scenarios. The novel step taken here is to use large-scale compute facilities to brute-force exhaustively evaluate AA in a variety of market environments based on those used for testing it in the original publications. Our results show that even in these simple environments AA is consistently outperformed by IBM’s GDX algorithm, first published in 2002. We summarize here results from more than one million market simulation experiments, orders of magnitude more testing than was reported in the original publications that first introduced AA. A 2019 ICAART paper by Cliff claimed that AA’s failings were revealed by testing it in more realistic experiments, with conditions closer to those found in real financial markets, but here we demonstrate that even in the simple experiment conditions that were used in the original AA papers, exhaustive testing shows AA to be outperformed by GDX. We close this paper with a discussion of the methodological implications of our work: any results from previous papers where any one trading algorithm is claimed to be superior to others on the basis of only a few thousand trials are probably best treated with some suspicion now. The rise of cloud computing means that the compute-power necessary to subject trading algorithms to millions of trials over a wide range of conditions is readily available at reasonable cost: we should make use of this; exhaustive testing such as is shown here should be the norm in future evaluations and comparisons of new trading algorithms.","Daniel Snashall,Dave Cliff",,2019,,,
968,SAME: sentiment-aware multi-modal embedding for detecting fake news,"How to effectively detect fake news and prevent its diffusion on social media has gained much attention in recent years. However, relatively little focus has been given on exploiting user comments left for posts and latent sentiments therein in detecting fake news. Inspired by the rich information available in user comments on social media, therefore, we investigate whether the latent sentiments hidden in user comments can potentially help distinguish fake news from reliable content. We incorporate users' latent sentiments into an end-to-end deep embedding framework for detecting fake news, named as SAME. First, we use multi-modal networks to deal with heterogeneous data modalities. Second, to learn semantically meaningful spaces per data source, we adopt an adversarial mechanism. Third, we define a novel regularization loss to bring embeddings of relevant pairs closer. Our comprehensive validation using two real-world datasets, PolitiFact and GossipCop, demonstrates the effectiveness of SAME in detecting fake news, significantly outperforming state-of-the-art methods.","Limeng Cui,Suhang Wang,Dongwon Lee,Dongwon Lee,Dongwon Lee",,2019,,,
969,A Multimodal Event-Driven LSTM Model for Stock Prediction Using Online News,"In finance, it is believed that market information, namely, fundamentals and news information, affects stock movements. Such media-aware stock movements essentially comprise a multimodal problem. Two unique challenges arise in processing these multimodal data. First, information from one data mode will interact with information from other data modes. A common strategy is to concatenate various data modes into one compound vector; however, this strategy ignores the interactions among different modes. The second challenge is the heterogeneity of the data in terms of sampling time. Specifically, fundamental data consist of continuous values sampled at fixed time intervals, whereas news information emerges randomly. This heterogeneity can cause valuable information to be partially missing or can distort the feature spaces. In addition, the study of media-aware stock movements in previous work has focused on the one-to-one problem, in which it is assumed that news affects only the performance of the stocks mentioned in the reports. However, news articles also impact related stocks and cause stock co-movements. In this article, we propose a tensor-based event-driven LSTM model to address these challenges. Experiments performed on the China securities market demonstrate the superiority of the proposed approach over state-of-the-art algorithms, including AZFinText, eMAQT, and TeSIA.","Qing Li,Qing Li,Qing Li,Jinghua Tan,Jun Wang,Hsinchun Chen",IEEE Transactions on Knowledge and Data Engineering,2020,,,
970,Residual Networks Behave Like Boosting Algorithms,"We show that Residual Networks (ResNet) is equivalent to boosting feature representation, without any modification to the underlying ResNet training algorithm. A regret bound based on Online Gradient Boosting theory is proved and suggests that ResNet could achieve Online Gradient Boosting regret bounds through neural network architectural changes with the addition of a shrinkage parameter in the identity skip-connections and using residual modules with max-norm bounds. Through this relation between ResNet and Online Boosting, novel feature representation boosting algorithms can be constructed based on altering residual modules. We demonstrate this through proposing decision tree residual modules to construct a new boosted decision tree algorithm and demonstrating generalization error bounds for both approaches; relaxing constraints within BoostResNet algorithm to allow it to be trained in an out-of-core manner. We evaluate convolution ResNet with and without shrinkage modifications to demonstrate its efficacy, and demonstrate that our online boosted decision tree algorithm is comparable to state-of-the-art offline boosted decision tree algorithms without the drawback of offline approaches.",Chapman Siu,,2019,,,
971,Participatory Sensing and Digital Twin City: Updating Virtual City Models for Enhanced Risk-Informed Decision-Making,"AbstractThe benefits of a digital twin city have been assessed based on real-time data collected from preinstalled Internet of Things (IoT) sensors (e.g.,traffic, energy use, air pollution, water ...","Youngjib Ham,Jaeyoon Kim",Journal of Management in Engineering,2020,,,
972,Practical Machine Learning Approach to Capture the Scholar Data Driven Alpha in AI Industry,"AI technologies are helping more and more companies leverage their resources to expand business, reach higher financial performance and become more valuable for investors. However, it is difficult to capture and predict the impacts of AI technologies on companies’ stock prices through traditional financial factors. Moreover, common information sources such as company’s earnings calls and news are not enough to quantity and predict the actual AI premium for a certain company. In this paper, we utilize scholar data as alternative data for trading strategy development and propose a practical machine learning approach to quantity the AI premium of a company and capture the scholar data driven alpha in the AI industry. First, we collect the scholar data from the Microsoft Academic Graph database, and conduct feature engineering based on AI publication and patent data, such as conference/journal publication counts, patent counts, fields of studies and paper citations. Second, we apply machine learning algorithms to weight and re-balance stocks using the scholar data and traditional financial factors every month, and construct portfolios using the “buy-and-hold-long only” strategy. Finally, we evaluate our factor and portfolio in terms of factor performance and portfolio’s cumulative return. The proposed scholar data driven approach achieves a cumulative return of 1029.1% during our backtesting period, which significantly outperforms the Nasdaq 100 index’s 529.5% and S&P 500’s 222.6%. The traditional financial factors approach only leads to 776.7%, which indicates that our scholar data driven approach is better at capturing investment alpha in AI industry than traditional financial factors.","Yunzhe Fang,Xiao-Yang Liu,Xiao-Yang Liu,Xiao-Yang Liu,Hongyang Yang",,2019,,,
973,Deep reinforcement learning for market making in corporate bonds: beating the curse of dimensionality,"ABSTRACTIn corporate bond markets, which are mainly OTC markets, market makers play a central role by providing bid and ask prices for bonds to asset managers. Determining the optimal bid and ask q...","Olivier Guéant,Iuliia Manziuk",Applied Mathematical Finance,2019,,,
974,SMiRL: Surprise Minimizing RL in Entropic Environments,,"Glen Berseth,Daniel Geng,Coline Devin,Dinesh Jayaraman,Chelsea Finn,Sergey Levine",,2019,,,
975,Multiobjective Evolution of Fuzzy Rough Neural Network via Distributed Parallelism for Stock Prediction,"Fuzzy rough theory can describe real-world situations in a mathematically effective and interpretable way, while evolutionary neural networks can be utilized to solve complex problems. Combining them with these complementary capabilities may lead to evolutionary fuzzy rough neural network with the interpretability and prediction capability. In this article, we propose modifications to the existing models of fuzzy rough neural network and then develop a powerful evolutionary framework for fuzzy rough neural networks by inheriting the merits of both the aforementioned systems. We first introduce rough neurons and enhance the consequence nodes, and further integrate the interval type-2 fuzzy set into the existing fuzzy rough neural network model. Thus, several modified fuzzy rough neural network models are proposed. While simultaneously considering the objectives of  prediction precision  and  network simplicity , each model is transformed into a multiobjective optimization problem by encoding the structure, membership functions, and the parameters of the network. To solve these optimization problems, distributed parallel multiobjective evolutionary algorithms are proposed. We enhance the optimization processes with several measures including optimizer replacement and parameter adaption. In the distributed parallel environment, the tedious and time-consuming neural network optimization can be alleviated by numerous computational resources, significantly reducing the computational time. Through experimental verification on complex stock time series prediction tasks, the proposed optimization algorithms and the modified fuzzy rough neural network models exhibit significant improvements the existing fuzzy rough neural network and the long short-term memory network.","Bin Cao,Jianwei Zhao,Zhihan Lv,Yu Gu,Yu Gu,Peng Yang,Saman K. Halgamuge",IEEE Transactions on Fuzzy Systems,2020,,,
976,Man versus Machine: A Comparison of Robo-Analyst and Traditional Research Analyst Investment Recommendations,"Advances in financial technology (FinTech) have revolutionized various product offerings in the financial services industry. One area of particular interest for this technology is the production of investment recommendations. Our study provides the first comprehensive analysis of the properties of investment recommendations generated by “Robo-Analysts,” which are human-analyst-assisted computer programs conducting automated research analysis. Our results indicate that Robo-Analyst recommendations differ from those produced by traditional “human” research analysts across several dimensions. First, Robo-Analysts collectively produce a more balanced distribution of buy, hold, and sell recommendations than do human analysts, consistent with them being less subject to behavioral biases and conflicts of interest. Second, consistent with automation facilitating a greater scale of research production, Robo-Analysts revise their recommendations more frequently than human analysts and also adopt different production processes. Their recommendation revisions rely less on earnings announcements, and more on the large volumes of data released in firms’ annual reports. Third, Robo-Analysts’ recommendation revisions exhibit weaker short-window return reactions, suggesting that investors do not trade on their signals. Importantly, portfolios formed based on the buy recommendations of Robo-Analysts appear to outperform those of human analysts, suggesting that their buy calls are more profitable. Overall, our results suggest that Robo-Analysts are a valuable, alternative information intermediary to traditional sell-side analysts for investment advice.","Braiden Coleman,Kenneth J. Merkley,Joseph Pacelli",,2020,,,
977,Maxmin Q-learning: Controlling the Estimation Bias of Q-learning,"Q-learning suffers from overestimation bias, because it approximates the maximum action value using the maximum estimated action value. Algorithms have been proposed to reduce overestimation bias, but we lack an understanding of how bias interacts with performance, and the extent to which existing algorithms mitigate bias. In this paper, we 1) highlight that the effect of overestimation bias on learning efficiency is environment-dependent; 2) propose a generalization of Q-learning, called \emph{Maxmin Q-learning}, which provides a parameter to flexibly control bias; 3) show theoretically that there exists a parameter choice for Maxmin Q-learning that leads to unbiased estimation with a lower approximation variance than Q-learning; and 4) prove the convergence of our algorithm in the tabular case, as well as convergence of several previous Q-learning variants, using a novel Generalized Q-learning framework. We empirically verify that our algorithm better controls estimation bias in toy environments, and that it achieves superior performance on several benchmark problems.","Qingfeng Lan,Yangchen Pan,Alona Fyshe,Martha White",arXiv: Artificial Intelligence,2020,,,
978,Financial time series forecasting with deep learning : A systematic literature review: 2005–2019,"Abstract   Financial time series forecasting is undoubtedly the top choice of computational intelligence for finance researchers in both academia and the finance industry due to its broad implementation areas and substantial impact. Machine Learning (ML) researchers have created various models, and a vast number of studies have been published accordingly. As such, a significant number of surveys exist covering ML studies on financial time series forecasting. Lately, Deep Learning (DL) models have appeared within the field, with results that significantly outperform their traditional ML counterparts. Even though there is a growing interest in developing models for financial time series forecasting, there is a lack of review papers that solely focus on DL for finance. Hence, the motivation of this paper is to provide a comprehensive literature review of DL studies on financial time series forecasting implementation. We not only categorized the studies according to their intended forecasting implementation areas, such as index, forex, and commodity forecasting, but we also grouped them based on their DL model choices, such as Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), and Long-Short Term Memory (LSTM). We also tried to envision the future of the field by highlighting its possible setbacks and opportunities for the benefit of interested researchers.","Omer Berat Sezer,Mehmet Ugur Gudelek,A. Murat Ozbayoglu,Ahmet Murat Ozbayoglu",Applied Soft Computing,2020,,,
979,Stein Variational Inference for Discrete Distributions.,"Gradient-based approximate inference methods, such as Stein variational gradient descent (SVGD), provide simple and general-purpose inference engines for differentiable continuous distributions. However, existing forms of SVGD cannot be directly applied to discrete distributions. In this work, we fill this gap by proposing a simple yet general framework that transforms discrete distributions to equivalent piecewise continuous distributions, on which the gradient-free SVGD is applied to perform efficient approximate inference. The empirical results show that our method outperforms traditional algorithms such as Gibbs sampling and discontinuous Hamiltonian Monte Carlo on various challenging benchmarks of discrete graphical models. We demonstrate that our method provides a promising tool for learning ensembles of binarized neural network (BNN), outperforming other widely used ensemble methods on learning binarized AlexNet on CIFAR-10 dataset. In addition, such transform can be straightforwardly employed in gradient-free kernelized Stein discrepancy to perform goodness-of-fit (GOF) test on discrete distributions. Our proposed method outperforms existing GOF test methods for intractable discrete distributions.","Jun Han,Jun Han,Fan Ding,Fan Ding,Xianglong Liu,Lorenzo Torresani,Lorenzo Torresani,Jian Peng,Jian Peng,Qiang Liu,Qiang Liu",arXiv: Learning,2020,,,
980,AutoAlpha: an Efficient Hierarchical Evolutionary Algorithm for Mining Alpha Factors in Quantitative Investment,"The multi-factor model is a widely used model in quantitative investment. The success of a multi-factor model is largely determined by the effectiveness of the alpha factors used in the model. This paper proposes a new evolutionary algorithm called AutoAlpha to automatically generate effective formulaic alphas from massive stock datasets. Specifically, first we discover an inherent pattern of the formulaic alphas and propose a hierarchical structure to quickly locate the promising part of space for search. Then we propose a new Quality Diversity search based on the Principal Component Analysis (PCA-QD) to guide the search away from the well-explored space for more desirable results. Next, we utilize the warm start method and the replacement method to prevent the premature convergence problem. Based on the formulaic alphas we discover, we propose an ensemble learning-to-rank model for generating the portfolio. The backtests in the Chinese stock market and the comparisons with several baselines further demonstrate the effectiveness of AutoAlpha in mining formulaic alphas for quantitative trading.","Tianping Zhang,Yuanqi Li,Yuanqi Li,Yifei Jin,Yifei Jin,Jian Li,Jian Li,Jian Li,Jian Li",arXiv: Computational Finance,2020,,,
981,End-to-End Reinforcement Learning for Self-driving Car,"Most of the current self-driving cars make use of multiple algorithms to drive. Furthermore, most of the approaches use supervised learning to train a model to drive the car autonomously. This approach leads to human bias being incorporated into the model. We implement the Deep Q-Learning algorithm to control a simulated car, end-to-end, autonomously. The algorithm is based on reinforcement learning which teaches machines what to do through interactions with the environment. The application of reinforcement learning for driving is of high relevance as it is highly dependent on interactions with the environment. Our model incorporates a CNN as the deep Q network. The system was tested on an open-source car-racing simulator called TORCS. The Deep Q-Learning approach allows the system to be more efficient and robust than a system that has been trained solely through supervised training. Our simulation results show that the system is able to drive autonomously and maneuver complex curves.","Rohan Chopra,Sanjiban Sekhar Roy,Sanjiban Sekhar Roy,Sanjiban Sekhar Roy",,2020,,,
982,Robust Market Making via Adversarial Reinforcement Learning,"We show that adversarial reinforcement learning (ARL) can be used to produce market marking agents that are robust to adversarial and adaptively chosen market conditions. To apply ARL, we turn the well-studied single-agent model of Avellaneda and Stoikov [2008] into a discrete-time zero-sum game between a market maker and adversary, a proxy for other market participants who would like to profit at the market maker's expense. We empirically compare two conventional single-agent RL agents with ARL, and show that our ARL approach leads to: 1) the emergence of naturally risk-averse behaviour without constraints or domain-specific penalties; 2) significant improvements in performance across a set of standard metrics, evaluated with or without an adversary in the test environment, and; 3) improved robustness to model uncertainty. We empirically demonstrate that our ARL method consistently converges, and we prove for several special cases that the profiles that we converge to are Nash equilibria in a corresponding simplified single-stage game.","Thomas Spooner,Rahul Savani",arXiv: Trading and Market Microstructure,2020,,,
983,Dynamic Identification for Representative Building Typologies: Three Case Studies from Bucharest Area,"The paper presents results from an experimental program implemented for three representative buildings in Bucharest metropolitan area and aimed to explore the potential of various dynamic identification methods in providing information about building state changes. The objective is to establish reference values of potential use in rapid earthquake damage detection systems. Each of the selected buildings was designed according to a different seismic code, in force at the time of its construction. The methods employed for this study were: the analysis of Fourier spectra, the analysis of the transfer function and the random decrement technique. To validate the results, the fundamental periods of vibration determined experimentally were compared with the corresponding values predicted by the empirical formulas specified in the corresponding editions of the Romanian seismic code. The results revealed consistent values for both the fundamental period and the damping ratio of the buildings. However, small variations of the two parameters were identified, depending on the time the recordings were performed, noise sources and levels and building occupancy. The results, in terms of validated data on the dynamic characteristics of Romanian building stock and of assessment of methods performance, add up to the information pool needed for the development of countrywide pre- and post-earthquake assisted decision tools.","Alexandru Tiganescu,Alexandru Tiganescu,Bogdan Grecu,Iolanda-Gabriela Craifaleanu,Iolanda Gabriela Craifaleanu",,2020,,,
984,AI-Enabled Blockchain: An Outlier-Aware Consensus Protocol for Blockchain-Based IoT Networks,"A new framework for a secure and robust consensus in blockchain-based IoT networks is proposed using machine learning. Hyperledger fabric, which is a blockchain platform developed as part of the Hyperledger project, though looks very apt for IoT applications, has comparatively low tolerance for malicious activities in an untrustworthy environment. To that end, we propose AI-enabled blockchain (AIBC) with a 2-step consensus protocol that uses an outlier detection algorithm for consensus in an IoT network implemented on hyperledger fabric platform. The outlier-aware consensus protocol exploits a supervised machine learning algorithm which detects anomaly activities via a learned detector in the first step. Then, the transactions go through the inherent Practical Byzantine Fault Tolerance (PBFT) consensus protocol in the hyperledger fabric for ledger update. We measure and report the performance of our framework with respect to the various delay components. Results reveal that our implemented AIBC network (2-step consensus protocol) improves hyperledger fabric performance in terms of fault tolerance by marginally compromising the delay performance.","Mehrdad Salimitari,Mohsen Joneidi,Mainak Chatterjee,Mainak Chatterjee",,2019,,,
985,DeepGait: Planning and Control of Quadrupedal Gaits Using Deep Reinforcement Learning,"This letter addresses the problem of legged locomotion in non-flat terrain. As legged robots such as quadrupeds are to be deployed in terrains with geometries which are difficult to model and predict, the need arises to equip them with the capability to generalize well to unforeseen situations. In this work, we propose a novel technique for training neural-network policies for terrain-aware locomotion, which combines state-of-the-art methods for model-based motion planning and reinforcement learning. Our approach is centered on formulating Markov decision processes using the evaluation of dynamic feasibility criteria in place of physical simulation. We thus employ policy-gradient methods to independently train policies which respectively plan and execute foothold and base motions in 3D environments using both proprioceptive and exteroceptive measurements. We apply our method within a challenging suite of simulated terrain scenarios which contain features such as narrow bridges, gaps and stepping-stones, and train policies which succeed in locomoting effectively in all cases.","Vassilios Tsounis,Mitja Alge,Joonho Lee,Farbod Farshidian,Marco Hutter",,2020,,,
986,Cost-Sensitive Portfolio Selection via Deep Reinforcement Learning,"Portfolio Selection is an important real-world financial task and has attracted extensive attention in artificial intelligence communities. This task, however, has two main difficulties: (i) the non-stationary price series and complex asset correlations make the learning of feature representation very hard; (ii) the practicality principle in financial markets requires controlling both transaction and risk costs. Most existing methods adopt handcraft features and consider no constraints for the costs, which may make them perform unsatisfactorily and fail to control both costs in practice. In this paper, we propose a cost-sensitive portfolio selection method with deep reinforcement learning. Specifically, a novel two-stream portfolio policy network is devised to extract both price series patterns and asset correlations, while a new cost-sensitive reward function is developed to maximize the accumulated return and constrain both costs via reinforcement learning. We theoretically analyze the near-optimality of the proposed reward, which shows that the growth rate of the policy regarding this reward can approach the theoretical optimum. We also empirically evaluate the proposed method on real-world datasets. Promising results demonstrate the effectiveness and superiority of the proposed method in terms of profitability, cost-sensitivity and representation abilities.","Yifan Zhang,Yifan Zhang,Peilin Zhao,Bin Li,Bin Li,Wu Qingyao,Qingyao Wu,Qingyao Wu,Junzhou Huang,Mingkui Tan",IEEE Transactions on Knowledge and Data Engineering,2020,,,
987,The Future of Digital Health with Federated Learning,"Data-driven Machine Learning has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing medical data is not fully exploited by ML primarily because it sits in data silos and privacy concerns restrict access to this data. However, without access to sufficient data, ML will be prevented from reaching its full potential and, ultimately, from making the transition from research to clinical practice. This paper considers key factors contributing to this issue, explores how Federated Learning (FL) may provide a solution for the future of digital health and highlights the challenges and considerations that need to be addressed.","Nicola Rieke,Jonny Hancox,Wenqi Li,Fausto Milletari,Holger R. Roth,Shadi Albarqouni,Spyridon Bakas,Mathieu N. Galtier,Bennett A. Landman,Klaus H. Maier-Hein,Sebastien Ourselin,Micah J. Sheller,Ronald M. Summers,Andrew Trask,Daguang Xu,Maximilian Baust,M. Jorge Cardoso,M. Jorge Cardoso",arXiv: Computers and Society,2020,,,
988,Beyond Clicks: Modeling Multi-Relational Item Graph for Session-Based Target Behavior Prediction,"Session-based target behavior prediction aims to predict the next item to be interacted with specific behavior types (e.g., clicking). Although existing methods for session-based behavior prediction leverage powerful representation learning approaches to encode items’ sequential relevance in a low-dimensional space, they suffer from several limitations. Firstly, they focus on only utilizing the same type of user behavior for prediction, but ignore the potential of taking other behavior data as auxiliary information. This is particularly crucial when the target behavior is sparse but important (e.g., buying or sharing an item). Secondly, item-to-item relations are modeled separately and locally in one behavior sequence, and they lack a principled way to globally encode these relations more effectively. To overcome these limitations, we propose a novel Multi-relational Graph Neural Network model for Session-based target behavior Prediction, namely MGNN-SPred for short. Specifically, we build a Multi-Relational Item Graph (MRIG) based on all behavior sequences from all sessions, involving target and auxiliary behavior types. Based on MRIG, MGNN-SPred learns global item-to-item relations and further obtains user preferences w.r.t. current target and auxiliary behavior sequences, respectively. In the end, MGNN-SPred leverages a gating mechanism to adaptively fuse user representations for predicting next item interacted with target behavior. The extensive experiments on two real-world datasets demonstrate the superiority of MGNN-SPred by comparing with state-of-the-art session-based prediction methods, validating the benefits of leveraging auxiliary behavior and learning item-to-item relations over MRIG.","Wen Wang,Wei Zhang,Wei Zhang,Wei Zhang,Wei Zhang,Wei Zhang,Wei Zhang,Shukai Liu,Qi Liu,Qi Liu,Bo Zhang,Bo Zhang,Bo Zhang,Leyu Lin,Lin Leyu,Leyu Lin,Leyu Lin,Leyu Lin,Leyu Lin,Hongyuan Zha",,2020,,,
989,Attentive Sequential Models of Latent Intent for Next Item Recommendation,"Users exhibit different intents across e-commerce services (e.g.discovering items, purchasing gifts, etc.) which drives them to interact with a wide variety of items in multiple ways (e.g.click, add-to-cart, add-to-favorites, purchase). To give better recommendations, it is important to capture user intent, in addition to considering their historic interactions. However these intents are by definition latent, as we observe only a user’s interactions, and not their underlying intent. To discover such latent intents, and use them effectively for recommendation, in this paper we propose an Attentive Sequential model of Latent Intent (ASLI in short). Our model first learns item similarities from users’ interaction histories via a self-attention layer, then uses a Temporal Convolutional Network layer to obtain a latent representation of the user’s intent from her actions on a particular category. We use this representation to guide an attentive model to predict the next item. Results from our experiments show that our model can capture the dynamics of user behavior and preferences, leading to state-of-the-art performance across datasets from two major e-commerce platforms, namely Etsy and Alibaba.","Mehrab Tanjim,Mehrab Tanjim,Congzhe Su,Congzhe Su,Ethan Benjamin,Diane Hu,Liangjie Hong,Liangjie Hong,Julian McAuley",,2020,,,
990,Domain Adaptive Multi-Modality Neural Attention Network for Financial Forecasting,"Financial time series analysis plays a central role in optimizing investment decision and hedging market risks. This is a challenging task as the problems are always accompanied by dual-level (i.e, data-level and task-level) heterogeneity. For instance, in stock price forecasting, a successful portfolio with bounded risks usually consists of a large number of stocks from diverse domains (e.g, utility, information technology, healthcare, etc.), and forecasting stocks in each domain can be treated as one task; within a portfolio, each stock is characterized by temporal data collected from multiple modalities (e.g, finance, weather, and news), which corresponds to the data-level heterogeneity. Furthermore, the finance industry follows highly regulated processes, which require prediction models to be interpretable, and the output results to meet compliance. Therefore, a natural research question is how to build a model that can achieve satisfactory performance on such multi-modality multi-task learning problems, while being able to provide comprehensive explanations for the end users. To answer this question, in this paper, we propose a generic time series forecasting framework named Dandelion, which leverages the consistency of multiple modalities and explores the relatedness of multiple tasks using a deep neural network. In addition, to ensure the interpretability of the framework, we integrate a novel trinity attention mechanism, which allows the end users to investigate the variable importance over three dimensions (i.e, tasks, modality and time). Extensive empirical results demonstrate that Dandelion achieves superior performance for financial market prediction across 396 stocks from 4 different domains over the past 15 years. In particular, two interesting case studies show the efficacy of Dandelion in terms of its profitability performance, and the interpretability of output results to end users.","Dawei Zhou,Dawei Zhou,Lecheng Zheng,Lecheng Zheng,Yada Zhu,Jianbo Li,Yada Zhu,Jingrui He",,2020,,,
991,Future Data Helps Training: Modeling Future Contexts for Session-based Recommendation,"Session-based recommender systems have attracted much attention recently. To capture the sequential dependencies, existing methods resort either to data augmentation techniques or left-to-right style autoregressive training.Since these methods are aimed to model the sequential nature of user behaviors, they ignore the future data of a target interaction when constructing the prediction model for it. However, we argue that the future interactions after a target interaction, which are also available during training, provide valuable signal on user preference and can be used to enhance the recommendation quality. Properly integrating future data into model training, however, is non-trivial to achieve, since it disobeys machine learning principles and can easily cause data leakage. To this end, we propose a new encoder-decoder framework named Gap-filling based Recommender (GRec), which trains the encoder and decoder by a gap-filling mechanism. Specifically, the encoder takes a partially-complete session sequence (where some items are masked by purpose) as input, and the decoder predicts these masked items conditioned on the encoded representation. We instantiate the general GRec framework using convolutional neural network with sparse kernels, giving consideration to both accuracy and efficiency. We conduct experiments on two real-world datasets covering short-, medium-, and long-range user sessions, showing that GRec significantly outperforms the state-of-the-art sequential recommendation methods. More empirical studies verify the high utility of modeling future contexts under our GRec framework.","Fajie Yuan,Xiangnan He,Xiangnan He,Haochuan Jiang,Guibing Guo,Jian Xiong,Zhezhao Xu,Yilin Xiong",,2020,,,
992,Federated Learning Based Mobile Edge Computing for Augmented Reality Applications,"The past decade has witnessed the prosperous growth of augmented reality (AR) devices, as they provide immersive and interactive experience for customers. AR applications have the properties of high data rate and latency sensitivity. Currently, the available bandwidth is relatively limited to transmit and process enormous generated data. Meanwhile, it is challenging for AR to accurately detect and classify the object in order to perfectly combine the corresponding virtual contents with the real world. In this work, we focus on how to solve the computation efficiency, low-latency object detection and classification problems of AR applications. Firstly, we introduce and analyze the practical mathematical model of AR, and connect the AR operating principles with the object detection and classification problem. To address this problem and reduce the executing latency simultaneously, we propose a framework collaborating mobile edge computing paradigm with federated learning, both of which are decentralized configurations. To evaluate our method, numerical results are calculated based on the open source data CIFAR-10. Compared to centralized learning, our proposed framework requires significantly fewer training iterations.","Dawei Chen,Dawei Chen,Linda Jiang Xie,BaekGyu Kim,Li Wang,Li Wang,Choong Seon Hong,Choong Seon Hong,Li-Chun Wang,Li-Chun Wang,Zhu Han",,2020,,,
993,Deep Learning for Stock Market Prediction.,"Prediction of stock groups' values has always been attractive and challenging for shareholders. This paper concentrates on the future prediction of stock market groups. Four groups named diversified financials, petroleum, non-metallic minerals and basic metals from Tehran stock exchange are chosen for experimental evaluations. Data are collected for the groups based on ten years of historical records. The values predictions are created for 1, 2, 5, 10, 15, 20 and 30 days in advance. The machine learning algorithms utilized for prediction of future values of stock market groups. We employed Decision Tree, Bagging, Random Forest, Adaptive Boosting (Adaboost), Gradient Boosting and eXtreme Gradient Boosting (XGBoost), and Artificial neural network (ANN), Recurrent Neural Network (RNN) and Long short-term memory (LSTM). Ten technical indicators are selected as the inputs into each of the prediction models. Finally, the result of predictions is presented for each technique based on three metrics. Among all the algorithms used in this paper, LSTM shows more accurate results with the highest model fitting ability. Also, for tree-based models, there is often an intense competition between Adaboost, Gradient Boosting, and XGBoost.","Mojtaba Nabipour,Pooyan Nayyeri,Hamed Jabani,Amir Mosavi,Ely Salwana,Shahaboddin Shamshirband,S Shahab",arXiv: Statistical Finance,2020,,,
994,The Adoption of Artificial Intelligence for Financial Investment Service,"Rapid technological advancement has significantly changed the way financial institutions operate and has caused continuing disruption. Artificial Intelligence (AI) provides an opportunity for financial sectors to transform their business processes and to offer innovative services to their customers. This paper aims to study the adoption of AI for financial investment service in Thailand. Data of 400 samples were collected and analysed using multiple linear regression. According to the analysis, factors influencing the use of AI for financial investment service are trust, perceived usefulness, knowledge of using application, and social norm (with p < 0.01).",Wasinee Noonpakdee,,2020,,,
995,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,"We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at this https URL.","Aravind Srinivas,Michael Laskin,Pieter Abbeel",arXiv: Learning,2020,,,
996,"Flash Boys 2.0: Frontrunning in Decentralized Exchanges, Miner Extractable Value, and Consensus Instability","Blockchains, and specifically smart contracts, have promised to create fair and transparent trading ecosystems.Unfortunately, we show that this promise has not been met. We document and quantify the widespread and rising deployment of arbitrage bots in blockchain systems, specifically in decentralized exchanges (or ""DEXes""). Like high-frequency traders on Wall Street, these bots exploit inefficiencies in DEXes, paying high transaction fees and optimizing network latency to frontrun, i.e., anticipate and exploit, ordinary users’ DEX trades.We study the breadth of DEX arbitrage bots in a subset of transactions that yield quantifiable revenue to these bots. We also study bots’ profit-making strategies, with a focus on blockchain-specific elements. We observe bots engage in what we call priority gas auctions (PGAs), competitively bidding up transaction fees in order to obtain priority ordering, i.e., early block position and execution, for their transactions. PGAs present an interesting and complex new continuous-time, partial-information, game-theoretic model that we formalize and study. We release an interactive web portal, frontrun.me, to provide the community with real-time data on PGAs.We additionally show that high fees paid for priority transaction ordering poses a systemic risk to consensus-layer security. We explain that such fees are just one form of a general phenomenon in DEXes and beyond—what we call miner extractable value (MEV)—that poses concrete, measurable, consensus-layer security risks. We show empirically that MEV poses a realistic threat to Ethereum today.Our work highlights the large, complex risks created by transaction-ordering dependencies in smart contracts and the ways in which traditional forms of financial-market exploitation are adapting to and penetrating blockchain economies.","Philip Daian,Steven Goldfeder,Tyler Kell,Yunqi Li,Xueyuan Zhao,Iddo Bentov,Lorenz Breidenbach,Ari Juels",,2020,,,
997,Financial portfolio optimization with online deep reinforcement learning and restricted stacked autoencoder—DeepBreath,"Abstract   The process of continuously reallocating funds into financial assets, aiming to increase the expected return of investment and minimizing the risk, is known as portfolio management. In this paper, a portfolio management framework is developed based on a deep reinforcement learning framework called DeepBreath. The DeepBreath methodology combines a restricted stacked autoencoder and a convolutional neural network (CNN) into an integrated framework. The restricted stacked autoencoder is employed in order to conduct dimensionality reduction and features selection, thus ensuring that only the most informative abstract features are retained. The CNN is used to learn and enforce the investment policy which consists of reallocating the various assets in order to increase the expected return on investment. The framework consists of both offline and online learning strategies: the former is required to train the CNN while the latter handles concept drifts i.e. a change in the data distribution resulting from unforeseen circumstances. These are based on passive concept drift detection and online stochastic batching. Settlement risk may occur as a result of a delay in between the acquisition of an asset and its payment failing to deliver the terms of a contract. In order to tackle this challenging issue, a blockchain is employed. Finally, the performance of the DeepBreath framework is tested with four test sets over three distinct investment periods. The results show that the return of investment achieved by our approach outperforms current expert investment strategies while minimizing the market risk.","Farzan Soleymani,Eric Paquet",Expert Systems With Applications,2020,,,
998,Extending Deep Reinforcement Learning Frameworks in Cryptocurrency Market Making,"There has been a recent surge in interest in the application of artificial intelligence to automated trading. Reinforcement learning has been applied to single- and multi-instrument use cases, such as market making or portfolio management. This paper proposes a new approach to framing cryptocurrency market making as a reinforcement learning challenge by introducing an event-based environment wherein an event is defined as a change in price greater or less than a given threshold, as opposed to by tick or time-based events (e.g., every minute, hour, day, etc.). Two policy-based agents are trained to learn a market making trading strategy using eight days of training data and evaluate their performance using 30 days of testing data. Limit order book data recorded from Bitmex exchange is used to validate this approach, which demonstrates improved profit and stability compared to a time-based approach for both agents when using a simple multi-layer perceptron neural network for function approximation and seven different reward functions.",Jonathan Sadighian,arXiv: Trading and Market Microstructure,2020,,,
999,DEX: A DApp for the Decentralized Marketplace,It is not an overstatement to say the true value of blockchain technology lies in its ability to use a decentralized model of interaction at the protocol level.,Chris Dai,,2020,,,
1000,A CNN–LSTM model for gold price time-series forecasting,"Gold price volatilities have a significant impact on many financial activities of the world. The development of a reliable prediction model could offer insights in gold price fluctuations, behavior and dynamics and ultimately could provide the opportunity of gaining significant profits. In this work, we propose a new deep learning forecasting model for the accurate prediction of gold price and movement. The proposed model exploits the ability of convolutional layers for extracting useful knowledge and learning the internal representation of time-series data as well as the effectiveness of long short-term memory (LSTM) layers for identifying short-term and long-term dependencies. We conducted a series of experiments and evaluated the proposed model against state-of-the-art deep learning and machine learning models. The preliminary experimental analysis illustrated that the utilization of LSTM layers along with additional convolutional layers could provide a significant boost in increasing the forecasting performance.","Ioannis E. Livieris,Emmanuel G. Pintelas,Panayiotis E. Pintelas",Neural Computing and Applications,2020,,,
1001,Precomputing avatar behavior from human motion data,"Creating controllable, responsive avatars is an important problem in computer games and virtual environments. Recently, large collections of motion capture data have been exploited for increased re...","LeeJehee,LeeKang Hoon","Graphical Models \/graphical Models and Image Processing \/computer Vision, Graphics, and Image Processing",2006,,,
1002,MNIST-NET10: A heterogeneous deep networks fusion based on the degree of certainty to reach 0.1% error rate. Ensembles overview and proposal,"Abstract   Ensemble methods have been widely used for improving the results of the best single classification model. A large body of works have achieved better performance mainly by applying one specific ensemble method. However, very few works have explored complex fusion schemes using heterogeneous ensembles with new aggregation strategies. This paper is three-fold: 1) It provides an overview of the most popular ensemble methods, 2) analyzes several fusion schemes using MNIST as guiding thread and 3) introduces MNIST-NET10, a complex heterogeneous fusion architecture based on a degree of certainty aggregation approach; it combines two heterogeneous schemes from the perspective of data, model and fusion strategy. MNIST-NET10 reaches a new record in MNIST with only 10 misclassified images. Our analysis shows that such complex heterogeneous fusion architectures based on the degree of certainty can be considered as a way of taking benefit from diversity.","Siham Tabik,Ricardo F. Alvear-Sandoval,María M. Ruiz,José-Luis Sancho-Gómez,Aníbal R. Figueiras-Vidal,Francisco Herrera,Francisco Herrera",Information Fusion,2020,,,
1003,Multi-Topic Misinformation Blocking With Budget Constraint on Online Social Networks,"Along with the development of Information Technology, Online Social Networks (OSN) are constantly developing and have become popular media in the world. Besides communication enhancement benefits, OSN have such limitations on rapid spread of false information as rumors, fake news, and contradictory news. False information spread is collectively referred to as misinformation which has significant on social communities. The more sources and topics of misinformation are, the greater the number of users are affected. Therefore, it is necessary to prevent the spread of misinformation with multiple topics within a given period of time. In this paper, we propose a Multiple Topics Linear Threshold model for misinformation diffusion, and define a misinformation blocking problem based on this model that takes account of multiple topics and budget constraint. The problem is to find a set of nodes that minimizes the impact of misinformation at an allowed cost when blocking them from the network. We prove that the problem is NP-hard and the time complexity of the objective function calculation is    $\#P$   -hard. We also prove that the objective function is monotone and submodular. We propose an approximation algorithm with approximation ratio    $(1-1/\sqrt {e})$    based on these attributes. For large networks, we propose an extended algorithm by using a tree data structure for quickly updating and calculating the objective function. Experiments conducted on real-world datasets show efficiency and effectiveness of our proposed algorithms in comparison with other state-of-the-art algorithms.","Dung V. Pham,Nguyen Long Giang,Giang L. Nguyen,Giang Nguyen,Tu N. Nguyen,Tu N. Nguyen,Tu N. Nguyen,Tu N. Nguyen,Canh V. Pham,Anh V. Nguyen.,Anh V. Nguyen,Anh V. Nguyen",IEEE Access,2020,,,
1004,A Survey of the Usages of Deep Learning for Natural Language Processing,"Over the last several years, the field of natural language processing has been propelled forward by an explosion in the use of deep learning models. This article provides a brief introduction to the field and a quick overview of deep learning architectures and methods. It then sifts through the plethora of recent studies and summarizes a large assortment of relevant contributions. Analyzed research areas include several core linguistic processing issues in addition to many applications of computational linguistics. A discussion of the current state of the art is then provided along with recommendations for future research in the field.","Daniel W. Otter,Julian Richard Medina,Jugal Kalita",IEEE Transactions on Neural Networks,2020,,,
1005,Reinforcement Learning Generalization with Surprise Minimization.,"Generalization remains a challenging problem for reinforcement learning algorithms, which are often trained and tested on the same set of environments. When test environments are perturbed but the task is semantically the same, agents can still fail to perform accurately. Particularly when they are trained on high-dimensional state spaces, such as images. We evaluate an surprise minimizing agent on a generalization benchmark to show an additional reward learned from a density model can help agents acquire robust skills on unseen procedurally generated diverse environments.",Jerry Zikun Chen,arXiv: Learning,2020,,,
1006,Thresholded ConvNet ensembles: neural networks for technical forecasting,"Much of modern practice in financial forecasting relies on technicals, an umbrella term for several heuristics applying visual pattern recognition to price charts. Despite its ubiquity in financial media, the reliability of its signals remains a contentious and highly subjective form of 'domain knowledge'. We investigate the predictive value of patterns in financial time series, applying machine learning and signal processing techniques to 22 years of US equity data. By reframing technical analysis as a poorly specified, arbitrarily preset feature-extractive layer in a deep neural network, we show that better convolutional filters can be learned directly from the data, and provide visual representations of the features being identified. We find that an ensemble of shallow, thresholded CNNs optimised over different resolutions achieves state-of-the-art performance on this domain, outperforming technical methods while retaining some of their interpretability.","Sid Ghoshal,Stephen J. Roberts",Neural Computing and Applications,2020,,,
1007,Empirical Asset Pricing via Machine Learning,"We synthesize the field of machine learning with the canonical problem of empirical asset pricing: measuring asset risk premia. In the familiar empirical setting of cross section and time series stock return prediction, we perform a comparative analysis of methods in the machine learning repertoire, including generalized linear models, dimension reduction, boosted regression trees, random forests, and neural networks. At the broadest level, we find that machine learning offers an improved description of expected return behavior relative to traditional forecasting methods. Our implementation establishes a new standard for accuracy in measuring risk premia summarized by an unprecedented out-of-sample return prediction R2. We identify the best performing methods (trees and neural nets) and trace their predictive gains to allowance of nonlinear predictor interactions that are missed by other methods. Lastly, we find that all methods agree on the same small set of dominant predictive signals that includes variations on momentum, liquidity, and volatility. Improved risk premia measurement through machine learning can simplify the investigation into economic mechanisms of asset pricing and justifies its growing role in innovative financial technologies.","Shihao Gu,Bryan T. Kelly,Dacheng Xiu",National Bureau of Economic Research,2018,,,
1008,"Federated Learning: Challenges, Methods, and Future Directions","Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.","Tian Li,Anit Kumar Sahu,Ameet Talwalkar,Virginia Smith",IEEE Signal Processing Magazine,2020,,,
1009,Deep Learning for Financial Applications : A Survey,"Computational intelligence in finance has been a very popular topic for both academia and financial industry in the last few decades. Numerous studies have been published resulting in various models. Meanwhile, within the Machine Learning (ML) field, Deep Learning (DL) started getting a lot of attention recently, mostly due to its outperformance over the classical models. Lots of different implementations of DL exist today, and the broad interest is continuing. Finance is one particular area where DL models started getting traction, however, the playfield is wide open, a lot of research opportunities still exist. In this paper, we tried to provide a state-of-the-art snapshot of the developed DL models for financial applications, as of today. We not only categorized the works according to their intended subfield in finance but also analyzed them based on their DL models. In addition, we also aimed at identifying possible future implementations and highlighted the pathway for the ongoing research within the field.","A. Murat Ozbayoglu,Ahmet Murat Ozbayoglu,Mehmet Ugur Gudelek,Omer Berat Sezer",Applied Soft Computing,2020,,,
1010,Twin-Delayed DDPG: A Deep Reinforcement Learning Technique to Model a Continuous Movement of an Intelligent Robot Agent,"In this current research, Twin-Delayed DDPG (TD3) algorithm has been used to solve the most challenging virtual Artificial Intelligence application by training a 4-ant-legged robot as an Intelligent Agent to run across a field. Twin-Delayed DDPG (TD3) is an incredibly smart AI model of a Deep Reinforcement Learning which combines the state-of-the-art methods in Artificial Intelligence. These includes Policy gradient, Actor-Critics, and continuous Double Deep Q-Learning. These Deep Reinforcement Learning approaches trained an Intelligent agent to interact with an environment with automatic feature engineering, that is, necessitating minimal domain knowledge. For the implementation of the TD3, we used a two-layer feedforward neural network of 400 and 300 hidden nodes respectively, with Rectified Linear Units (ReLU) as an activation function between each layer for both the Actor and Critics. We, then added a final tanh unit after the output of the Actor. The Critic receives both the state and action as input to the first layer. Both the network parameters were updated using Adam optimizer. The idea behind the Twin-Delayed DDPG (TD3) is to reduce overestimation bias in Deep Q-Learning with discrete actions which are ineffective in an Actor-Critic domain setting. Based on the Maximum Average Reward over the evaluation time-step, our model achieved an approximate maximum of 2364. Therefore, we can truly say that, TD3 has obviously improved on both the learning speed and performance of the Deep Deterministic Policy Gradient (DDPG) in a challenging environment in a continuous control domain.","Stephen Dankwa,Wenfeng Zheng,Wenfeng Zheng,Wenfeng Zheng",,2019,,,
1011,An Adaptive Master-Slave Regularized Model for Unexpected Revenue Prediction Enhanced with Alternative Data,"Revenue prediction is an essential component in security analysis since the revenue of a company has a great impact on the performance of its stock. For investment, one of the most valuable pieces of information is the company’s unexpected revenue, which is the difference between the officially reported revenue and the consensus estimate for revenue predicted by analysts. Since it is the unexpected revenue that indicates something exceeding or under analysts’ expectation, it is an indispensable factor that influences the performance of a stock. Besides conventional trading data from stock market and companies’ financial reports, recent years have witnessed an extensive application of alternative data for gaining an information edge in stock investment.In this paper, we study the challenging problem of better predicting unexpected revenue of a company via machine learning with alternative data. To the best of our knowledge, this is the first work studying this problem in literature. However, it is nontrivial to quantitatively model the relations between the unexpected revenue and the information provided by alternative data with a machine learning approach. Thus we proposed an adaptive master-slave regularized model, called AMS for short, to effectively leverage alternative data for unexpected revenue prediction. AMS first trains a master model upon a company graph, which captures the relations among companies, using a graph neural network (GNN). Then for a target company, the master model generates an adaptive slave-model, which is specially optimized for this target company. Finally, we use this slave-model to predict the unexpected revenue of the target company. Besides its excellent prediction performance, another critical advantage of our AMS model lies in its superior interpretability, which is crucial for portfolio managers to understand the predicted results. With extensive experiments using two real-world alternative datasets, we have demonstrated the effectiveness of our model against a set of competitors.","Jin Xu,Jin Xu,Jin Xu,Jin Xu,Jin Xu,Jingbo Zhou,Jingbo Zhou,Jingbo Zhou,Yongpo Jia,Jian Li,Jian Li,Jian Li,Xiong Hui",,2020,,,
1012,Exploiting Cross-session Information for Session-based Recommendation with Graph Neural Networks,"Different from the traditional recommender system, the session-based recommender system introduces the concept of the session, i.e., a sequence of interactions between a user and multiple items within a period, to preserve the user's recent interest. The existing work on the session-based recommender system mainly relies on mining sequential patterns within individual sessions, which are not expressive enough to capture more complicated dependency relationships among items. In addition, it does not consider the cross-session information due to the anonymity of the session data, where the linkage between different sessions is prevented. In this article, we solve these problems with the graph neural networks technique. First, each session is represented as a graph rather than a linear sequence structure, based on which a novel Full Graph Neural Network (FGNN) is proposed to learn complicated item dependency. To exploit and incorporate cross-session information in the individual session's representation learning, we further construct a Broadly Connected Session (BCS) graph to link different sessions and a novel Mask-Readout function to improve session embedding based on the BCS graph. Extensive experiments have been conducted on two e-commerce benchmark datasets, i.e., Yoochoose and Diginetica, and the experimental results demonstrate the superiority of our proposal through comparisons with state-of-the-art session-based recommender models.","Ruihong Qiu,Zi Huang,Jingjing Li,Hongzhi Yin",ACM Transactions on Information Systems,2020,,,
1013,Deep Learning for Portfolio Optimization,"We adopt deep learning models to directly optimise the portfolio Sharpe ratio. The framework we present circumvents the requirements for forecasting expected returns and allows us to directly optimise portfolio weights by updating model parameters. Instead of selecting individual assets, we trade Exchange-Traded Funds (ETFs) of market indices to form a portfolio. Indices of different asset classes show robust correlations and trading them substantially reduces the spectrum of available assets to choose from. We compare our method with a wide range of algorithms with results showing that our model obtains the best performance over the testing period, from 2011 to the end of April 2020, including the financial instabilities of the first quarter of 2020. A sensitivity analysis is included to understand the relevance of input features and we further study the performance of our approach under different cost rates and different risk levels via volatility scaling.","Zihao Zhang,Zihao Zhang,Zihao Zhang,Stefan Zohren,Stephen J. Roberts,Stephen Roberts,Stephen Roberts",,2020,,,
1014,Bitcoin-Compatible Virtual Channels,"Current permissionless cryptocurrencies such as Bitcoin suffer from a limited transaction rate and slow confirmation time, which hinders further adoption. Payment channels are one of the most promising solutions to address these problems, as they allow the parties of the channel to perform arbitrarily many payments in a peer-to-peer fashion while uploading only two transactions on the blockchain. This concept has been generalized into payment channel networks where a path of payment channels is used to settle the payment between two users that might not share a direct channel between them. However, this approach requires the active involvement of each user in the path, making the system less reliable (they might be offline), more expensive (they charge fees per payment), and slower (intermediaries need to be actively involved in the payment). To mitigate this issue, recent work has introduced the concept of virtual channels (IEEE S&P’19), which involve intermediaries only in the initial creation of a bridge between payer and payee, who can later on independently perform arbitrarily many off-chain transactions. Unfortunately, existing constructions are only available for Ethereum, as they rely on its account model and Turing-complete scripting language. The realization of virtual channels in other blockchain technologies with limited scripting capabilities, like Bitcoin, was so far considered an open challenge.In this work, we present the first virtual channel protocols that are built on the UTXO-model and require a scripting language supporting only a digital signature scheme and a timelock functionality, being thus backward compatible with virtually every cryptocurrency, including Bitcoin. We formalize the security properties of virtual channels as an ideal functionality in the Universal Composability framework and prove that our protocol constitutes a secure realization thereof. We have prototyped and evaluated our protocol on the Bitcoin blockchain, demonstrating its efficiency: for n sequential payments, they require an off-chain exchange of 9+2n transactions or a total of 3524+695n bytes, with no on-chain footprint in the optimistic case. This is a substantial improvement compared to routing payments in a payment channel network, which requires 8n transactions with a total of 3026n bytes to be exchanged.","Lukas Aumayr,Oguzhan Ersoy,Oguzhan Ersoy,Andreas Erwig,Sebastian Faust,Kristina Hostáková,Matteo Maffei,Pedro Moreno-Sanchez,Siavash Riahi",,2020,,,
1015,Hierarchical Attentive Transaction Embedding With Intra- and Inter-Transaction Dependencies for Next-Item Recommendation,"A transaction-based recommender system (TBRS) aims to predict the next item by modeling dependencies in transactional data. Generally, two kinds of dependencies considered are intra-transaction dependency and inter-transaction dependency. Most existing TBRSs recommend next item by only modeling the intra-transaction dependency within the current transaction while ignoring inter-transaction dependency with recent transactions that may also affect the next item. However, as not all recent transactions are relevant to the current and next items, the relevant ones should be identified and prioritized. In this paper, we propose a novel hierarchical attentive transaction embedding (HATE) model to tackle these issues. Specifically, a two-level attention mechanism integrates both item embedding and transaction embedding to build an attentive context representation that incorporates both intraand inter-transaction dependencies. With the learned context representation, HATE then recommends the next item. Experimental evaluations on two real-world transaction datasets show that HATE significantly outperforms the state-ofthe-art methods in terms of recommendation accuracy.","Shoujin Wang,Longbing Cao,Liang Hu,Shlomo Berkovsky,Xiaoshui Huang,Lin Xiao,Lin Xiao,Lin Xiao,Wenpeng Lu",The Missouri Review,2020,,,
1016,Relation-Aware Transformer for Portfolio Policy Learning,"Portfolio selection is an important yet challenging task in AI for FinTech. One of the key issues is how to represent the non-stationary price series of assets in a portfolio, which is important for portfolio decisions. The existing methods, however, fall short of capturing: 1) the complicated sequential patterns for asset price series and 2) the price correlations among multiple assets. In this paper, under a deep reinforcement learning paradigm for portfolio selection, we propose a novel Relation-aware Transformer (RAT) to handle these aspects. Specifically, being equipped with our newly developed attention modules, RAT is structurally innovated to capture both sequential patterns and asset correlations for portfolio selection. Based on the extracted sequential features, RAT is able to make profitable portfolio decisions regarding each asset via a newly devised leverage operation. Extensive experiments on real-world crypto-currency and stock datasets verify the state-of-the-art performance of RAT.","Ke Xu,Yifan Zhang,Yifan Zhang,Deheng Ye,Deheng Ye,Peilin Zhao,Peilin Zhao,Mingkui Tan",,2020,,,
1017,Modeling Personalized Item Frequency Information for Next-basket Recommendation,"Next-basket recommendation (NBR) is prevalent in e-commerce and retail industry. In this scenario, a user purchases a set of items (a basket) at a time. NBR performs sequential modeling and recommendation based on a sequence of baskets. NBR is in general more complex than the widely studied sequential (session-based) recommendation which recommends the next item based on a sequence of items. Recurrent neural network (RNN) has proved to be very effective for sequential modeling, and thus been adapted for NBR. However, we argue that existing RNNs cannot directly capture item frequency information in the recommendation scenario. Through careful analysis of real-world datasets, we find that personalized item frequency (PIF) information (which records the number of times that each item is purchased by a user) provides two critical signals for NBR. But, this has been largely ignored by existing methods. Even though existing methods such as RNN based methods have strong representation ability, our empirical results show that they fail to learn and capture PIF. As a result, existing methods cannot fully exploit the critical signals contained in PIF. Given this inherent limitation of RNNs, we propose a simple item frequency based k-nearest neighbors (kNN) method to directly utilize these critical signals. We evaluate our method on four public real-world datasets. Despite its relative simplicity, our method frequently outperforms the state-of-the-art NBR methods - including deep learning based methods using RNNs - when patterns associated with PIF play an important role in the data.","Haoji Hu,Xiangnan He,Jinyang Gao,Jinyang Gao,Zhi-Li Zhang,Zhi-Li Zhang,Zhi-Li Zhang",,2020,,,
1018,Modeling the Stock Relation with Graph Network for Overnight Stock Movement Prediction,"Stock movement prediction is a hot topic in the Fintech area. Previous works usually predict the price movement in a daily basis, although the market impact of news can be absorbed much shorter, and the exact time is hard to estimate. In this work, we propose a more practical objective to predict the overnight stock movement between the previous close price and the open price. As no trading operation occurs after market close, the market impact of overnight news will be reflected by the overnight movement. One big obstacle for such task is the lacking of data, in this work we collect and publish the overnight stock price movement dataset of Reuters Financial News. Another challenge is that the stocks in the market are not independent, which is omitted by previous works. To make use of the connection among stocks, we propose a LSTM Relational Graph Convolutional Network (LSTM-RGCN) model, which models the connection among stocks with their correlation matrix. Extensive experiment results show that our model outperforms the baseline models. Further analysis shows that the introduction of the graph enables our model to predict the movement of stocks that are not directly associated with news as well as the whole market, which is not available in most previous methods.","Wei Li,Wei Li,Wei Li,Wei Li,Ruihan Bao,Ruihan Bao,Keiko Harimoto,Deli Chen,Deli Chen,Jingjing Xu,Qi Su",,2020,,,
1019,Multi-Agent Reinforcement Learning in a Realistic Limit Order Book Market Simulation,"Optimal order execution is widely studied by industry practitioners and academic researchers because it determines the profitability of investment decisions and high-level trading strategies, particularly those involving large volumes of orders. However, complex and unknown market dynamics pose enormous challenges for the development and validation of optimal execution strategies. We propose a model-free approach by training Reinforcement Learning (RL) agents in a realistic market simulation environment with multiple agents. First, we have configured a multi-agent historical order book simulation environment for execution tasks based on an Agent-Based Interactive Discrete Event Simulation (ABIDES) [arXiv:1904.12066]. Second, we formulated the problem of optimal execution in an RL setting in which an intelligent agent can make order execution and placement decisions based on market microstructure trading signals in HFT. Third, we developed and trained an RL execution agent using the Double Deep Q-Learning (DDQL) algorithm in the ABIDES environment. In some scenarios, our RL agent converges towards a Time-Weighted Average Price (TWAP) strategy. Finally, we evaluated the simulation with our RL agent by comparing the simulation on the actual market Limit Order Book (LOB) characteristics.","Michaël Karpe,Jin Fang,Zhongyao Ma,Chen Wang",,2020,,,
1020,Memory Augmented Neural Model for Incremental Session-based Recommendation,"Increasing concerns with privacy have stimulated interests in Session-based Recommendation (SR) using no personal data other than what is observed in the current browser session. Existing methods are evaluated in static settings which rarely occur in real-world applications. To better address the dynamic nature of SR tasks, we study an incremental SR scenario, where new items and preferences appear continuously. We show that existing neural recommenders can be used in incremental SR scenarios with small incremental updates to alleviate computation overhead and catastrophic forgetting. More importantly, we propose a general framework called Memory Augmented Neural model (MAN). MAN augments a base neural recommender with a continuously queried and updated nonparametric memory, and the predictions from the neural and the memory components are combined through another lightweight gating network. We empirically show that MAN is well-suited for the incremental SR task, and it consistently outperforms state-oft-he-art neural and nonparametric methods. We analyze the results and demonstrate that it is particularly good at incrementally learning preferences on new and infrequent items.","Fei Mi,Boi Faltings",,2020,,,
1021,Multi-scale Two-way Deep Neural Network for Stock Trend Prediction,"Stock Trend Prediction(STP) has drawn wide attention from various fields, especially Artificial Intelligence. Most previous studies are single-scale oriented which results in information loss from a multi-scale perspective. In fact, multi-scale behavior is vital for making intelligent investment decisions. A mature investor will thoroughly investigate the state of a stock market at various time scales. To automatically learn the multi-scale information in stock data, we propose a Multi-scale Two-way Deep Neural Network. It learns multi-scale patterns from two types of scale-information, wavelet-based and downsampling-based, by eXtreme Gradient Boosting and Recurrent Convolutional Neural Network, respectively. After combining the learned patterns from the two-way, our model achieves state-of-the-art performance on FI-2010 and CSI-2016, where the latter is our published long-range stock dataset to help future studies for STP task. Extensive experimental results on the two datasets indicate that multi-scale information can significantly improve the STP performance and our model is superior in capturing such information.","Guang Liu,Guang Liu,Yuzhao Mao,Qi Sun,Qi Sun,Hailong Huang,Weiguo Gao,Xuan Li,Jianping Shen,Ruifan Li,Xiaojie Wang,Xiaojie Wang,Xiaojie Wang",,2020,,,
1022,An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization,"In this article, we propose an end-to-end adaptive framework for optimal trade execution based on Proximal Policy Optimization (PPO). We use two methods to account for the time dependencies in the market data based on two different neural network architecture: 1) Long short-term memory (LSTM) networks, 2) Fully-connected networks (FCN) by stacking the most recent limit orderbook (LOB) information as model inputs. The proposed framework can make trade execution decisions based on level-2 limit order book (LOB) information such as bid/ask prices and volumes directly without manually designed attributes as in previous research. Furthermore, we use a sparse reward function, which gives the agent reward signals at the end of each episode as an indicator of its relative performances against the baseline model, rather than implementation shortfall (IS) or a shaped reward function. The experimental results have demonstrated advantages over IS and the shaped reward function in terms of performance and simplicity. The proposed framework has outperformed the industry commonly used baseline models such as TWAP, VWAP, and AC as well as several Deep Reinforcement Learning (DRL) models on most of the 14 US equities in our experiments.","Siyu Lin,Peter A. Beling,Peter A. Beling",,2020,,,
1023,Data-Driven Market-Making via Model-Free Learning,"This paper studies when a market-making firm should place orders to maximize their expected net profit, while also constraining risk, assuming orders are maintained on an electronic limit order book (LOB). To do this, we use a model-free and off-policy method, Q-learning, coupled with state aggregation, to develop a proposed trading strategy that can be implemented using a simple lookup table. Our main training dataset is derived from event-by-event data recording the state of the LOB. Our proposed trading strategy has passed both in-sample and out-of-sample testing in the backtester of the market-making firm with whom we are collaborating, and it also outperforms other benchmark strategies. As a result, the firm desires to put the strategy into production.","Yueyang Zhong,YeeMan Bergstrom,Amy R. Ward,Amy Ward",,2020,,,
1024,Incorporating User Micro-behaviors and Item Knowledge into Multi-task Learning for Session-based Recommendation,"Session-based recommendation (SR) has become an important and popular component of various e-commerce platforms, which aims to predict the next interacted item based on a given session. Most of existing SR models only focus on exploiting the consecutive items in a session interacted by a certain user, to capture the transition pattern among the items. Although some of them have been proven effective, the following two insights are often neglected. First, a user's micro-behaviors, such as the manner in which the user locates an item, the activities that the user commits on an item (e.g., reading comments, adding to cart), offer fine-grained and deep understanding of the user's preference. Second, the item attributes, also known as item knowledge, provide side information to model the transition pattern among interacted items and alleviate the data sparsity problem. These insights motivate us to propose a novel SR model MKM-SR in this paper, which incorporates user Micro-behaviors and item Knowledge into Multi-task learning for Session-based Recommendation. Specifically, a given session is modeled on micro-behavior level in MKM-SR, i.e., with a sequence of item-operation pairs rather than a sequence of items, to capture the transition pattern in the session sufficiently. Furthermore, we propose a multi-task learning paradigm to involve learning knowledge embeddings which plays a role as an auxiliary task to promote the major task of SR. It enables our model to obtain better session representations, resulting in more precise SR recommendation results. The extensive evaluations on two benchmark datasets demonstrate MKM-SR's superiority over the state-of-the-art SR models, justifying the strategy of incorporating knowledge learning.","Wenjing Meng,Deqing Yang,Deqing Yang,Yanghua Xiao",,2020,,,
1025,Intention2Basket: A Neural Intention-driven Approach for Dynamic Next-basket Planning,"User purchase behaviours are complex and dynamic, which are usually observed as multiple choice actions across a sequence of shopping baskets. Most of the existing next-basket prediction approaches model user actions as homogeneous sequence data without considering complex and heterogeneous user intentions, impeding deep under-standing of user behaviours from the perspective of human inside drivers and thus reducing the prediction performance. Psychological theories have indicated that user actions are essentially driven by certain underlying intentions (e.g., diet and entertainment). Moreover, different intentions may influence each other while different choices usually have different utilities to accomplish an intention. Inspired by such psychological insights, we formalize the next-basket prediction as an Intention Recognition, Modelling and Accomplishing problem and further design the Intention2Basket (Int2Ba in short) model. In Int2Ba, an Intention Recognizer, a Coupled Intention Chain Net, and a Dynamic Basket Planner are specifically designed to respectively recognize, model and accomplish the heterogeneous intentions behind a sequence of baskets to better plan the next-basket. Extensive experiments on real-world datasets show the superiority of Int2Ba over the state-of-the-art approaches.","Shoujin Wang,Liang Hu,Yan Wang,Quan Z. Sheng,Quan Z. Sheng,Mehmet A. Orgun,Longbing Cao",,2020,,,
1026,MAPS: Multi-agent Reinforcement Learning-based Portfolio Management System,"Generating an investment strategy using advanced deep learning methods in stock markets has recently been a topic of interest. Most existing deep learning methods focus on proposing an optimal model or network architecture by maximizing return. However, these models often fail to consider and adapt to the continuously changing market conditions. In this paper, we propose the Multi-Agent reinforcement learning-based Portfolio management System (MAPS). MAPS is a cooperative system in which each agent is an independent ""investor"" creating its own portfolio. In the training procedure, each agent is guided to act as diversely as possible while maximizing its own return with a carefully designed loss function. As a result, MAPS as a system ends up with a diversified portfolio. Experiment results with 12 years of US market data show that MAPS outperforms most of the baselines in terms of Sharpe ratio. Furthermore, our results show that adding more agents to our system would allow us to get a higher Sharpe ratio by lowering risk with a more diversified portfolio.","Jinho Lee,Raehyun Kim,Raehyun Kim,Seok-Won Yi,Jaewoo Kang",,2020,,,
1027,Weighted QMIX: Expanding Monotonic Value Function Factorisation.,"QMIX is a popular $Q$-learning algorithm for cooperative MARL in the centralised training and decentralised execution paradigm. In order to enable easy decentralisation, QMIX restricts the joint action $Q$-values it can represent to be a monotonic mixing of each agent's utilities. However, this restriction prevents it from representing value functions in which an agent's ordering over its actions can depend on other agents' actions. To analyse this representational limitation, we first formalise the objective QMIX optimises, which allows us to view QMIX as an operator that first computes the $Q$-learning targets and then projects them into the space representable by QMIX. This projection returns a representable $Q$-value that minimises the unweighted squared error across all joint actions. We show in particular that this projection can fail to recover the optimal policy even with access to $Q^*$, which primarily stems from the equal weighting placed on each joint action. We rectify this by introducing a weighting into the projection, in order to place more importance on the better joint actions. We propose two weighting schemes and prove that they recover the correct maximal action for any joint action $Q$-values, and therefore for $Q^*$ as well. Based on our analysis and results in the tabular setting we introduce two scalable versions of our algorithm, Centrally-Weighted (CW) QMIX and Optimistically-Weighted (OW) QMIX and demonstrate improved performance on both predator-prey and challenging multi-agent StarCraft benchmark tasks.","Tabish Rashid,Gregory Farquhar,Bei Peng,Shimon Whiteson",,2020,,,
1028,Multi-label classification with weighted classifier selection and stacked ensemble,"Abstract   Multi-label classification has attracted increasing attention in various applications, such as medical diagnosis and semantic annotation. With such trend, a large number of ensemble approaches have been proposed for multi-label classification tasks. Most of these approaches construct the ensemble members by using bagging schemes, butfew stacked ensemble approaches are developed. Existing research on stacked ensemble approaches remains active, but several issues remain such as (1) little has been done to learn the weights of classifiers for classifier selection; (2) the relationship between pairwise label correlations and multi-label classification performance has not been investigated sufficiently. To address these issues, we propose a novel stacked ensemble approach that simultaneously exploits label correlations and the process of learning weights of ensemble members. In our approach, first, a weighted stacked ensemble with sparsity regularization is developed to facilitate classifier selection and ensemble members construction for multi-label classification. Second, in order to improve the classification performance, the pairwise label correlations are further considered for determining weights of these ensemble members. Finally, we develop an optimization algorithm based on both of the accelerated proximal gradient and the block coordinate descent techniques to achieve the optimal ensemble solution efficiently. Extensive experiments on publicly available datasets and real Cardiovascular and Cerebrovascular Disease datasets demonstrate that our proposed algorithm outperforms related state-of-the-art methods from perspectives of benchmarking and real-world applications.","Yuelong Xia,Ke Chen,Yun Yang,Yun Yang",Information Sciences,2020,,,
1029,Risk-Sensitive Reinforcement Learning: a Martingale Approach to Reward Uncertainty,"We introduce a novel framework to account for sensitivity to rewards uncertainty in sequential decision-making problems. While risk-sensitive formulations for Markov decision processes studied so far focus on the distribution of the cumulative reward as a whole, we aim at learning policies sensitive to the uncertain/stochastic nature of the rewards, which has the advantage of being conceptually more meaningful in some cases. To this end, we present a new decomposition of the randomness contained in the cumulative reward based on the Doob decomposition of a stochastic process, and introduce a new conceptual tool - the \textit{chaotic variation} - which can rigorously be interpreted as the risk measure of the martingale component associated to the cumulative reward process. We innovate on the reinforcement learning side by incorporating this new risk-sensitive approach into model-free algorithms, both policy gradient and value function based, and illustrate its relevance on grid world and portfolio optimization problems.","Nelson Vadori,Nelson Vadori,Sumitra Ganesh,Sumitra Ganesh,Prashant Reddy,Prashant P. Reddy,Manuela Veloso,Manuela Veloso",,2020,,,
1030,Event Study and Principal Component Analysis Based on Sentiment Analysis – A Combined Methodology to Study the Stock Market with an Empirical Study,"This paper provides an improved method by introducing Sentiment Analysis into the Event Study and Principal Component Analysis. The model is constructed by using the heuristic mean-end analysis. This method enables us to take into investors’ feelings towards related stocks when we study the stock market’s reaction to a given event. This paper investigates the Chinese A-shared market over 2013–2019 to study the influence of rumors and the offsetting impact of rumor clarifications on the stock price. The results indicate that no matter investor sentiment is bullish or bearish, stock price reacts significantly to rumors before as well as when the rumor goes public. Furthermore, clarification offsets the positive abnormal returns caused by rumors with bullish sentiment substantially at a limited level. Still, after five days, it creates a positive effect like the positive rumor does on the stock price. Under the bearish sentiment, clarification brings an insignificant impact on the stock price. The results indicate that the source of rumor may not come from the media and investment decisions established on rumors would be beneficial to investors before as well as after they are published. Moreover, official clarification causes an offset effect, but it is very limited.","Qianwen Xu,Victor Chang,Ching-Hsien Hsu",Information Systems Frontiers,2020,,,
1031,Continuous‐time mean–variance portfolio selection: A reinforcement learning framework,"We approach the continuous-time mean-variance (MV) portfolio selection with reinforcement learning (RL). The problem is to achieve the best tradeoff between exploration and exploitation, and is formulated as an entropy-regularized, relaxed stochastic control problem. We prove that the optimal feedback policy for this problem must be Gaussian, with time-decaying variance. We then establish connections between the entropy-regularized MV and the classical MV, including the solvability equivalence and the convergence as exploration weighting parameter decays to zero. Finally, we prove a policy improvement theorem, based on which we devise an implementable RL algorithm. We find that our algorithm outperforms both an adaptive control based method and a deep neural networks based algorithm by a large margin in our simulations.","Haoran Wang,Xun Yu Zhou",Mathematical Finance,2020,,,
1032,Stein Variational Inference for Discrete Distributions,"Gradient-based approximate inference methods, such as Stein variational gradient descent (SVGD), provide simple and general-purpose inference engines for differentiable continuous distributions. However, existing forms of SVGD cannot be directly applied to discrete distributions. In this work, we fill this gap by proposing a simple yet general framework that transforms discrete distributions to equivalent piecewise continuous distributions, on which the gradient-free SVGD is applied to perform efficient approximate inference. The empirical results show that our method outperforms traditional algorithms such as Gibbs sampling and discontinuous Hamiltonian Monte Carlo on various challenging benchmarks of discrete graphical models. We demonstrate that our method provides a promising tool for learning ensembles of binarized neural network (BNN), outperforming other widely used ensemble methods on learning binarized AlexNet on CIFAR-10 dataset. In addition, such transform can be straightforwardly employed in gradient-free kernelized Stein discrepancy to perform goodness-of-fit (GOF) test on discrete distributions. Our proposed method outperforms existing GOF test methods for intractable discrete distributions.","Jun Han,Fan Ding,Xianglong Liu,Lorenzo Torresani,Jian Peng,Qiang Liu,Qiang Liu",,2020,,,
1033,Multi-level Fitness Critics for Cooperative Coevolution,"In many multiagent domains, and particularly in tightly coupled domains, teasing an agent's contribution to the system performance based on a single episodic return is difficult. This well-known difficulty hits state-to-action mapping approaches such as neural networks trained by evolutionary algorithms particularly hard. This paper introduces fitness critics, which leverage the expected fitness to evaluate an agent's performance. This approach turns a sparse performance metric (policy evaluation) into a dense performance metric (state-action evaluation) by relating the episodic feedback to the state-action pairs experienced during the execution of that policy. In the tightly-coupled multi-rover domain (where multiple rovers have to perform a particular task simultaneously), only teams using fitness critics were able to demonstrate effective learning on tasks with tight coupling while other coevolved teams were unable to learn at all.","Golden Rockefeller,Shauharda Khadka,Kagan Tumer",,2020,,,
1034,A Study of Deep Learning-Based Approaches for Session-Based Recommendation Systems,"Recommending relevant items of interest for a user is the main purpose of the recommendation system. In the past, those systems achieve the recommended list based on long-term user profiles. However, personal data privacy is becoming a big challenge recently. Thus, the recommendation system needs to reduce the dependence on user profiles while preserving high accuracy on the recommendation. Session-based recommendation is a recently proposed approach for the recommendation system to overcome the issue of user profiles dependency. The relevance of the problem is quite high and has triggered interest among researchers in observing the activities of users. It increased several proposals for session-based recommendation algorithms that aim to predict the next actions. In this paper, we would like to compare the performance of such algorithms by using various datasets and evaluation metrics. A deep learning approach named GRU4REC (Hidasi et al. in Session-based recommendations with recurrent neural networks, 2015) and simpler methods are included in our comparison. Real-world datasets from three different domains are included in our experiment. Our experiments reveal that in some cases of numerous unpopular items dataset, GRU4REC’s performance is lower than expected. However, its performance is significantly increased after applying our proposed sampling method. Therefore, our obtained results suggested that there is still room for improving deep learning session-based recommendation algorithms.","Tran Khanh Dang,Quang Phu Nguyen,Van Sinh Nguyen",,2020,,,
1035,Likelihood Quantile Networks for Coordinating Multi-Agent Reinforcement Learning,"When multiple agents learn in a decentralized manner, the environment appears non-stationary from the perspective of an individual agent due to the exploration and learning of the other agents. Recently proposed deep multi-agent reinforcement learning methods have tried to mitigate this non-stationarity by attempting to determine which samples are from other agent exploration or suboptimality and take them less into account during learning. Based on the same philosophy, this paper introduces a decentralized quantile estimator, which aims to improve performance by distinguishing non-stationary samples based on the likelihood of returns. In particular, each agent considers the likelihood that other agent explorations and policy changes are occurring, essentially utilizing the agent's own estimations to weigh the learning rate that should be applied towards the given samples. We introduce a formal method of calculating differences of our return distribution representations and methods for utilizing it to guide updates. We also explore the effect of risk-seeking strategies for adjusting learning over time and propose adaptive risk distortion functions that guide risk sensitivity. Our experiments, on traditional benchmarks and new domains, show our methods are more stable, sample efficient and more likely to converge to a joint optimal policy than previous methods.","Xueguang Lyu,Christopher Amato",,2020,,,
1036,A Smart Healthcare Monitoring System for Heart Disease Prediction Based On Ensemble Deep Learning and Feature Fusion,"Abstract   The accurate prediction of heart disease is essential to efficiently treating cardiac patients before a heart attack occurs. This goal can be achieved using an optimal machine learning model with rich healthcare data on heart diseases. Various systems based on machine learning have been presented recently to predict and diagnose heart disease. However, these systems cannot handle high-dimensional datasets due to the lack of a smart framework that can use different sources of data for heart disease prediction. In addition, the existing systems utilize conventional techniques to select features from a dataset and compute a general weight for them based on their significance. These methods have also failed to enhance the performance of heart disease diagnosis. In this paper, a smart healthcare system is proposed for heart disease prediction using ensemble deep learning and feature fusion approaches. First, the feature fusion method combines the extracted features from both sensor data and electronic medical records to generate valuable healthcare data. Second, the information gain technique eliminates irrelevant and redundant features, and selects the important ones, which decreases the computational burden and enhances the system performance. In addition, the conditional probability approach computes a specific feature weight for each class, which further improves system performance. Finally, the ensemble deep learning model is trained for heart disease prediction. The proposed system is evaluated with heart disease data and compared with traditional classifiers based on feature fusion, feature selection, and weighting techniques. The proposed system obtains accuracy of 98.5%, which is higher than existing systems. This result shows that our system is more effective for the prediction of heart disease, in comparison to other state-of-the-art methods.","Farman Ali,Shaker El-Sappagh,S. M. Riazul Islam,Daehan Kwak,Amjad Ali,Amjad Ali,Muhammad Imran,Kyung Sup Kwak",Information Fusion,2020,,,
1037,Time-series forecasting of Bitcoin prices using high-dimensional features: a machine learning approach.,"Bitcoin is a decentralized cryptocurrency, which is a type of digital asset that provides the basis for peer-to-peer financial transactions based on blockchain technology. One of the main problems with decentralized cryptocurrencies is price volatility, which indicates the need for studying the underlying price model. Moreover, Bitcoin prices exhibit non-stationary behavior, where the statistical distribution of data changes over time. This paper demonstrates high-performance machine learning-based classification and regression models for predicting Bitcoin price movements and prices in short and medium terms. In previous works, machine learning-based classification has been studied for an only one-day time frame, while this work goes beyond that by using machine learning-based models for one, seven, thirty and ninety days. The developed models are feasible and have high performance, with the classification models scoring up to 65% accuracy for next-day forecast and scoring from 62 to 64% accuracy for seventh–ninetieth-day forecast. For daily price forecast, the error percentage is as low as 1.44%, while it varies from 2.88 to 4.10% for horizons of seven to ninety days. These results indicate that the presented models outperform the existing models in the literature.","Mohammed Mudassir,Shada Bennbaia,Devrim Unal,Mohammad Hammoudeh,Mohammad Hammoudeh",Neural Computing and Applications,2020,,,
1038,GAG: Global Attributed Graph Neural Network for Streaming Session-based Recommendation,"Streaming session-based recommendation (SSR) is a challenging task that requires the recommender system to do the session-based recommendation (SR) in the streaming scenario. In the real-world applications of e-commerce and social media, a sequence of user-item interactions generated within a certain period are grouped as a session, and these sessions consecutively arrive in the form of streams. Most of the recent SR research has focused on the static setting where the training data is first acquired and then used to train a session-based recommender model. They need several epochs of training over the whole dataset, which is infeasible in the streaming setting. Besides, they can hardly well capture long-term user interests because of the neglect or the simple usage of the user information. Although some streaming recommendation strategies have been proposed recently, they are designed for streams of individual interactions rather than streams of sessions. In this paper, we propose a Global Attributed Graph (GAG) neural network model with a Wasserstein reservoir for the SSR problem. On one hand, when a new session arrives, a session graph with a global attribute is constructed based on the current session and its associate user. Thus, the GAG can take both the global attribute and the current session into consideration to learn more comprehensive representations of the session and the user, yielding a better performance in the recommendation. On the other hand, for the adaptation to the streaming session scenario, a Wasserstein reservoir is proposed to help preserve a representative sketch of the historical data. Extensive experiments on two real-world datasets have been conducted to verify the superiority of the GAG model compared with the state-of-the-art methods.","Qiu Ruihong,Qiu Ruihong,Ruihong Qiu,Yin Hongzhi,Yin Hongzhi,Hongzhi Yin,Zi Huang,Huang Zi,Huang Zi,Tong Chen,Tong Chen",,2020,,,
1039,A hybrid two-stage financial stock forecasting algorithm based on clustering and ensemble learning,"This paper investigates the problem of the stock closing price forecasting for the stock market. Based on existing two-stage fusion models in the literature, two new prediction models based on clustering have been proposed, where k-means clustering method is adopted to cluster several common technical indicators. In addition, ensemble learning has also been applied to improve the prediction accuracy. Finally, a hybrid prediction model, which combines both the k-means clustering and ensemble learning, has been proposed. The experimental results on a number of Chinese stocks demonstrate that the hybrid prediction model obtains the best predicting accuracy of the stock price. The k-means clustering on the stock technical indicators can further enhance the prediction accuracy of the ensemble learning.","Ying Xu,Cuijuan Yang,Shaoliang Peng,Yusuke Nojima",Applied Intelligence,2020,,,
1040,Uncertainty-Aware Lookahead Factor Models for Quantitative Investing,"On a periodic basis, publicly traded companies report fundamentals, financial data including revenue, earnings, debt, among others. Quantitative finance research has identified several factors, functions of the reported data that historically correlate with stock market performance. In this paper, we first show through simulation that if we could select stocks via factors calculated on future fundamentals (via oracle), that our portfolios would far outperform standard factor models. Motivated by this insight, we train deep nets to forecast future fundamentals from a trailing 5-year history. We propose lookahead factor models which plug these predicted future fundamentals into traditional factors. Finally, we incorporate uncertainty estimates from both neural heteroscedastic regression and a dropout-based heuristic, improving performance by adjusting our portfolios to avert risk. In retrospective analysis, we leverage an industry-grade portfolio simulator (backtester) to show simultaneous improvement in annualized return and Sharpe ratio. Specifically, the simulated annualized return for the uncertainty-aware model is 17.7% (vs 14.0% for a standard factor model) and the Sharpe ratio is 0.84 (vs 0.52).","Lakshay Chauhan,John Alberg,Zachary C. Lipton",arXiv: Statistical Finance,2020,,,
1041,Deep deterministic portfolio optimization,"Abstract   Can deep reinforcement learning algorithms be exploited as solvers for optimal trading strategies? The aim of this work is to test reinforcement learning algorithms on conceptually simple, but mathematically non-trivial, trading environments. The environments are chosen such that an optimal or close-to-optimal trading strategy is known. We study the deep deterministic policy gradient algorithm and show that such a reinforcement learning agent can successfully recover the essential features of the optimal trading strategies and achieve close-to-optimal rewards.","Ayman Chaouki,Stephen J. Hardiman,Christian Schmidt,Christian Schmidt,Emmanuel Sérié,Joachim de Lataillade",The Journal of Finance and Data Science,2020,,,
1042,Intelligent Fault Diagnosis by Fusing Domain Adversarial Training and Maximum Mean Discrepancy via Ensemble Learning,"Nowadays, the industrial Internet of Things (IIoT) has been successfully utilized in smart manufacturing. The massive amount of data in IIoT promote the development of deep learning-based health monitoring for industrial equipment. Since monitoring data for mechanical fault diagnosis collected on different working conditions or equipment have domain mismatch, models trained with training data may not work in practical applications. Therefore, it is essential to study fault diagnosis methods with domain adaptation ability. In this article, we propose an intelligent fault diagnosis method based on an improved domain adaptation method. Specifically, two feature extractors concerning feature space distance and domain mismatch are trained using maximum mean discrepancy and domain adversarial training respectively to enhance feature representation. Since separate classifiers are trained for feature extractors, ensemble learning is further utilized to obtain final results. Experimental results indicate that the proposed method is effective and applicable in diagnosing faults with domain mismatch.","Yibin Li,Yan Song,Lei Jia,Shengyao Gao,Qiqiang Li,Qiqiang Li,Meikang Qiu",IEEE Transactions on Industrial Informatics,2020,,,
1043,Deep Learning modeling of Limit Order Book: a comparative perspective.,"The present work addresses theoretical and practical questions in the domain of Deep Learning for High Frequency Trading, with a thorough review and analysis of the literature and state-of-the-art models. Random models, Logistic Regressions, LSTMs, LSTMs equipped with an Attention mask, CNN-LSTMs and MLPs are compared on the same tasks, feature space, and dataset and clustered according to pairwise similarity and performance metrics. The underlying dimensions of the modeling techniques are hence investigated to understand whether these are intrinsic to the Limit Order Book's dynamics. It is possible to observe that the Multilayer Perceptron performs comparably to or better than state-of-the-art CNN-LSTM architectures indicating that dynamic spatial and temporal dimensions are a good approximation of the LOB's dynamics, but not necessarily the true underlying dimensions.","Antonio Briola,Jeremy D. Turiel,Tomaso Aste",arXiv: Trading and Market Microstructure,2020,,,
1044,Recency Aware Collaborative Filtering for Next Basket Recommendation,"E-commerce and online services are getting more and more ubiquitous day by day. Like many other e-commerce paradigms, online grocery services can highly benefit from recommender systems, especially when it comes to predicting users' shopping behavior. This specific scenario owns peculiar characteristics, such as repetitiveness and loyalty, which makes the task very different from the standard recommendations. In this work, we present an efficient solution to compute the next basket recommendation, under a more general top-n recommendation framework. We propose a set of collaborative filtering based techniques able to capture users' shopping patterns. Furthermore, we analyzed how recency plays a key role in this particular task. We finally compare our method with state-of-the-art algorithms on two online grocery service datasets.","Guglielmo Faggioli,Mirko Polato,Fabio Aiolli,Fabio Aiolli",,2020,,,
1045,Efficiency Analysis of Machine Learning Intelligent Investment Based on K-Means Algorithm,"With the rapid development of technologies such as big data, intelligent data analysis and cloud computing, the application of Internet financial technology has become more and more extensive, and with the advent of the era of large asset management in the domestic wealth management industry, in order to improve the efficiency of financial services, traditional finance is needed. The products and services provided by the industry have been innovated, resulting in smart investment. Compared with traditional investment, smart investment as a new business model has the advantages of low threshold, low cost and high efficiency. However, as far as its nature is concerned, smart investment must first play the role of an investment adviser. Therefore, for enterprises or individuals who invest, the investment efficiency of smart investment is the most important. At present, the research on the efficiency analysis of smart investment, due to the improper selection of algorithm models or the lack of deep data mining, leads to the analysis of the investment efficiency of smart investment products is inconsistent with or even deviated from the actual situation. In view of these problems, this paper selects China Merchants Bank’s Capricorn Intelligence as the research object, and analyzes the investment efficiency of smart investment based on K-means cluster analysis and data mining technology. The results show that Capricorn has a certain randomness in the selection process of the fund, and chooses to reduce the rate of return in order to control the risk. The investment portfolio formulated for the customer has obvious timing. The results show that the machine learning based on K-means algorithm makes a concrete analysis of the investment efficiency of Capricorn Smart Investment, this method can also be used for the efficiency analysis of other smart investment products.","Liang Li,Jia Wang,Xuetao Li",IEEE Access,2020,,,
1046,"A practical tutorial on bagging and boosting based ensembles for machine learning: Algorithms, software tools, performance study, practical perspectives and opportunities","Abstract   Ensembles, especially ensembles of decision trees, are one of the most popular and successful techniques in machine learning. Recently, the number of ensemble-based proposals has grown steadily. Therefore, it is necessary to identify which are the appropriate algorithms for a certain problem. In this paper, we aim to help practitioners to choose the best ensemble technique according to their problem characteristics and their workflow. To do so, we revise the most renowned bagging and boosting algorithms and their software tools. These ensembles are described in detail within their variants and improvements available in the literature. Their online-available software tools are reviewed attending to the implemented versions and features. They are categorized according to their supported programming languages and computing paradigms. The performance of 14 different bagging and boosting based ensembles, including XGBoost, LightGBM and Random Forest, is empirically analyzed in terms of predictive capability and efficiency. This comparison is done under the same software environment with 76 different classification tasks. Their predictive capabilities are evaluated with a wide variety of scenarios, such as standard multi-class problems, scenarios with categorical features and big size data. The efficiency of these methods is analyzed with considerably large data-sets. Several practical perspectives and opportunities are also exposed for ensemble learning.","Sergio González,Salvador García,Javier Del Ser,Lior Rokach,Lior Rokach,Francisco Herrera",Information Fusion,2020,,,
1047,Efficacy of Modern Neuro-Evolutionary Strategies for Continuous Control Optimization.,"We analyze the efficacy of modern neuro-evolutionary strategies for continuous control optimization. Overall the results collected on a wide variety of qualitatively different benchmark problems indicate that these methods are generally effective and scale well with respect to the number of parameters and the complexity of the problem and are relatively robust with respect to the setting of hyper-parameters. The comparison of the most promising methods indicates that the OpenAI-ES algorithm outperforms or equals the other algorithms on all considered problems. Moreover, we demonstrate how the reward functions optimized for reinforcement learning methods are not necessarily effective for evolutionary strategies and vice versa. This finding has important consequences since it implies that the comparison performed to date are biased toward one or the other class of algorithm and since it might lead to reconsideration of the relatively efficacy of the two classes of algorithms","Paolo Pagliuca,Nicola Milano,Stefano Nolfi",,2020,,,
1048,From LiDAR point cloud towards digital twin city: Clustering city objects based on Gestalt principles,"Abstract   Recent advancement of remote sensing technologies has brought in accurate, dense, and inexpensive city-scale Light Detection And Ranging (LiDAR) point clouds, which can be utilized to model city objects (e.g., buildings, roads, and automobiles) for creating Digital Twin Cities (DTCs). However, processing such unstructured point clouds is very challenging, epitomized by high cost, movable objects, limited object classes, and high information inadequacy/redundancy. We noticed that many city objects are not in random shapes; rather, they have invariant cross-sections following the Gestalt design principles, including proximity, connectivity, symmetry, and similarity. In this paper, we present a novel unsupervised method, called Clustering Of Symmetric Cross-sections of Objects (COSCO), to process urban LiDAR point clouds to a hierarchy of objects based on their characteristic cross-sections. First, city objects are segmented as connected patches of proximate 3D points. Then, symmetric cross-sections are detected for symmetric city objects. Finally, the taxonomy and groups of city objects are recognized from a hierarchical clustering analysis of the dissimilarity matrix. Experimental results showed that COSCO detected the correct taxonomy and types of 12 cars from 24,126 LiDAR points in 8.28s. Based on the cross-sections and taxonomy, a digital twin was created by registering online free 3D car models in 29.58s. The contribution of this paper is twofold. First, it presents an effective unsupervised method for understanding and developing DTC objects in LiDAR point clouds by harnessing innate Gestalt design principles. Secondly, COSCO can be an efficient LiDAR pre-processing tool for recognizing symmetric city objects’ cross-sections, positions, heading directions, dimensions, and possible types for smart city applications in GIScience, Architecture, Engineering, Construction and Operation (AECO), and autonomous vehicles.","Fan Xue,Weisheng Lu,Zhe Chen,Chris Webster",Isprs Journal of Photogrammetry and Remote Sensing,2020,,,
1049,Multi-DQN: An ensemble of Deep Q-learning agents for stock market forecasting,"Abstract   The stock market forecasting is one of the most challenging application of machine learning, as its historical data are naturally noisy and unstable. Most of the successful approaches act in a supervised manner, labeling training data as being of positive or negative moments of the market. However, training machine learning classifiers in such a way may suffer from over-fitting, since the market behavior depends on several external factors like other markets trends, political events, etc. In this paper, we aim at minimizing such problems by proposing an ensemble of reinforcement learning approaches which do not use annotations (i.e. market goes up or down) to learn, but rather learn how to maximize a return function over the training stage. In order to achieve this goal, we exploit a Q-learning agent trained several times with the same training data and investigate its ensemble behavior in important real-world stock markets. Experimental results in intraday trading indicate better performance than the conventional  Buy-and-Hold  strategy, which still behaves well in our setups. We also discuss qualitative and quantitative analyses of these results.","Salvatore Carta,Anselmo Ferreira,Alessandro Sebastian Podda,Diego Reforgiato Recupero,Antonio Sanna",Expert Systems With Applications,2021,,,
1050,metric-learn: Metric Learning Algorithms in Python,"metric-learn is an open source Python package implementing supervised and weakly-supervised distance metric learning algorithms. As part of scikit-learn-contrib, it provides a unified interface compatible with scikit-learn which allows to easily perform cross-validation, model selection, and pipelining with other machine learning estimators. metric-learn is thoroughly tested and available on PyPi under the MIT licence.","William de Vazelhes,CJ Carey,CJ Carey,Yuan Tang,Nathalie Vauquier,Aurélien Bellet",,2019,,,
1051,Active k-labelsets ensemble for multi-label classification,"Abstract   The random k-labelsets ensemble (RAkEL) is a multi-label learning strategy that integrates many single-label learning models. Each single-label model is constructed using a label powerset (LP) technique based on a randomly generated size-k label subset. Although RAkEL can improve the generalization capability and reduce the complexity of the original LP method, the quality of the randomly generated label subsets could be low. On the one hand, the transformed classes may be difficult to separate in the feature space, negatively affecting the performance; on the other hand, the classes might be highly imbalanced, resulting in difficulties in using the existing single-label algorithms. To solve these problems, we propose an active k-labelsets ensemble (ACkEL) paradigm. Borrowing the idea of active learning, a label-selection criterion is proposed to evaluate the separability and balance level of the classes transformed from a label subset. Subsequently, by randomly selecting the first label or label subset, the remaining ones are iteratively chosen based on the proposed criterion. ACkEL can be realized in both the disjoint and overlapping modes, which adopt pool-based and stream-based frameworks, respectively. Experimental comparisons demonstrate the feasibility and effectiveness of the proposed methods.","Xizhao Wang,Ran Wang,Sam Kwong,Xu Wang,Yuheng Jia",Pattern Recognition,2021,,,
1052,Research directions in session-based and sequential recommendation,,"Dietmar Jannach,Bamshad Mobasher,Shlomo Berkovsky",User Modeling and User-adapted Interaction,2020,,,
1053,Ensemble deep learning in bioinformatics,"The remarkable flexibility and adaptability of ensemble methods and deep learning models have led to the proliferation of their application in bioinformatics research. Traditionally, these two machine learning techniques have largely been treated as independent methodologies in bioinformatics applications. However, the recent emergence of ensemble deep learning—wherein the two machine learning techniques are combined to achieve synergistic improvements in model accuracy, stability and reproducibility—has prompted a new wave of research and application. Here, we share recent key developments in ensemble deep learning and look at how their contribution has benefited a wide range of bioinformatics research from basic sequence analysis to systems biology. While the application of ensemble deep learning in bioinformatics is diverse and multifaceted, we identify and discuss the common challenges and opportunities in the context of bioinformatics research. We hope this Review Article will bring together the broader community of machine learning researchers, bioinformaticians and biologists to foster future research and development in ensemble deep learning, and inspire novel bioinformatics applications that are unattainable by traditional methods. Recent developments in machine learning have seen the merging of ensemble and deep learning techniques. The authors review advances in ensemble deep learning methods and their applications in bioinformatics, and discuss the challenges and opportunities going forward.","Yue Cao,Thomas A. Geddes,Jean Yee Hwa Yang,Pengyi Yang",Nature Machine Intelligence,2020,,,
1054,Deep ensemble neural-like P systems for segmentation of central serous chorioretinopathy lesion,"Abstract   Automatic segmentation of the central serous chorioretinopathy (CSC) lesion and its related ellipsoid zone from the Bruch membrane (EZ-BM) areas is important in the early diagnosis and treatment of retinopathy to prevent vision loss. However, the large variations in the locations and shapes of the CSC lesion, as well as the low contrast of EZ-BM areas with their surroundings make the segmentation task challenging. To address these challenges, in this paper, we propose a new parallel neural-like P system named the deep ensemble neural-like (DEN) P system, which combines the strengths of spiking neural P systems (SN P systems) and deep convolutional neural networks (CNNs) for more accurate and efficient segmentation of CSC lesion and the related EZ-BM areas. The DEN P system establishes three modules with new rules and neuron architectures, which is implemented end-to-end in neurons. Specifically, we propose an ensemble fully convolutional network (FCN) module to train several FCNs with different initializations to obtain effective features, which leverage the strength of ensemble learning in a DEN P system. Benefiting from the parallelism of DEN P systems, FCNs are conducted in different neurons simultaneously. To achieve efficient classification, we propose a multiloss module with three different loss functions to alleviate DEN P system falling into a single-loss function. Different losses are also conducted in different neurons parallelly. To further enhance the performance, we introduce a coarse-fine compensation module to correct detection errors. Being a parallel computational paradigm, DEN P systems are less time consuming, completing CSC lesion and EZ-BM areas on 1,280 images in 0.04s with an average dice ratio of     0  .  93  ±  0  .  04     and     0  .  95  ±  0  .  02    , respectively. Moreover, the ablation study shows that the proposed modules are critical for effective learning. The extension and generalization of the DEN P systems are also investigated .","Jie Xue,Zhuo Wang,Zhuo Wang,Zhuo Wang,Deting Kong,Yuan Wang,Xiyu Liu,Xiyu Liu,Wen Fan,Songtao Yuan,Sijie Niu,Dengwang Li",Information Fusion,2021,,,
1055,Fast RobustSTL: Efficient and Robust Seasonal-Trend Decomposition for Time Series with Complex Patterns,"Many real-world time series data exhibit complex patterns with trend, seasonality, outlier and noise. Robustly and accurately decomposing these components would greatly facilitate time series tasks including anomaly detection, forecasting and classification. RobustSTL is an effective seasonal-trend decomposition for time series data with complicated patterns. However, it cannot handle multiple seasonal components properly. Also it suffers from its high computational complexity, which limits its usage in practice. In this paper, we extend RobustSTL to handle multiple seasonality. To speed up the computation, we propose a special generalized ADMM algorithm to perform the decomposition efficiently. We rigorously prove that the proposed algorithm converges approximately as standard ADMM while reducing the complexity from O(N2) to O(N log N) for each iteration. We empirically study our proposed algorithm with other state-of-the-art seasonal-trend decomposition methods, including MSTL, STR, TBATS, on both synthetic and real-world datasets with single and multiple seasonality. The experimental results demonstrate the superior performance of our decomposition algorithm in terms of both effectiveness and efficiency.","Qingsong Wen,Zhe Zhang,Yan Li,Liang Sun,Liang Sun,Liang Sun,Liang Sun,Liang Sun",,2020,,,
1056,Using Data Augmentation Based Reinforcement Learning for Daily Stock Trading,"In algorithmic trading, adequate training data set is key to making profits. However, stock trading data in units of a day can not meet the great demand for reinforcement learning. To address this problem, we proposed a framework named data augmentation based reinforcement learning (DARL) which uses minute-candle data (open, high, low, close) to train the agent. The agent is then used to guide daily stock trading. In this way, we can increase the instances of data available for training in hundreds of folds, which can substantially improve the reinforcement learning effect. But not all stocks are suitable for this kind of trading. Therefore, we propose an access mechanism based on skewness and kurtosis to select stocks that can be traded properly using this algorithm. In our experiment, we find proximal policy optimization (PPO) is the most stable algorithm to achieve high risk-adjusted returns. Deep Q-learning (DQN) and soft actor critic (SAC) can beat the market in Sharp Ratio.","Yuyu Yuan,Wen Wen,Jincui Yang",Electronics,2020,,,
1057,Handling Information Loss of Graph Neural Networks for Session-based Recommendation,"Recently, graph neural networks (GNNs) have gained increasing popularity due to their convincing performance in various applications. Many previous studies also attempted to apply GNNs to session-based recommendation and obtained promising results. However, we spot that there are two information loss problems in these GNN-based methods for session-based recommendation, namely the lossy session encoding problem and the ineffective long-range dependency capturing problem. The first problem is the lossy session encoding problem. Some sequential information about item transitions is ignored because of the lossy encoding from sessions to graphs and the permutation-invariant aggregation during message passing. The second problem is the ineffective long-range dependency capturing problem. Some long-range dependencies within sessions cannot be captured due to the limited number of layers. To solve the first problem, we propose a lossless encoding scheme and an edge-order preserving aggregation layer based on GRU that is dedicatedly designed to process the losslessly encoded graphs. To solve the second problem, we propose a shortcut graph attention layer that effectively captures long-range dependencies by propagating information along shortcut connections. By combining the two kinds of layers, we are able to build a model that does not have the information loss problems and outperforms the state-of-the-art models on three public datasets.","Tianwen Chen,Raymond Chi-Wing Wong",,2020,,,
1058,A Novel Deep Learning Model by Stacking Conditional Restricted Boltzmann Machine and Deep Neural Network,"A real-world system often exhibits complex dynamics arising from interaction among its subunits. In machine learning and data mining, these interactions are usually formulated as dependency and correlation among system variables. Similar to Convolution Neural Network dealing with spatially correlated features and Recurrent Neural Network with temporally correlated features, in this paper we present a novel deep learning model to tackle functionally interactive features by stacking a Conditional Restricted Boltzmann Machine and a Deep Neural Network (CRBM-DNN). Variables with their dependency relationships are organized into a bipartite graph, which is further converted into a Restricted Boltzmann Machine conditioned by domain knowledge. We integrate this CRBM and a DNN into one deep learning model constrained by one overall cost function. CRBM-DNN can solve both supervised and unsupervised learning problems. Compared to a regular neural network of the same size, CRBM-DNN has fewer parameters so they require fewer training samples. We perform extensive comparative studies with a large number of supervised learning and unsupervised learning methods using several challenging real-world datasets, and achieve significant superior performance.","Tianyu Kang,Ping Chen,John Quackenbush,Wei Ding",,2020,,,
1059,Deep Hedging of Derivatives Using Reinforcement Learning,"This paper shows how reinforcement learning can be used to derive optimal hedging strategies for derivatives when there are transaction costs. The paper illustrates the approach by showing the diﬀerence between using delta hedging and optimal hedging for a short position in a call option when the objective is to minimize a function equal to the mean hedging cost plus a constant times the standard deviation of the hedging cost. Two situations are considered. In the ﬁrst, the asset price follows geometric Brownian motion. In the second, the asset price follows a stochastic volatility process. The paper extends the basic reinforcement learning approach in a number of ways. First, it uses two diﬀerent Q-functions so that both the expected value of the cost and the expected value of the square of the cost are tracked for diﬀerent state/action combinations. This approach increases the range of objective functions that can be used. Second, it uses a learning algorithm that allows for continuous state and action space. Third, it compares the accounting P&L approach (where the hedged position is valued at each step) and the cash ﬂow approach (where cash inﬂows and outﬂows are used). We ﬁnd that a hybrid approach involving the use of an accounting P&L approach that incorporates a relatively simple valuation model works well. The valuation model does not have to correspond to the process assumed for the underlying asset price.","Jay Cao,Jacky Chen,John C. Hull,Zissis Poulos",,2019,,,
1060,Deep Learning for Portfolio Optimization,"In this article, the authors adopt deep learning models to directly optimize the portfolio Sharpe ratio. The framework they present circumvents the requirements for forecasting expected returns and allows them to directly optimize portfolio weights by updating model parameters. Instead of selecting individual assets, they trade exchange-traded funds of market indexes to form a portfolio. Indexes of different asset classes show robust correlations, and trading them substantially reduces the spectrum of available assets from which to choose. The authors compare their method with a wide range of algorithms, with results showing that the model obtains the best performance over the testing period of 2011 to the end of April 2020, including the financial instabilities of the first quarter of 2020. A sensitivity analysis is included to clarify the relevance of input features, and the authors further study the performance of their approach under different cost rates and different risk levels via volatility scaling.  TOPICS:Exchange-traded funds and applications, mutual fund performance, portfolio construction  Key Findings  • In this article, the authors utilize deep learning models to directly optimize the portfolio Sharpe ratio. They present a framework that bypasses traditional forecasting steps and allows portfolio weights to be optimized by updating model parameters.  • The authors trade exchange-traded funds of market indexes to form a portfolio. Doing this substantially reduces the scope of possible assets to choose from, and these indexes have shown robust correlations.  • The authors back test their methods from 2011 to the end of April 2020, including the financial instabilities due to COVID-19. Their model delivers good performance under transaction costs, and a detailed study shows the rationality of their approach during the crisis.","Zihao Zhang,Zihao Zhang,Zihao Zhang,Stefan Zohren,Stephen J. Roberts,Stephen Roberts",,2020,,,
1061,Particle Swarm optimisation for Evolving Deep Neural Networks for Image Classification by Evolving and Stacking Transferable Blocks,"Deep Convolutional Neural Networks (CNNs) have been widely used in image classification tasks, but the process of designing CNN architectures is very complex, so Neural Architecture Search (NAS), automatically searching for optimal CNN architectures, has attracted more and more research interests. However, the computational cost of NAS is often too high to be applied to real-life applications. In this paper, an efficient particle swarm optimisation method named EPSOCNN is proposed to evolve CNN architectures inspired by the idea of transfer learning. EPSOCNN successfully reduces the computation cost by minimising the search space to a single block and utilising a small subset of the training set to evaluate CNNs during the evolutionary process. Meanwhile, EPSOCNN also keeps very competitive classification accuracy by stacking the evolved block multiple times to fit the whole training dataset. The proposed EPSOCNN algorithm is evaluated on CIFAR-10 dataset and compared with 13 peer competitors including deep CNNs crafted by hand, learned by reinforcement learning methods and evolved by evolutionary computation approaches. It shows very promising results with regard to the classification accuracy, the number of parameters and the computational cost. Besides, the evolved transferable block from CIFAR-10 is transferred and evaluated on two other datasets — CIFAR-100 and SVHN. It shows promising results on both of the datasets, which demonstrate the transferability of the evolved block. All of the experiments have been performed multiple times and Student’s t-test is used to compare the proposed method with peer competitors from the statistical point of view.","Bin Wang,Bing Xue,Mengjie Zhang",,2020,,,
1062,Double-Wing Mixture of Experts for Streaming Recommendations,"Streaming Recommender Systems (SRSs) commonly train recommendation models on newly received data only to address user preference drift, i.e., the changing user preferences towards items. However, this practice overlooks the long-term user preferences embedded in historical data. More importantly, the common heterogeneity in data stream greatly reduces the accuracy of streaming recommendations. The reason is that different preferences (or characteristics) of different types of users (or items) cannot be well learned by a unified model. To address these two issues, we propose a Variational and Reservoir-enhanced Sampling based Double-Wing Mixture of Experts framework, called VRS-DWMoE, to improve the accuracy of streaming recommendations. In VRS-DWMoE, we first devise variational and reservoir-enhanced sampling to wisely complement new data with historical data, and thus address the user preference drift issue while capturing long-term user preferences. After that, we propose a Double-Wing Mixture of Experts (DWMoE) model to first effectively learn heterogeneous user preferences and item characteristics, and then make recommendations based on them. Specifically, DWMoE contains two Mixture of Experts (MoE, an effective ensemble learning model) to learn user preferences and item characteristics, respectively. Moreover, the multiple experts in each MoE learn the preferences (or characteristics) of different types of users (or items) where each expert specializes in one underlying type. Extensive experiments demonstrate that VRS-DWMoE consistently outperforms the state-of-the-art SRSs.","Yan Zhao,Yan Zhao,Shoujin Wang,Shoujin Wang,Yan Wang,Yan Wang,Hongwei Liu,Hongwei Liu,Hongzhi Wang,Weizhe Zhang,Weizhe Zhang,Weizhe Zhang",arXiv: Information Retrieval,2020,,,
1063,Optimization of blockchain investment portfolio under artificial bee colony algorithm,"Abstract   To improve the security of asset securitization and reduce investment risk, the risk reduction methods of asset securitization are investigated. First, the risk and return of investment are measured using multiple methods, and several portfolio methods are introduced. Second, there are problems of fraud risk, underlying assets, and asymmetry risk when assets are being transformed into securitization. The blockchain technology is used to reduce the probability of these risks and improve investment security. Finally, the artificial bee colony (ABC) algorithm is applied to the optimization of the blockchain investment portfolio. Since the traditional ABC algorithm can only solve the problem of single-objective optimization, an external population is constructed to improve it. The results show that the improved ABC algorithm can simultaneously optimize multiple features in the investment portfolio, reduce the error of investors when making decisions, and improve the balance between investment returns and risks. At the same time, after the algorithm is evaluated, it is found that the accuracy and practicality have been improved to a certain extent. Also, it is proved that the ABC algorithm can solve the problem of portfolio optimization, improve the security of asset securitization, and ensure the balance of investment return and risk to a certain extent.","Yulin Deng,Hongfeng Xu,Jie Wu",Journal of Computational and Applied Mathematics,2020,,,
1064,The future of digital health with federated learning,"Data-driven machine learning (ML) has emerged as a promising approach for building accurate and robust statistical models from medical data, which is collected in huge volumes by modern healthcare systems. Existing medical data is not fully exploited by ML primarily because it sits in data silos and privacy concerns restrict access to this data. However, without access to sufficient data, ML will be prevented from reaching its full potential and, ultimately, from making the transition from research to clinical practice. This paper considers key factors contributing to this issue, explores how federated learning (FL) may provide a solution for the future of digital health and highlights the challenges and considerations that need to be addressed.","Nicola Rieke,Jonny Hancox,Wenqi Li,Fausto Milletari,Holger R. Roth,Shadi Albarqouni,Spyridon Bakas,Mathieu N. Galtier,Bennett A. Landman,Klaus H. Maier-Hein,Sebastien Ourselin,Micah J. Sheller,Ronald M. Summers,Andrew Trask,Daguang Xu,Maximilian Baust,M. Jorge Cardoso,M. Jorge Cardoso",,2020,,,
1065,Energy-based Surprise Minimization for Multi-Agent Value Factorization.,"Multi-Agent Reinforcement Learning (MARL) has demonstrated significant success in training decentralised policies in a centralised manner by making use of value factorization methods. However, addressing surprise across spurious states and approximation bias remain open problems for multi-agent settings. We introduce the Energy-based MIXer (EMIX), an algorithm which minimizes surprise utilizing the energy across agents. Our contributions are threefold; (1) EMIX introduces a novel surprise minimization technique across multiple agents in the case of multi-agent partially-observable settings. (2) EMIX highlights the first practical use of energy functions in MARL (to our knowledge) with theoretical guarantees and experiment validations of the energy operator. Lastly, (3) EMIX presents a novel technique for addressing overestimation bias across agents in MARL. When evaluated on a range of challenging StarCraft II micromanagement scenarios, EMIX demonstrates consistent state-of-the-art performance for multi-agent surprise minimization. Moreover, our ablation study highlights the necessity of the energy-based scheme and the need for elimination of overestimation bias in MARL. Our implementation of EMIX and videos of agents are available at https://karush17.github.io/emix-web/.","Karush Suri,Xiao Qi Shi,Konstantinos N. Plataniotis,Yuri A. Lawryshyn,Yuri Lawryshyn",arXiv: Learning,2020,,,
1066,Qlib: An AI-oriented Quantitative Investment Platform.,"Quantitative investment aims to maximize the return and minimize the risk in a sequential trading period over a set of financial instruments. Recently, inspired by rapid development and great potential of AI technologies in generating remarkable innovation in quantitative investment, there has been increasing adoption of AI-driven workflow for quantitative research and practical investment. In the meantime of enriching the quantitative investment methodology, AI technologies have raised new challenges to the quantitative investment system. Particularly, the new learning paradigms for quantitative investment call for an infrastructure upgrade to accommodate the renovated workflow; moreover, the data-driven nature of AI technologies indeed indicates a requirement of the infrastructure with more powerful performance; additionally, there exist some unique challenges for applying AI technologies to solve different tasks in the financial scenarios. To address these challenges and bridge the gap between AI technologies and quantitative investment, we design and develop Qlib that aims to realize the potential, empower the research, and create the value of AI technologies in quantitative investment.","Xiao Yang,Weiqing Liu,Weiqing Liu,Liu Weiqing,Dong Zhou,Jiang Bian,Jiang Bian,Tie-Yan Liu",arXiv: General Finance,2020,,,
1067,Impact of intelligence methodologies on education and training process,,Vijayalakshmi Saravanan,Journal of Intelligent and Fuzzy Systems,2020,,,
1068,ADER: Adaptively Distilled Exemplar Replay Towards Continual Learning for Session-based Recommendation,"Session-based recommendation has received growing attention recently due to the increasing privacy concern. Despite the recent success of neural session-based recommenders, they are typically developed in an offline manner using a static dataset. However, recommendation requires continual adaptation to take into account new and obsolete items and users, and requires “continual learning” in real-life applications. In this case, the recommender is updated continually and periodically with new data that arrives in each update cycle, and the updated model needs to provide recommendations for user activities before the next model update. A major challenge for continual learning with neural models is catastrophic forgetting, in which a continually trained model forgets user preference patterns it has learned before. To deal with this challenge, we propose a method called Adaptively Distilled Exemplar Replay (ADER) by periodically replaying previous training samples (i.e., exemplars) to the current model with an adaptive distillation loss. Experiments are conducted based on the state-of-the-art SASRec model using two widely used datasets to benchmark ADER with several well-known continual learning techniques. We empirically demonstrate that ADER consistently outperforms other baselines, and it even outperforms the method using all historical data at every update cycle. This result reveals that ADER is a promising solution to mitigate the catastrophic forgetting issue towards building more realistic and scalable session-based recommenders.","Fei Mi,Xiaoyu Lin,Boi Faltings",,2020,,,
1069,Which Trading Agent is Best? Using a Threaded Parallel Simulation of a Financial Market Changes the Pecking-Order,"This paper presents novel results generated from a new simulation model of a contemporary financial market, that cast serious doubt on the previously widely accepted view of the relative performance of various well-known public-domain automated-trading algorithms. Various public-domain trading algorithms have been proposed over the past 25 years in a kind of arms-race, where each new trading algorithm was compared to the previous best, thereby establishing a ""pecking order"", i.e. a partially-ordered dominance hierarchy from best to worst of the various trading algorithms. Many of these algorithms were developed and tested using simple minimal simulations of financial markets that only weakly approximated the fact that real markets involve many different trading systems operating asynchronously and in parallel. In this paper we use BSE, a public-domain market simulator, to run a set of experiments generating benchmark results from several well-known trading algorithms. BSE incorporates a very simple time-sliced approach to simulating parallelism, which has obvious known weaknesses. We then alter and extend BSE to make it threaded, so that different trader algorithms operate asynchronously and in parallel: we call this simulator Threaded-BSE (TBSE). We then re-run the trader experiments on TBSE and compare the TBSE results to our earlier benchmark results from BSE. Our comparison shows that the dominance hierarchy in our more realistic experiments is different from the one given by the original simple simulator. We conclude that simulated parallelism matters a lot, and that earlier results from simple simulations comparing different trader algorithms are no longer to be entirely trusted.","Michael Rollins,Dave Cliff",,2020,,,
1070,DoubleEnsemble: A New Ensemble Method Based on Sample Reweighting and Feature Selection for Financial Data Analysis,"Modern machine learning models (such as deep neural networks and boosting decision tree models) have become increasingly popular in financial market prediction, due to their superior capacity to extract complex non-linear patterns. However, since financial datasets have very low signal-to-noise ratio and are non-stationary, complex models are often very prone to overfitting and suffer from instability issues. Moreover, as various machine learning and data mining tools become more widely used in quantitative trading, many trading firms have been producing an increasing number of features (aka factors). Therefore, how to automatically select effective features becomes an imminent problem. To address these issues, we propose DoubleEnsemble, an ensemble framework leveraging learning trajectory based sample reweighting and shuffling based feature selection. Specifically, we identify the key samples based on the training dynamics on each sample and elicit key features based on the ablation impact of each feature via shuffling. Our model is applicable to a wide range of base models, capable of extracting complex patterns, while mitigating the overfitting and instability issues for financial market prediction. We conduct extensive experiments, including price prediction for cryptocurrencies and stock trading, using both DNN and gradient boosting decision tree as base models. Our experiment results demonstrate that DoubleEnsemble achieves a superior performance compared with several baseline methods.","Chuheng Zhang,Yuanqi Li,Xi Chen,Xi Chen,Yifei Jin,Yifei Jin,Pingzhong Tang,Pingzhong Tang,Jian Li,Jian Li,Jian Li",arXiv: Learning,2020,,,
1071,Beating the Stock Market with a Deep Reinforcement Learning Day Trading System,"In this study we investigate the potential of using Deep Reinforcement Learning (DRL) to day trade stocks, taking into account the constraints imposed by the stock market, such as liquidity, latency, slippage and transaction costs. More specifically, we use a Deep Deterministic Policy Gradient (DDPG) algorithm to solve a series of asset allocation problems in order to define the percentage of capital that must be invested in each asset at each period, executing exclusively day trade operations. DDPG is a model-free, off-policy actor-critic method that can learn policies in high-dimensional and continuous action and state spaces, like the ones normally found in financial market environments. The proposed day trading system was tested in B3 - Brazil Stock Exchange, an important and understudied market, especially considering the application of DRL techniques to alpha generation. A series of experiments were performed from the beginning of 2017 until the end of 2019 and compared with ten benchmarks, including Ibovespa, the most important Brazilian market index, and the stock portfolios suggested by the main Brazilian banks and brokers during these years. The results were evaluated considering return and risk metrics and showed that the proposed method outperformed the benchmarks by a huge margin. The best results obtained by the algorithm had a cumulative percentage return of 311% in three years, with an annual average maximum drawdown around 19%.","Leonardo Conegundes,Adriano C. M. Pereira",,2020,,,
1072,Grasp for Stacking via Deep Reinforcement Learning,"Integrated robotic arm system should contain both grasp and place actions. However, most grasping methods focus more on how to grasp objects, while ignoring the placement of the grasped objects, which limits their applications in various industrial environments. In this research, we propose a model-free deep Q-learning method to learn the grasping-stacking strategy end-to-end from scratch. Our method maps the images to the actions of the robotic arm through two deep networks: the grasping network (GNet) using the observation of the desk and the pile to infer the gripper’s position and orientation for grasping, and the stacking network (SNet) using the observation of the platform to infer the optimal location when placing the grasped object. To make a long-range planning, the two observations are integrated in the grasping for stacking network (GSN). We evaluate the proposed GSN on a grasping-stacking task in both simulated and real-world scenarios.","Junhao Zhang,Wei Zhang,Wei Zhang,Wei Zhang,Wei Zhang,Wei Zhang,Ran Song,Ran Song,Lin Ma,Lin Ma,Yibin Li",,2020,,,
1073,Multimodal Multi-Task Financial Risk Forecasting,"Stock price movement and volatility prediction aim to predict stocks' future trends to help investors make sound investment decisions and model financial risk. Companies' earnings calls are a rich, underexplored source of multimodal information for financial forecasting. However, existing fintech solutions are not optimized towards harnessing the interplay between the multimodal verbal and vocal cues in earnings calls. In this work, we present a multi-task solution that utilizes domain specialized textual features and audio attentive alignment for predictive financial risk and price modeling. Our method advances existing solutions in two aspects: 1) tailoring a deep multimodal text-audio attention model, 2) optimizing volatility, and price movement prediction in a multi-task ensemble formulation. Through quantitative and qualitative analyses, we show the effectiveness of our deep multimodal approach.","Ramit Sawhney,Puneet Mathur,Ayush Mangal,Piyush Khanna,Rajiv Ratn Shah,Roger Zimmermann",,2020,,,
1074,Bridging the gap between Markowitz planning and deep reinforcement learning,"While researchers in the asset management industry have mostly focused on techniques based on financial and risk planning techniques like Markowitz efficient frontier, minimum variance, maximum diversification or equal risk parity, in parallel, another community in machine learning has started working on reinforcement learning and more particularly deep reinforcement learning to solve other decision making problems for challenging task like autonomous driving, robot learning, and on a more conceptual side games solving like Go. This paper aims to bridge the gap between these two approaches by showing Deep Reinforcement Learning (DRL) techniques can shed new lights on portfolio allocation thanks to a more general optimization setting that casts portfolio allocation as an optimal control problem that is not just a one-step optimization, but rather a continuous control optimization with a delayed reward. The advantages are numerous: (i) DRL maps directly market conditions to actions by design and hence should adapt to changing environment, (ii) DRL does not rely on any traditional financial risk assumptions like that risk is represented by variance, (iii) DRL can incorporate additional data and be a multi inputs method as opposed to more traditional optimization methods. We present on an experiment some encouraging results using convolution networks.","Eric Benhamou,Eric Benhamou,Eric Benhamou,David Saltiel,Sandrine Ungari,Abhishek Mukhopadhyay",arXiv: Learning,2020,,,
1075,Modelling local and global dependencies for next-item recommendations,"Session-based recommender systems (SBRSs) aim at predicting the next item by modelling the complex dependencies within and across sessions. Most of the existing SBRSs make recommendations only based on local dependencies (i.e., the dependencies between items within a session), while ignoring global dependencies (i.e., the dependencies across multiple sessions), leading to information loss and thus reducing the recommendation accuracy. Moreover, they are usually not able to recommend cold-start items effectively due to their limited session information. To alleviate these shortcomings of SBRSs, we propose a novel heterogeneous mixed graph learning (HMGL) framework to effectively learn both local and global dependencies for next-item recommendations. The HMGL framework mainly contains a heterogeneous mixed graph (HMG) construction module and an HMG learning module. The HMG construction module map both the session information and the item attribute information into a unified graph to connect items within and across sessions. The HMG learning module learns a unified representation for each item by simultaneously modelling the local and global dependencies over the HMG. The learned representation is then used for next-item recommendations. Results of extensive experiments on real-world datasets show the superiority of HMGL framework over the start-of-the-art methods in terms of recommendation accuracy.","Nan Wang,Shoujin Wang,Yan Wang,Yan Wang,Quan Z. Sheng,Quan Z. Sheng,Mehmet A. Orgun,Mehmet Orgun",,2020,,,
1076,Empirical analysis of session-based recommendation algorithms,"Recommender systems are tools that support online users by pointing them to potential items of interest in situations of information overload. In recent years, the class of session-based recommendation algorithms received more attention in the research literature. These algorithms base their recommendations solely on the observed interactions with the user in an ongoing session and do not require the existence of long-term preference profiles. Most recently, a number of deep learning-based (“neural”) approaches to session-based recommendations have been proposed. However, previous research indicates that today’s complex neural recommendation methods are not always better than comparably simple algorithms in terms of prediction accuracy. With this work, our goal is to shed light on the state of the art in the area of session-based recommendation and on the progress that is made with neural approaches. For this purpose, we compare twelve algorithmic approaches, among them six recent neural methods, under identical conditions on various datasets. We find that the progress in terms of prediction accuracy that is achieved with neural methods is still limited. In most cases, our experiments show that simple heuristic methods based on nearest-neighbors schemes are preferable over conceptually and computationally more complex methods. Observations from a user study furthermore indicate that recommendations based on heuristic methods were also well accepted by the study participants. To support future progress and reproducibility in this area, we publicly share the session-rec evaluation framework that was used in our research.","Malte Ludewig,Noemi Mauro,Sara Latifi,Dietmar Jannach",User Modeling and User-adapted Interaction,2020,,,
1077,Option Hedging with Risk Averse Reinforcement Learning,"In this paper we show how risk-averse reinforcement learning can be used to hedge options. We apply a state-of-the-art risk-averse algorithm: Trust Region Volatility Optimization (TRVO) to a vanilla option hedging environment, considering realistic factors such as discrete time and transaction costs. Realism makes the problem twofold: the agent must both minimize volatility and contain transaction costs, these tasks usually being in competition. We use the algorithm to train a sheaf of agents each characterized by a different risk aversion, so to be able to span an efficient frontier on the volatility-p\&l space. The results show that the derived hedging strategy not only outperforms the Black \& Scholes delta hedge, but is also extremely robust and flexible, as it can efficiently hedge options with different characteristics and work on markets with different behaviors than what was used in training.","Edoardo Vittori,Michele Trapletti,Marcello Restelli",arXiv: Trading and Market Microstructure,2020,,,
1078,The Era of Intelligent Recommendation: Editorial on Intelligent Recommendation with Advanced AI and Learning,The articles in this special section address intelligent recommender systems using advanced artificial intelligence (AI) learning applications.,"Shoujin Wang,Gabriella Pasi,Liang Hu,Longbing Cao",The Missouri Review,2020,,,
1079,The ensemble deep learning model for novel COVID-19 on CT images.,"Abstract   The rapid detection of the novel coronavirus disease, COVID-19, has a positive effect on preventing propagation and enhancing therapeutic outcomes. This article focuses on the rapid detection of COVID-19. We propose an ensemble deep learning model for novel COVID-19 detection from CT images. 2933 lung CT images from COVID-19 patients were obtained from previous publications, authoritative media reports, and public databases. The images were preprocessed to obtain 2500 high-quality images. 2500 CT images of lung tumor and 2500 from normal lung were obtained from a hospital. Transfer learning was used to initialize model parameters and pretrain three deep convolutional neural network models: AlexNet, GoogleNet, and ResNet. These models were used for feature extraction on all images. Softmax was used as the classification algorithm of the fully connected layer. The ensemble classifier EDL-COVID was obtained via relative majority voting. Finally, the ensemble classifier was compared with three component classifiers to evaluate accuracy, sensitivity, specificity, F value, and Matthews correlation coefficient. The results showed that the overall classification performance of the ensemble model was better than that of the component classifier. The evaluation indexes were also higher. This algorithm can better meet the rapid detection requirements of the novel coronavirus disease COVID-19.","Zhou Tao,Tao Zhou,Lu Huiling,Zaoli Yang,Zaoli Yang,Zaoli Yang,Shi Qiu,Shi Qiu,Shi Qiu,Huo Bingqiang,Dong Yali",Applied Soft Computing,2020,,,
1080,Generalized Negative Correlation Learning for Deep Ensembling,"Ensemble algorithms offer state of the art performance in many machine learning applications. A common explanation for their excellent performance is due to the bias-variance decomposition of the mean squared error which shows that the algorithm's error can be decomposed into its bias and variance. Both quantities are often opposed to each other and ensembles offer an effective way to manage them as they reduce the variance through a diverse set of base learners while keeping the bias low at the same time. Even though there have been numerous works on decomposing other loss functions, the exact mathematical connection is rarely exploited explicitly for ensembling, but merely used as a guiding principle. In this paper, we formulate a generalized bias-variance decomposition for arbitrary twice differentiable loss functions and study it in the context of Deep Learning. We use this decomposition to derive a Generalized Negative Correlation Learning (GNCL) algorithm which offers explicit control over the ensemble's diversity and smoothly interpolates between the two extremes of independent training and the joint training of the ensemble. We show how GNCL encapsulates many previous works and discuss under which circumstances training of an ensemble of Neural Networks might fail and what ensembling method should be favored depending on the choice of the individual networks.","Sebastian Buschjäger,Lukas Pfahler,Katharina Morik",arXiv: Learning,2020,,,
1081,Sentiment Analysis of Financial News,"Sentiment analysis is a subdiscipline covered under data mining and computational semantics. It refers to the comprehension of gathered data that is procured from sentiment rich sources like news, social media sites, reviews, and so forth. In the current era where data is becoming increasingly voluminous and yet crucial to all businesses, manual analysis of data does not remain viable in this fast-moving world. Thus, it is necessary to make use of artificial intelligence and datamining techniques. Amongst several variables, a key determinant that result in the fluctuation in stock prices is the gains or losses incurred by a company. As most traders get their information from news, it makes news as a core influential factor to forecast change in the stock market. This study focuses on sentiment classification and shows its effect on the change in stock market prices. It generates investing insight by applying sentiment analysis using VADER (Valence Aware Dictionary and Sentiment Reasoner) tool on some of the most liquid stocks.",Arul Agarwal,,2020,,,
1082,Non-IID Recommender Systems: A Review and Framework of Recommendation Paradigm Shifting,"While recommendation plays an increasingly critical role in our living, study, work, and entertainment, the recommendations we receive are often for irrelevant, duplicate, or uninteresting products and services. A critical reason for such bad recommendations lies in the intrinsic assumption that recommended users and items are independent and identically distributed (IID) in existing theories and systems. Another phenomenon is that, while tremendous efforts have been made to model specific aspects of users or items, the overall user and item characteristics and their non-IIDness have been overlooked. In this paper, the non-IID nature and characteristics of recommendation are discussed, followed by the non-IID theoretical framework in order to build a deep and comprehensive understanding of the intrinsic nature of recommendation problems, from the perspective of both couplings and heterogeneity. This non-IID recommendation research triggers the paradigm shift from IID to non-IID recommendation research and can hopefully deliver informed, relevant, personalized, and actionable recommendations. It creates exciting new directions and fundamental solutions to address various complexities including cold-start, sparse data-based, cross-domain, group-based, and shilling attack-related issues.",Longbing Cao,arXiv: Information Retrieval,2020,,,
1083,RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems,"To address the sparsity and cold start problem of collaborative filtering, researchers usually make use of side information, such as social networks or item attributes, to improve recommendation performance. This paper considers the knowledge graph as the source of side information. To address the limitations of existing embedding-based and path-based methods for knowledge-graph-aware recommendation, we propose RippleNet, an end-to-end framework that naturally incorporates the knowledge graph into recommender systems. Similar to actual ripples propagating on the water, RippleNet stimulates the propagation of user preferences over the set of knowledge entities by automatically and iteratively extending a user's potential interests along links in the knowledge graph. The multiple ""ripples"" activated by a user's historically clicked items are thus superposed to form the preference distribution of the user with respect to a candidate item, which could be used for predicting the final clicking probability. Through extensive experiments on real-world datasets, we demonstrate that RippleNet achieves substantial gains in a variety of scenarios, including movie, book and news recommendation, over several state-of-the-art baselines.","Hongwei Wang,Fuzheng Zhang,Jialin Wang,Miao Zhao,Miao Zhao,Wenjie Li,Xing Xie,Minyi Guo",,2018,,,
1084,Personalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks,"Session-based recommendations are highly relevant in many modern on-line services (e.g. e-commerce, video streaming) and recommendation settings. Recently, Recurrent Neural Networks have been shown to perform very well in session-based settings. While in many session-based recommendation domains user identifiers are hard to come by, there are also domains in which user profiles are readily available. We propose a seamless way to personalize RNN models with cross-session information transfer and devise a Hierarchical RNN model that relays end evolves latent hidden states of the RNNs across user sessions. Results on two industry datasets show large improvements over the session-only RNNs.","Massimo Quadrana,Alexandros Karatzoglou,Balázs Hidasi,Paolo Cremonesi",,2017,,,
1085,Twitter mood predicts the stock market,"Behavioral economics tells us that emotions can profoundly affect individual behavior and decision-making. Does this also apply to societies at large, i.e., can societies experience mood states that affect their collective decision making? By extension is the public mood correlated or even predictive of economic indicators? Here we investigate whether measurements of collective mood states derived from large-scale Twitter feeds are correlated to the value of the Dow Jones Industrial Average (DJIA) over time. We analyze the text content of daily Twitter feeds by two mood tracking tools, namely OpinionFinder that measures positive vs. negative mood and Google-Profile of Mood States (GPOMS) that measures mood in terms of 6 dimensions (Calm, Alert, Sure, Vital, Kind, and Happy). We cross-validate the resulting mood time series by comparing their ability to detect the public's response to the presidential election and Thanksgiving day in 2008. A Granger causality analysis and a Self-Organizing Fuzzy Neural Network are then used to investigate the hypothesis that public mood states, as measured by the OpinionFinder and GPOMS mood time series, are predictive of changes in DJIA closing values. Our results indicate that the accuracy of DJIA predictions can be significantly improved by the inclusion of specific public mood dimensions but not others. We find an accuracy of 87.6% in predicting the daily up and down changes in the closing values of the DJIA and a reduction of the Mean Average Percentage Error by more than 6%.","Johan Bollen,Huina Mao,Xiao-Jun Zeng","arXiv: Computational Engineering, Finance, and Science",2010,,,
1086,ChainSplitter: Towards Blockchain-Based Industrial IoT Architecture for Supporting Hierarchical Storage,"The fast developing Industrial Internet of Things (IIoT) technologies provide a promising opportunity to build large-scale systems to connect numerous heterogeneous devices into the Internet. Most existing IIoT infrastructures are based on a centralized architecture, which is easier for management but cannot effectively support immutable and verifiable services among multiple parties. Blockchain technology provides many desired features for large-scale IIoT infrastructures, such as decentralization, trustworthiness, trackability, and immutability. This paper presents a blockchain-based IIoT architecture to support immutable and verifiable services. However, when applying blockchain technology to the IIoT infrastructure, the required storage space posts a grant challenge to resource-constrained IIoT infrastructures. To address the storage issue, this paper proposes a hierarchical blockchain storage structure, ChainSplitter. Specially, the proposed architecture features a hierarchical storage structure where the majority of the blockchain is stored in the clouds, while the most recent blocks are stored in the overlay network of the individual IIoT networks. The proposed architecture seamlessly binds local IIoT networks, the blockchain overlay network, and the cloud infrastructure together through two connectors, the blockchain connector and the cloud connector, to construct the hierarchical blockchain storage. The blockchain connector in the overlay network builds blocks in blockchain from data generated in IIoT networks, and the cloud connector resolves the blockchain synchronization issues between the overlay network and the clouds. We also provide a case study to show the efficiency of the proposed hierarchical blockchain storage in a practical Industrial IoT case.","Gang Wang,Gang Wang,Gang Wang,Gang Wang,Zhijie Shi,Mark J. Nixon,Mark J. Nixon,Mark Nixon,Song Han,Song Han,Song Han",,2019,,,
1087,Market Basket Prediction Using User-Centric Temporal Annotated Recurring Sequences,"Nowadays, a hot challenge for supermarket chains is to offer personalized services to their customers. Market basket prediction, i.e., supplying the customer a shopping list for the next purchase according to her current needs, is one of these services. Current approaches are not capable of capturing at the same time the different factors influencing the customer's decision process: co-occurrence, sequentuality, periodicity and recurrency of the purchased items. To this aim, we define a pattern named Temporal Annotated Recurring Sequence (TARS). We define the method to extract TARS and develop a predictor for next basket named TBP (TARS Based Predictor) that, on top of TARS, is able to understand the level of the customer's stocks and recommend the set of most necessary items. A deep experimentation shows that TARS can explain the customers' purchase behavior, and that TBP outperforms the state-of-the-art competitors.","Riccardo Guidotti,Giulio Rossetti,Luca Pappalardo,Fosca Giannotti,Dino Pedreschi,Dino Pedreschi,Dino Pedreschi,Dino Pedreschi",,2017,,,
1088,Acoustic Monitoring of the Psycho-Emotional State of Operational Personnel in the Management of High-Risk Objects Based on Neuromorphic Self-Learning Systems,"High confidence in the definition of operational personnel psycho-emotional state is achieved by using a modern element base (processing environment) - a unified neuromorphic platform in the form of a fractal memristive structure with space-parametric self-learning. Experimental study of the proposed platform revealed its ability to synthesize new structural solutions and ""dimensions"" of the operational personnel psycho-emotional state information representation.","Alexander V. Alyushin,Vasilii G. Arkhangelsky,Sergey A. Alyushin",,2020,,,
1089,Automated detection of COVID-19 using ensemble of transfer learning with deep convolutional neural network based on CT scans.,"Purpose  COVID-19 has infected millions of people worldwide. One of the most important hurdles in controlling the spread of this disease is the inefficiency and lack of medical tests. Computed tomography (CT) scans are promising in providing accurate and fast detection of COVID-19. However, determining COVID-19 requires highly trained radiologists and suffers from inter-observer variability. To remedy these limitations, this paper introduces an automatic methodology based on an ensemble of deep transfer learning for the detection of COVID-19.  Methods  A total of 15 pre-trained convolutional neural networks (CNNs) architectures: EfficientNets(B0-B5), NasNetLarge, NasNetMobile, InceptionV3, ResNet-50, SeResnet 50, Xception, DenseNet121, ResNext50 and Inception_resnet_v2 are used and then fine-tuned on the target task. After that, we built an ensemble method based on majority voting of the best combination of deep transfer learning outputs to further improve the recognition performance. We have used a publicly available dataset of CT scans, which consists of 349 CT scans labeled as being positive for COVID-19 and 397 negative COVID-19 CT scans that are normal or contain other types of lung diseases.  Results  The experimental results indicate that the majority voting of 5 deep transfer learning architecture with EfficientNetB0, EfficientNetB3, EfficientNetB5, Inception_resnet_v2, and Xception has the higher results than the individual transfer learning structure and among the other models based on precision (0.857), recall (0.854) and accuracy (0.85) metrics in diagnosing COVID-19 from CT scans.  Conclusion  Our study based on an ensemble deep transfer learning system with different pre-trained CNNs architectures can work well on a publicly available dataset of CT images for the diagnosis of COVID-19 based on CT scans.","Parisa Gifani,Parisa Gifani,Ahmad Shalbaf,Majid Vafaeezadeh",International Journal of Computer Assisted Radiology and Surgery,2020,,,
1090,"Sequential Recommender Systems: Challenges, Progress and Prospects","The emerging topic of sequential recommender systems has attracted increasing attention in recent years.Different from the conventional recommender systems including collaborative filtering and content-based filtering, SRSs try to understand and model the sequential user behaviors, the interactions between users and items, and the evolution of users preferences and item popularity over time. SRSs involve the above aspects for more precise characterization of user contexts, intent and goals, and item consumption trend, leading to more accurate, customized and dynamic recommendations.In this paper, we provide a systematic review on SRSs.We first present the characteristics of SRSs, and then summarize and categorize the key challenges in this research area, followed by the corresponding research progress consisting of the most recent and representative developments on this topic.Finally, we discuss the important research directions in this vibrant area.","Shoujin Wang,Liang Hu,Yan Wang,Longbing Cao,Quan Z. Sheng,Mehmet A. Orgun",,2019,,,
1091,Evaluation of session-based recommendation algorithms,"Recommender systems help users find relevant items of interest, for example on e-commerce or media streaming sites. Most academic research is concerned with approaches that personalize the recommendations according to long-term user profiles. In many real-world applications, however, such long-term profiles often do not exist and recommendations therefore have to be made solely based on the observed behavior of a user during an ongoing session. Given the high practical relevance of the problem, an increased interest in this problem can be observed in recent years, leading to a number of proposals for session-based recommendation algorithms that typically aim to predict the user’s immediate next actions. In this work, we present the results of an in-depth performance comparison of a number of such algorithms, using a variety of datasets and evaluation measures. Our comparison includes the most recent approaches based on recurrent neural networks like gru4rec, factorized Markov model approaches such as fism or fossil, as well as simpler methods based, e.g., on nearest neighbor schemes. Our experiments reveal that algorithms of this latter class, despite their sometimes almost trivial nature, often perform equally well or significantly better than today’s more complex approaches based on deep neural networks. Our results therefore suggest that there is substantial room for improvement regarding the development of more sophisticated session-based recommendation algorithms.","Malte Ludewig,Dietmar Jannach",User Modeling and User-adapted Interaction,2018,,,
1092,Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks,"Predicting a user's preference in a short anonymous interaction session instead of long-term history is a challenging problem in the real-life session-based recommendation, e.g., e-commerce and media stream. Recent research of the session-based recommender system mainly focuses on sequential patterns by utilizing the attention mechanism, which is straightforward for the session's natural sequence sorted by time. However, the user's preference is much more complicated than a solely consecutive time pattern in the transition of item choices. In this paper, therefore, we study the item transition pattern by constructing a session graph and propose a novel model which collaboratively considers the sequence order and the latent order in the session graph for a session-based recommender system. We formulate the next item recommendation within the session as a graph classification problem. Specifically, we propose a weighted attention graph layer and a Readout function to learn embeddings of items and sessions for the next item recommendation. Extensive experiments have been conducted on two benchmark E-commerce datasets, Yoochoose and Diginetica, and the experimental results show that our model outperforms other state-of-the-art methods.","Qiu Ruihong,Ruihong Qiu,Jingjing Li,Li Jingjing,Zi Huang,Huang Zi,Yin Hongzhi,Hongzhi Yin",arXiv: Information Retrieval,2019,,,
1093,Session-Based Recommendation with Graph Neural Networks,"The problem of session-based recommendation aims to predict user actions based on anonymous sessions. Previous methods model a session as a sequence and estimate user representations besides item representations to make recommendations. Though achieved promising results, they are insufficient to obtain accurate user vectors in sessions and neglect complex transitions of items. To obtain accurate item embedding and take complex transitions of items into account, we propose a novel method, i.e. Session-based Recommendation with Graph Neural Networks, SR-GNN for brevity. In the proposed method, session sequences are modeled as graphstructured data. Based on the session graph, GNN can capture complex transitions of items, which are difficult to be revealed by previous conventional sequential methods. Each session is then represented as the composition of the global preference and the current interest of that session using an attention network. Extensive experiments conducted on two real datasets show that SR-GNN evidently outperforms the state-of-the-art session-based recommendation methods consistently.","Shu Wu,Yuyuan Tang,Yanqiao Zhu,Liang Wang,Liang Wang,Liang Wang,Liang Wang,Xing Xie,Tieniu Tan",,2019,,,
1094,AlphaStock: A Buying-Winners-and-Selling-Losers Investment Strategy using Interpretable Deep Reinforcement Attention Networks,"Recent years have witnessed the successful marriage of finance innovations and AI techniques in various finance applications including quantitative trading (QT). Despite great research efforts devoted to leveraging deep learning (DL) methods for building better QT strategies, existing studies still face serious challenges especially from the side of finance, such as the balance of risk and return, the resistance to extreme loss, and the interpretability of strategies, which limit the application of DL-based strategies in real-life financial markets. In this work, we propose AlphaStock, a novel reinforcement learning (RL) based investment strategy enhanced by interpretable deep attention networks, to address the above challenges. Our main contributions are summarized as follows: i) We integrate deep attention networks with a Sharpe ratio-oriented reinforcement learning framework to achieve a risk-return balanced investment strategy; ii) We suggest modeling interrelationships among assets to avoid selection bias and develop a cross-asset attention mechanism; iii) To our best knowledge, this work is among the first to offer an interpretable investment strategy using deep reinforcement learning models. The experiments on long-periodic U.S. and Chinese markets demonstrate the effectiveness and robustness of AlphaStock over diverse market states. It turns out that AlphaStock tends to select the stocks as winners with high long-term growth, low volatility, high intrinsic value, and being undervalued recently.","Jingyuan Wang,Jingyuan Wang,Yang Zhang,Ke Tang,Junjie Wu,Junjie Wu,Junjie Wu,Zhang Xiong,Zhang Xiong,Zhang Xiong,Zhang Xiong",arXiv: Trading and Market Microstructure,2019,,,
1095,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,"RNNs have been shown to be excellent models for sequential data and in particular for data that is generated by users in an session-based manner. The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations. In this work we introduce novel ranking loss functions tailored to RNNs in the recommendation setting. The improved performance of these losses over alternatives, along with further tricks and refinements described in this work, allow for an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 53% over classical collaborative filtering approaches. Unlike data augmentation-based improvements, our method does not increase training times significantly. We further demonstrate the performance gain of the RNN over baselines in an online A/B test.","Balázs Hidasi,Alexandros Karatzoglou",,2018,,,
1096,Towards Neural Mixture Recommender for Long Range Dependent User Sequences,"Understanding temporal dynamics has proved to be highly valuable for accurate recommendation. Sequential recommenders have been successful in modeling the dynamics of users and items over time. However, while different model architectures excel at capturing various temporal ranges or dynamics, distinct application contexts require adapting to diverse behaviors. In this paper we examine how to build a model that can make use of different temporal ranges and dynamics depending on the request context. We begin with the analysis of an anonymized Youtube dataset comprising millions of user sequences. We quantify the degree of long-range dependence in these sequences and demonstrate that both short-term and long-term dependent behavioral patterns co-exist. We then propose a neural Multi-temporal-range Mixture Model (M3) as a tailored solution to deal with both short-term and long-term dependencies. Our approach employs a mixture of models, each with a different temporal range. These models are combined by a learned gating mechanism capable of exerting different model combinations given different contextual information. In empirical evaluations on a public dataset and our own anonymized YouTube dataset, M3 consistently outperforms state-of-the-art sequential recommendation methods.","Jiaxi Tang,Francois Belletti,Sagar Jain,Minmin Chen,Alex Beutel,Can Xu,Can Xu,Can Xu,Ed H. Chi",arXiv: Learning,2019,,,
1097,Deep reinforcement learning for page-wise recommendations,"Recommender systems can mitigate the information overload problem by suggesting users' personalized items. In real-world recommendations such as e-commerce, a typical interaction between the system and its users is - users are recommended a page of items and provide feedback; and then the system recommends a new page of items. To effectively capture such interaction for recommendations, we need to solve two key problems - (1) how to update recommending strategy according to user's real-time feedback, and 2) how to generate a page of items with proper display, which pose tremendous challenges to traditional recommender systems. In this paper, we study the problem of page-wise recommendations aiming to address aforementioned two challenges simultaneously. In particular, we propose a principled approach to jointly generate a set of complementary items and the corresponding strategy to display them in a 2-D page; and propose a novel page-wise recommendation framework based on deep reinforcement learning, DeepPage, which can optimize a page of items with proper display based on real-time feedback from users. The experimental results based on a real-world e-commerce dataset demonstrate the effectiveness of the proposed framework.","Xiangyu Zhao,Xiangyu Zhao,Long Xia,Liang Zhang,Zhuoye Ding,Dawei Yin,Jiliang Tang",,2018,,,
1098,Session-aware Information Embedding for E-commerce Product Recommendation,"Most of the existing recommender systems assume that user's visiting history can be constantly recorded. However, in recent online services, the user identification may be usually unknown and only limited online user behaviors can be used. It is of great importance to model the temporal online user behaviors and conduct recommendation for the anonymous users. In this paper, we propose a list-wise deep neural network based architecture to model the limited user behaviors within each session. To train the model efficiently, we first design a session embedding method to pre-train a session representation, which incorporates different kinds of user search behaviors such as clicks and views. Based on the learnt session representation, we further propose a list-wise ranking model to generate the recommendation result for each anonymous user session. We conduct quantitative experiments on a recently published dataset from an e-commerce company. The evaluation results validate the effectiveness of the proposed method, which can outperform the state-of-the-art.","Chen Wu,Chen Wu,Chen Wu,Chen Wu,Ming Yan,Ming Yan",,2017,,,
1099,Quant GANs: Deep Generation of Financial Time Series,"Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity.","Magnus Wiese,Robert Knobloch,Ralf Korn,Peter Kretschmer",arXiv: Mathematical Finance,2019,,,
1100,"Federated Learning: Challenges, Methods, and Future Directions.","Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.","Tian Li,Anit Kumar Sahu,Ameet Talwalkar,Virginia Smith",arXiv: Learning,2019,,,
1101,DeepWalk: online learning of social representations,"We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.   DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data.   DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.","Bryan Perozzi,Rami Al-Rfou,Steven Skiena",,2014,,,
1102,Scalable reinforcement-learning-based neural architecture search for cancer deep learning research,"Cancer is a complex disease, the understanding and treatment of which are being aided through increases in the volume of collected data and in the scale of deployed computing power. Consequently, there is a growing need for the development of data-driven and, in particular, deep learning methods for various tasks such as cancer diagnosis, detection, prognosis, and prediction. Despite recent successes, however, designing high-performing deep learning models for nonimage and nontext cancer data is a time-consuming, trial-and-error, manual task that requires both cancer domain and deep learning expertise. To that end, we develop a reinforcement-learning-based neural architecture search to automate deep-learning-based predictive model development for a class of representative cancer data. We develop custom building blocks that allow domain experts to incorporate the cancer-data-specific characteristics. We show that our approach discovers deep neural network architectures that have significantly fewer trainable parameters, shorter training time, and accuracy similar to or higher than those of manually designed architectures. We study and demonstrate the scalability of our approach on up to 1,024 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.","Prasanna Balaprakash,Romain Egele,Romain Egele,Romain Egele,Misha Salim,Misha Salim,Stefan M. Wild,Venkatram Vishwanath,Fangfang Xia,Thomas Brettin,Rick Stevens,Rick Stevens,Rick Stevens",,2019,,,
1103,Incorporating User Micro-behaviors and Item Knowledge into Multi-task Learning for Session-based Recommendation,,"Wenjing Meng,Deqing Yang,Yanghua Xiao",,2020,,,
1104,Transfer Learning for sEMG-based Hand Gesture Classification using Deep Learning in a Master- Slave Architecture,Recent advancements in diagnostic learning and development of gesture-based human machine interfaces have driven surface electromyography (sEMG) towards significant importance. Analysis of hand gestures requires an accurate assessment of sEMG signals. The proposed work presents a novel sequential master-slave architecture consisting of deep neural networks (DNNs) for classification of signs from the Indian sign language using signals recorded from multiple sEMG channels. The performance of the master-slave network is augmented by leveraging additional synthetic feature data generated by long short term memory networks. Performance of the proposed network is compared to that of a conventional DNN prior to and after the addition of synthetic data. Up to 14% improvement is observed in the conventional DNN and up to 9% improvement in master-slave network on addition of synthetic data with an average accuracy value of 93.5% asserting the suitability of the proposed approach.,"Karush Suri,Rinki Gupta",,2018,,,
1105,"Deep Learning for Sequential Recommendation: Algorithms, Influential Factors, and Evaluations","In the field of sequential recommendation, deep learning--(DL) based methods have received a lot of attention in the past few years and surpassed traditional models such as Markov chain-based and f...","Hui Fang,FangHui,ZhangDanning,Danning Zhang,ShuYiheng,Yiheng Shu,GuoGuibing,Guibing Guo",ACM Transactions on Information Systems,2020,,,
1106,Deep neural ensemble for retinal vessel segmentation in fundus images towards achieving label-free angiography,"Automated segmentation of retinal blood vessels in label-free fundus images entails a pivotal role in computed aided diagnosis of ophthalmic pathologies, viz., diabetic retinopathy, hypertensive disorders and cardiovascular diseases. The challenge remains active in medical image analysis research due to varied distribution of blood vessels, which manifest variations in their dimensions of physical appearance against a noisy background. In this paper we formulate the segmentation challenge as a classification task. Specifically, we employ unsupervised hierarchical feature learning using ensemble of two level of sparsely trained denoised stacked autoencoder. First level training with bootstrap samples ensures decoupling and second level ensemble formed by different network architectures ensures architectural revision. We show that ensemble training of auto-encoders fosters diversity in learning dictionary of visual kernels for vessel segmentation. SoftMax classifier is used for fine tuning each member autoencoder and multiple strategies are explored for 2-level fusion of ensemble members. On DRIVE dataset, we achieve maximum average accuracy of 95.33% with an impressively low standard deviation of 0.003 and Kappa agreement coefficient of 0.708. Comparison with other major algorithms substantiates the high efficacy of our model.","Avisek Lahiri,Abhijit Roy,Abhijit Guha Roy,Debdoot Sheet,Prabir Kumar Biswas",,2016,,,
1107,GAG: Global Attributed Graph Neural Network for Streaming Session-based Recommendation,"Streaming session-based recommendation (SSR) is a challenging task that requires the recommender system to do the session-based recommendation (SR) in the streaming scenario. In the real-world applications of e-commerce and social media, a sequence of user-item interactions generated within a certain period are grouped as a session, and these sessions consecutively arrive in the form of streams. Most of the recent SR research has focused on the static setting where the training data is first acquired and then used to train a session-based recommender model. They need several epochs of training over the whole dataset, which is infeasible in the streaming setting. Besides, they can hardly well capture long-term user interests because of the neglect or the simple usage of the user information. Although some streaming recommendation strategies have been proposed recently, they are designed for streams of individual interactions rather than streams of sessions. In this paper, we propose a G lobal A ttributed G raph (GAG) neural network model with a Wasserstein reservoir for the SSR problem. On one hand, when a new session arrives, a session graph with a global attribute is constructed based on the current session and its associate user. Thus, the GAG can take both the global attribute and the current session into consideration to learn more comprehensive representations of the session and the user, yielding a better performance in the recommendation. On the other hand, for the adaptation to the streaming session scenario, a Wasserstein reservoir is proposed to help preserve a representative sketch of the historical data. Extensive experiments on two real-world datasets have been conducted to verify the superiority of the GAG model compared with the state-of-the-art methods.","Ruihong Qiu,Hongzhi Yin,Zi Huang,Tong Chen",,2020,,,
1108,FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading in Quantitative Finance,"As deep reinforcement learning (DRL) has been recognized as an effective approach in quantitative finance, getting hands-on experiences is attractive to beginners. However, to train a practical DRL trading agent that decides where to trade, at what price, and what quantity involves error-prone and arduous development and debugging. In this paper, we introduce a DRL library FinRL that facilitates beginners to expose themselves to quantitative finance and to develop their own stock trading strategies. Along with easily-reproducible tutorials, FinRL library allows users to streamline their own developments and to compare with existing schemes easily. Within FinRL, virtual environments are configured with stock market datasets, trading agents are trained with neural networks, and extensive backtesting is analyzed via trading performance. Moreover, it incorporates important trading constraints such as transaction cost, market liquidity and the investor's degree of risk-aversion. FinRL is featured with completeness, hands-on tutorial and reproducibility that favors beginners: (i) at multiple levels of time granularity, FinRL simulates trading environments across various stock markets, including NASDAQ-100, DJIA, SP (ii) organized in a layered architecture with modular structure, FinRL provides fine-tuned state-of-the-art DRL algorithms (DQN, DDPG, PPO, SAC, A2C, TD3, etc.), commonly-used reward functions and standard evaluation baselines to alleviate the debugging workloads and promote the reproducibility, and (iii) being highly extendable, FinRL reserves a complete set of user-import interfaces. Furthermore, we incorporated three application demonstrations, namely single stock trading, multiple stock trading, and portfolio allocation. The FinRL library will be available on Github at link this https URL.","Xiao-Yang Liu,Xiao-Yang Liu,Hongyang Yang,Hongyang Yang,Qian Chen,Runjia Zhang,Liuqing Yang,Liuqing Yang,Bowen Xiao,Christina Dan Wang,Christina Wang,Christina Dan Wang",arXiv: Trading and Market Microstructure,2020,,,
1109,TAGNN: Target Attentive Graph Neural Networks for Session-based Recommendation,"Session-based recommendation nowadays plays a vital role in many websites, which aims to predict users' actions based on anonymous sessions. There have emerged many studies that model a session as a sequence or a graph via investigating temporal transitions of items in a session. However, these methods compress a session into one fixed representation vector without considering the target items to be predicted. The fixed vector will restrict the representation ability of the recommender model, considering the diversity of target items and users' interests. In this paper, we propose a novel target attentive graph neural network (TAGNN) model for session-based recommendation. In TAGNN, target-aware attention adaptively activates different user interests with respect to varied target items. The learned interest representation vector varies with different target items, greatly improving the expressiveness of the model. Moreover, TAGNN harnesses the power of graph neural networks to capture rich item transitions in sessions. Comprehensive experiments conducted on real-world datasets demonstrate its superiority over state-of-the-art methods.","Feng Yu,Yanqiao Zhu,Qiang Liu,Qiang Liu,Shu Wu,Liang Wang,Liang Wang,Liang Wang,Tieniu Tan",,2020,,,
1110,A CNN-BiLSTM-AM method for stock price prediction,"In recent years, with the rapid development of the economy, more and more people begin to invest into the stock market. Accurately predicting the change of stock price can reduce the investment risk of stock investors and effectively improve the investment return. Due to the volatility characteristics of the stock market, stock price prediction is often a nonlinear time series prediction. Stock price is affected by many factors. It is difficult to predict through a simple model. Therefore, this paper proposes a CNN-BiLSTM-AM method to predict the stock closing price of the next day. This method is composed of convolutional neural networks (CNN), bi-directional long short-term Memory (BiLSTM), and attention mechanism (AM). CNN is used to extract the features of the input data. BiLSTM uses the extracted feature data to predict stock closing price of the next day. AM is used to capture the influence of feature states on the stock closing price at different times in the past to improve the prediction accuracy. In order to prove the effectiveness of this method, this method and other seven methods are used to predict the stock closing price of the next day for 1000 trading days of the Shanghai Composite Index. The results show that the performance of this method is the best, MAE and RMSE are the smallest (which are 21.952 and 31.694). R2 is the largest (its value is 0.9804). Compared with other methods, the CNN-BiLSTM-AM method is more suitable for the prediction of stock price and for providing a reliable way for investors’ to make stock investment decisions.","Wenjie Lu,Jiazheng Li,Jiazheng Li,Jingyang Wang,Jingyang Wang,Lele Qin",Neural Computing and Applications,2020,,,
1111,A CNN-LSTM-Based Model to Forecast Stock Prices,"Stock price data have the characteristics of time series. At the same time, based on machine learning long short-term memory (LSTM) which has the advantages of analyzing relationships among time series data through its memory function, we propose a forecasting method of stock price based on CNN-LSTM. In the meanwhile, we use MLP, CNN, RNN, LSTM, CNN-RNN, and other forecasting models to predict the stock price one by one. Moreover, the forecasting results of these models are analyzed and compared. The data utilized in this research concern the daily stock prices from July 1, 1991, to August 31, 2020, including 7127 trading days. In terms of historical data, we choose eight features, including opening price, highest price, lowest price, closing price, volume, turnover, ups and downs, and change. Firstly, we adopt CNN to efficiently extract features from the data, which are the items of the previous 10 days. And then, we adopt LSTM to predict the stock price with the extracted feature data. According to the experimental results, the CNN-LSTM can provide a reliable stock price forecasting with the highest prediction accuracy. This forecasting method not only provides a new research idea for stock price forecasting but also provides practical experience for scholars to study financial time series data.","Wenjie Lu,Jiazheng Li,Jiazheng Li,Yifan Li,Aijun Sun,Jianxuan Wang,Jingyang Wang",Complexity,2020,,,
1112,ETH Relay: A Cost-efficient Relay for Ethereum-based Blockchains,"Current blockchain relay schemes require the immediate validation of each relayed block header by the destination blockchain. This leads to high operating cost when deploying these relays between Ethereum-based blockchains where validating block headers on-chain is computationally expensive.To overcome these limitations, we introduce a novel relay scheme that employs a validation-on-demand pattern combined with economic incentives to reduce the cost of operating a relay between Ethereum-based blockchains by up to 92%. With this relay scheme, decentralized interoperability between blockchains like Ethereum and Ethereum Classic becomes feasible.","Philipp Frauenthaler,Marten Sigwart,Christof Spanring,Michael Sober,Stefan Schulte-Merker,Stefan Schulte",,2020,,,
1113,European Nordic Countries Stock Market Listed Companies’: Factor and Cluster Analysis Approach,"Public financial markets are crucial in the access to the funding and as a platform for investments to the investors in today’s world. Nordic European Union countries such as Sweden, Finland and Denmark are considered to have advanced and well-developed stock markets, while neighboring three Baltic States have rather small stock market. Backbone of the stock market are there listed companies. In this analysis authors attempt to analyze 510 Nordic countries listed companies’ absolute value indicators using factor and cluster analysis and to compare results with similar analysis of the Baltic States. Factor and cluster analysis revealed the homogeneity of Nordic countries stock market listed companies’ absolute values, authors obtained three complex factors, explaining 89% of dispersion within the indicators, which in turn resulted in being able to obtain the portrait of Nordic States stock market listed company. Similar results were obtained for Baltic States listed companies, though on different scale. Authors have not seen as detailed analysis of Nordic Stock market on the level of listed companies financial statement analysis. Time period covered in this research of the financials are from 2004 to 2018. The analysis could be beneficial for other researchers focusing on the Nordic region stock market companies and also to the policy makers in the Baltic States, how the neighboring well-developed countries indicators could be interpreted and obtained results used for the enhancement of Baltic States stock market.    Doi:   10.28991/esj-2020-01244     Full Text:   PDF","Aija Pilvere-Javorska,Irina Pilvere,Irina Pilvere",,2020,,,
1114,A new ensemble convolutional neural network with diversity regularization for fault diagnosis,"Abstract   Fault diagnosis is an essential technique to ensure the safety in modern industry. With the development of smart manufacturing, deep learning (DL) has been widely used to handle with massive mechanical data in fault diagnosis. However, the individual DL method suffers from the low generalization ability. In this research, a new improved snapshot ensemble Convolutional Neural Network (ISECNN) is proposed in order to obtain a stable and well-performed DL based fault diagnosis method. ISECNN applies the diversity regularization to generate several local minima and keeps their diversity during the training process, as the increasing of the diverse would promote the generalization ability of the group of local minima. Then, ISECNN combines all the local minima to form the ensemble method. The proposed ISECNN has been conducted on two famous bearing datasets. The prediction accuracy and the standard deviation are applied as the criterion. The experimental results show that ISECNN can increase the generalization ability without decreasing the prediction accuracy. ISECNN is also compared with traditional DL and machine learning methods, and the results validate the potential performance of ISECNN in the fault diagnosis field.","Long Wen,Long Wen,Long Wen,Xiaotong Xie,Quan-Ke Pan,Xinyu Li,Liang Gao",Journal of Manufacturing Systems,2020,,,
1115,Commission Fee is not Enough: A Hierarchical Reinforced Framework for Portfolio Management,"Portfolio management via reinforcement learning is at the forefront of fintech research, which explores how to optimally reallocate a fund into different financial assets over the long term by trial-and-error. Existing methods are impractical since they usually assume each reallocation can be finished immediately and thus ignoring the price slippage as part of the trading cost. To address these issues, we propose a hierarchical reinforced stock trading system for portfolio management (HRPM). Concretely, we decompose the trading process into a hierarchy of portfolio management over trade execution and train the corresponding policies. The high-level policy gives portfolio weights at a lower frequency to maximize the long term profit and invokes the low-level policy to sell or buy the corresponding shares within a short time window at a higher frequency to minimize the trading cost. We train two levels of policies via pre-training scheme and iterative training scheme for data efficiency. Extensive experimental results in the U.S. market and the China market demonstrate that HRPM achieves significant improvement against many state-of-the-art approaches.","Rundong Wang,Rundong Wang,Hongxin Wei,Hongxin Wei,Bo An,Zhouyan Feng,Jun Yao",arXiv: Artificial Intelligence,2020,,,
1116,CURL: Contrastive Unsupervised Representations for Reinforcement Learning,"We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at this https URL.","Aravind Srinivas,Michael Laskin,Pieter Abbeel",,2020,,,
1117,"Mastering Atari, Go, chess and shogi by planning with a learned model","Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3-the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4-the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi-canonical environments for high-performance planning-the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.","Julian Schrittwieser,Ioannis Antonoglou,Thomas Hubert,Karen Simonyan,Laurent Sifre,Simon Schmitt,Arthur Guez,Edward Lockhart,Demis Hassabis,Thore Graepel,Timothy Lillicrap,David Silver,David L. Silver",Nature,2020,,,
1118,Methods Matter: A Trading Agent with No Intelligence Routinely Outperforms AI-Based Traders,"There is a long tradition of research using computational intelligence, i.e. methods from artificial intelligence (AI) and machine learning (ML), to automatically discover, implement, and fine-tune strategies for autonomous adaptive automated trading in financial markets, with a sequence of research papers on this topic published at major AI conferences such as IJCAI and in prestigious journals such as Artificial Intelligence: we show evidence here that this strand of research has taken a number of methodological mis-steps and that actually some of the reportedly best-performing public-domain AI/ML trading strategies can routinely be out-performed by extremely simple trading strategies that involve no AI or ML at all. The results that we highlight here could easily have been revealed at the time that the relevant key papers were published, more than a decade ago, but the accepted methodology at the time of those publications involved a somewhat minimal approach to experimental evaluation of trader-agents, making claims on the basis of a few thousand test-sessions of the trader-agent in a small number of market scenarios. In this paper we present results from exhaustive testing over wide ranges of parameter values, using parallel cloud-computing facilities, where we conduct millions of tests and thereby create much richer data from which firmer conclusions can be drawn. We show that the best public-domain AI/ML traders in the published literature can be routinely outperformed by a “sub-zero-intelligence” trading strategy that at face value appears to be so simple as to be financially ruinous, but which interacts with the market in such a way that in practice it is more profitable than the well-known AI/ML strategies from the research literature. That such a simple strategy can outperform established AI/ML-based strategies is a sign that perhaps the AI/ML trading strategies were good answers to the wrong question.","Dave Cliff,Michael Rollins",,2020,,,
1119,Exploring the influence of multimodal social media data on stock performance: an empirical perspective and analysis,"Despite the extensive academic interest in social media sentiment for financial fields, multimodal data in the stock market has been neglected. The purpose of this paper is to explore the influence of multimodal social media data on stock performance, and investigate the underlying mechanism of two forms of social media data, i.e. text and pictures.,This research employs panel vector autoregressive models to quantify the effect of the sentiment derived from two modalities in social media, i.e. text information and picture information. Through the models, the authors examine the short-term and long-term associations between social media sentiment and stock performance, measured by three metrics. Specifically, the authors design an enhanced sentiment analysis method, integrating random walk and word embeddings through Global Vectors for Word Representation (GloVe), to construct a domain-specific lexicon and apply it to textual sentiment analysis. Secondly, the authors exploit a deep learning framework based on convolutional neural networks to analyze the sentiment in picture data.,The empirical results derived from vector autoregressive models reveal that both measures of the sentiment extracted from textual information and pictorial information in social media are significant leading indicators of stock performance. Moreover, pictorial information and textual information have similar relationships with stock performance.,To the best of the authors’ knowledge, this is the first study that incorporates multimodal social media data for sentiment analysis, which is valuable in understanding pictures of social media data. The study offers significant implications for researchers and practitioners. This research informs researchers on the attention of multimodal social media data. The study’s findings provide some managerial recommendations, e.g. watching not only words but also pictures in social media.","Hui Yuan,Hui Yuan,Yuanyuan Tang,Wei Xu,Raymond Y. K. Lau",Internet Research,2021,,,
1120,Can news help measure economic sentiment? An application in COVID-19 times,"Abstract   We construct a new newspaper-based sentiment indicator for Spain that allows to monitor economic activity in real-time. As opposed to survey-based confidence indicators that are released at the end of the month, our indicator can be constructed on a daily basis. We compare our index with the popular Economic Sentiment Indicator of the European Commission and show that ours performs significantly better in nowcasting the Spanish GDP. Moreover, it proves to be helpful to predict the current COVID-19 recession from an earlier date.","Pablo Aguilar,Pablo Aguilar,Pablo S. Aguilar,Corinna Ghirelli,Matías Pacce,Matías Pacce,Alberto Urtasun",Economics Letters,2021,,,
1121,General factorization framework for context-aware recommendations,"Context-aware recommendation algorithms focus on refining recommendations by considering additional information, available to the system. This topic has gained a lot of attention recently. Among others, several factorization methods were proposed to solve the problem, although most of them assume explicit feedback which strongly limits their real-world applicability. While these algorithms apply various loss functions and optimization strategies, the preference modeling under context is less explored due to the lack of tools allowing for easy experimentation with various models. As context dimensions are introduced beyond users and items, the space of possible preference models and the importance of proper modeling largely increases. 
In this paper we propose a General Factorization Framework (GFF), a single flexible algorithm that takes the preference model as an input and computes latent feature matrices for the input dimensions. GFF allows us to easily experiment with various linear models on any context-aware recommendation task, be it explicit or implicit feedback based. The scaling properties makes it usable under real life circumstances as well. 
We demonstrate the framework's potential by exploring various preference models on a 4-dimensional context-aware problem with contexts that are available for almost any real life datasets. We show in our experiments -- performed on five real life, implicit feedback datasets -- that proper preference modelling significantly increases recommendation accuracy, and previously unused models outperform the traditional ones. Novel models in GFF also outperform state-of-the-art factorization algorithms. 
We also extend the method to be fully compliant to the Multidimensional Dataspace Model, one of the most extensive data models of context-enriched data. Extended GFF allows the seamless incorporation of information into the fac[truncated]","Balázs Hidasi,Domonkos Tikk",arXiv: Information Retrieval,2014,,,
1122,Forecasting the Equity Risk Premium: The Role of Technical Indicators,"Academic research relies extensively on macroeconomic variables to forecast the U.S. equity risk premium, with relatively little attention paid to the technical indicators widely employed by practitioners. Our paper fills this gap by comparing the predictive ability of technical indicators with that of macroeconomic variables. Technical indicators display statistically and economically significant in-sample and out-of-sample predictive power, matching or exceeding that of macroeconomic variables. Furthermore, technical indicators and macroeconomic variables provide complementary information over the business cycle: technical indicators better detect the typical decline in the equity risk premium near business-cycle peaks, whereas macroeconomic variables more readily pick up the typical rise in the equity risk premium near cyclical troughs. Consistent with this behavior, we show that combining information from both technical indicators and macroeconomic variables significantly improves equity risk premium forecasts versus using either type of information alone. Overall, the substantial countercyclical fluctuations in the equity risk premium appear well captured by the combined information in technical indicators and macroeconomic variables.

Data, as supplemental material, are available at  http://dx.doi.org/10.1287/mnsc.2013.1838 .

This paper was accepted by Wei Jiang, finance.","Christopher J. Neely,David E. Rapach,Jun Tu,Guofu Zhou,Guofu Zhou",Management Science,2014,,,
1123,Risk Reduction in Large Portfolios: Why Imposing the Wrong Constraints Helps,"Green and Hollifield (1992) argue that the presence of a dominant factor would result in extreme negative weights in mean-variance efficient portfolios even in the absence of estimation errors. In that case, imposing no-short-sale constraints should hurt, whereas empirical evidence is often to the contrary. We reconcile this apparent contradiction. We explain why constraining portfolio weights to be nonnegative can reduce the risk in estimated optimal portfolios even when the constraints are wrong. Surprisingly, with no-short-sale constraints in place, the sample covariance matrix performs as well as covariance matrix estimates based on factor models, shrinkage estimators, and daily data. MARKOWITZ'S (1952, 1959) PORTFOLIO THEORY is one of the most important theoretical developments in finance. Mean-variance efficient portfolios play an important role in this theory. Such portfolios constructed using sample moments often involve large negative weights in a number of assets. Since negative portfolio weights (short positions) are difficult to implement in practice, most investors impose the constraint that portfolio weights should be nonnegative when constructing mean-variance efficient portfolios. Green and Hollifield (1992) argue that because a single factor dominates the covariance structure, it would be difficult to dismiss the observed extreme negative and positive weights as being entirely due to the imprecise estimation of the inputs. They consider the global minimum variance portfolio to avoid the effect of estimation error in the mean on portfolio weights. They note that when returns are generated by a single factor model, minimum variance portfolios can be constructed in two steps. First, naively diversify over the set of high beta stocks and","Ravi Jagannathan,Tongshu Ma",Journal of Finance,2003,,,
1124,Profitable momentum trading strategies for individual investors,"For nearly three decades, scientific studies have explored momentum investing strategies and observed stable excess returns in various financial markets. However, the trading strategies typically analyzed in such research are not accessible to individual investors due to short selling constraints, nor are they profitable due to high trading costs. Incorporating these constraints, we explore a simplified momentum trading strategy that only exploits excess returns from topside momentum for a small number of individual stocks. Building on US data from the New York Stock Exchange from July 1991 to December 2010, we analyze whether such a simplified momentum strategy outperforms the benchmark after factoring in realistic transaction costs and risks. We find that the strategy can indeed work for individual investors with initial investment amounts of at least $5,000. In further attempts to improve this practical trading strategy, we analyze an overlapping momentum trading strategy consisting of a more frequent trading of a smaller number of “winner” stocks. We find that increasing the trading frequency initially increases the risk-adjusted returns of these portfolios up to an optimal point, after which excessive transaction costs begin to dominate the scene. In a calibration study, we find that, depending on the initial investment amount of the portfolio, the optimal momentum trading frequency ranges from bi-yearly to monthly.","Bryan Foltice,Thomas Langer",Financial Markets and Portfolio Management,2015,,,
1125,On estimating the expected return on the market: An exploratory investigation,"The expected market return is a number frequently required for the solution of many investment and corporate finance problems, but by comparison with other financial variables, there has been little research on estimating this expected return. Current practice for estimating the expected market return adds the historical average realized excess market returns to the current observed interest rate. While this model explicitly reflects the dependence of the market return on the interest rate, it fails to account for the effect of changes in the level of market risk. Three models of equilibrium expected market returns which reflect this dependence are analyzed in this paper. Estimation procedures which incorporate the prior restriction that equilibrium expected excess returns on the market must be positive are derived and applied to return data for the period 1926–1978. The principal conclusions from this exploratory investigation are: (1) in estimating models of the expected market return, the non-negativity restriction of the expected excess return should be explicity included as part of the specification: (2) estimators which use realized returns should be adjusted for heteroscedasticity.",Robert C. Merton,Journal of Financial Economics,1980,,,
1126,Financial Trading as a Game: A Deep Reinforcement Learning Approach,"An automatic program that generates constant profit from the financial market is lucrative for every market practitioner. Recent advance in deep reinforcement learning provides a framework toward end-to-end training of such trading agent. In this paper, we propose an Markov Decision Process (MDP) model suitable for the financial trading task and solve it with the state-of-the-art deep recurrent Q-network (DRQN) algorithm. We propose several modifications to the existing learning algorithm to make it more suitable under the financial trading setting, namely 1. We employ a substantially small replay memory (only a few hundreds in size) compared to ones used in modern deep reinforcement learning algorithms (often millions in size.) 2. We develop an action augmentation technique to mitigate the need for random exploration by providing extra feedback signals for all actions to the agent. This enables us to use greedy policy over the course of learning and shows strong empirical performance compared to more commonly used epsilon-greedy exploration. However, this technique is specific to financial trading under a few market assumptions. 3. We sample a longer sequence for recurrent neural network training. A side product of this mechanism is that we can now train the agent for every T steps. This greatly reduces training time since the overall computation is down by a factor of T. We combine all of the above into a complete online learning algorithm and validate our approach on the spot foreign exchange market.",Chien Yi Huang,,2018,,,
1127,Empirical Asset Pricing via Machine Learning,"We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premia. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best performing methods (trees and neural networks) and trace their predictive gains to allowance of nonlinear predictor interactions that are missed by other methods. All methods agree on the same set of dominant predictive signals which includes variations on momentum, liquidity, and volatility. Improved risk premium measurement through machine learning simplifies the investigation into economic mechanisms of asset pricing and highlights the value of machine learning in financial innovation.","Shihao Gu,Bryan T. Kelly,Dacheng Xiu",Review of Financial Studies,2020,,,
1128,Overreaction and Insider Trading: Evidence from Growth and Value Portfolios,"Insider transactions are not random across growth and value stocks. We find that insider buying climbs as stocks change from growth to value categories. Insider buying also is greater after low stock returns, and lower after high stock returns. These findings are consistent with a version of overreaction which says that prices of value stocks tend to lie below fundamental values, and prices of growth stocks tend to lie above fundamental values. Do MARKET PRICES REFLECT investor overreaction?1 The purpose of this paper is to provide evidence on this question. However, new evidence is unlikely to change many minds if the findings depend on the newly controversial capital asset pricing model (CAPM) or multifactor models, or if they involve datasnooping.2 We bypass these problems by not using CAPM or a benchmark model, and by examining a new set of data. We measure insider buying and selling in stocks that are ranked by measures such as the ratio of cash flow per share to price per share (CF/P).3 Current practice refers to stocks with low CF/P as growth stocks, and to stocks with high CF/P as value stocks. We examine the direction of insider trades along the growth/value spectrum to see if they are consistent with attempts to profit from market overreaction. By overreaction, we mean price movements that predictably reverse. This experiment does not require an asset-pricing model because we do not examine the returns accruing to insider trades.4","Michael S. Rozeff,Mir A. Zaman",Journal of Finance,1998,,,
1129,"Buy Low, Sell High: A High Frequency Trading Perspective",We develop a high frequency (HF) trading strategy where the HF trader uses her superior speed to process information and to post limit sell and buy orders. By introducing a multifactor mutually exc...,"Álvaro Cartea,Sebastian Jaimungal,Jason Ricci",Siam Journal on Financial Mathematics,2014,,,
1130,The Price of Bodies: A Hedonic Pricing Model of Avatar Attributes in a Synthetic World,"Summary

This paper explores a unique new source of social valuation: a market for bodies. The internet hosts a number of large synthetic worlds which users can visit by piloting a computer-generated body, known as an avatar. Avatars can have an asset value, in that users can spend time to increase their skills; these asset values can be directly observed in online markets. Auction data for avatars from the synthetic fantasy world of EverQuest are used here to explore a number of questions involving the relative value of different body characteristics. Hedonic analysis of the auction price data suggests that the ‘level’, a game-design metric that indicates the overall functionality or power of the avatar, is by far the most important attribute of the body. Other attributes that show significant price effects include: sex and class (i.e. being a wizard rather than warrior type of character). The male-female price difference is interesting because there are actually no sex-based differences in the abilities of the avatar bodies, by design. Price differences here must be caused by some other aspect of buyer preferences, ones unrelated to power or functionality of the avatar itself.

Zusammenfassung

Dieser Artikel untersucht eine einmalige neue Quelle fur soziale Wertschatzung: einen Markt fur Korper. Im Internet befinden sich zahlreiche grosse synthetische Welten, die die Anwender mit einer computergenerierten Figur (genannt Avatar) besuchen. Avatare haben einen Sachwert, da die Anwender Zeit damit verbringen, ihre Fahigkeiten zu verbessern. Diese Werte konnen auf Online-Markten direkt beobachtet werden. Wir benutzen hier Auktionsdaten der Fantasiewelt ‘EverQuest’, um den relativen Wert von verschiedenen korperlichen Attributen zu untersuchen. Eine Analyse der Auktionspreise lasst vermuten, dass das ‘Level’, ein Mass im Spieldesign, das die gesamte Funktionalitat oder Macht des Avatars beschreibt, die bei weitem wichtigste Charakteristik ist. Weitere Attribute mit signifikanten Auswirkungen auf den Preis sind Geschlecht und Klasse (z. B. Zauberer oder Krieger). Der Preisunterschied zwischen mannlichen und weiblichen Charakteren ist interessant, da praktisch keine geschlechtsabhangigen Unterschiede der Fahigkeiten vorgegeben sind. Preisunterschiede mussen hier auf anderen Aspekten der Kauferpraferenzen beruhen, die nicht mit der Macht und der Funktionalitat der Avatare in Zusammenhang stehen.

Resume

Cet article explore une nouvelle et unique source d'evaluation sociale: un marche pour des corps. Sur internet, on trouve un grand nombre de mondes synthetiques que les utilisateurs visitent en se servant d'un corps virtuel, le soi-disant avatar. Les avatars ont une valeur reelle puisque les utilisateurs passent du temps a ameliorer leur habilete; ces valeurs peuvent etre observees directement sur des marches en ligne. Les donnees des encheres du monde virtuel d'EverQuest sont utilisees ici pour evaluer un nombre de questions relatives aux valeurs de differentes caracteristiques des avatars. L'analyse hedonique des prix aux encheres suggere que le ‘niveau’, une mesure inherente au jeu qui indique la fonctionnalite ou le pouvoir d'un avatar, est de loin la caracteristique la plus importante. D'autres attributs qui exercent une influence significative sur le prix sont le sexe et la classe (p.ex. etre un magicien plutot qu'un guerrier). La difference de prix entre les sujets masculins et feminins est interessante, puisqu'il n'existe pas de difference dependant du sexe quant a l'habilete des caracteres. La difference de prix doit donc deriver d'une autre preference des acheteurs qui n'est pas en relation avec le pouvoir ou la fonctionnalite des avatars en tant que tels.",Edward Castronova,Kyklos,2004,,,
1131,Mean reversion in stock prices: Evidence and Implications,"Abstract   This paper investigates transitory components in stock prices. After showing that statistical tests have little power to detect persistent deviations between market prices and fundamental values, we consider whether prices are mean-reverting, using data from the United States and 17 other countries. Our point estimates imply positive autocorrelation in returns over short horizons and negative autocorrelation over longer horizons, although random-walk price behavior cannot be rejected at conventional statistical levels. Substantial movements in required returns are needed to account for these correlation patterns. Persistent, but transitory, disparities between prices and fundamental values could also explain our findings.","James M. Poterba,Lawrence H. Summers",Journal of Financial Economics,1988,,,
1132,A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem,"Financial portfolio management is the process of constant redistribution of a fund into different financial products. This paper presents a financial-model-free Reinforcement Learning framework to provide a deep machine learning solution to the portfolio management problem. The framework consists of the Ensemble of Identical Independent Evaluators (EIIE) topology, a Portfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL) scheme, and a fully exploiting and explicit reward function. This framework is realized in three instants in this work with a Convolutional Neural Network (CNN), a basic Recurrent Neural Network (RNN), and a Long Short-Term Memory (LSTM). They are, along with a number of recently reviewed or published portfolio-selection strategies, examined in three back-test experiments with a trading period of 30 minutes in a cryptocurrency market. Cryptocurrencies are electronic and decentralized alternatives to government-issued money, with Bitcoin as the best-known example of a cryptocurrency. All three instances of the framework monopolize the top three positions in all experiments, outdistancing other compared trading algorithms. Although with a high commission rate of 0.25% in the backtests, the framework is able to achieve at least 4-fold returns in 50 days.","Zhengyao Jiang,Dixing Xu,Jinjun Liang",,2017,,,
1133,Disentangled Recurrent Wasserstein Autoencoder,"Learning disentangled representations leads to interpretable models and facilitates data generation with style transfer, which has been extensively studied on static data such as images in an unsupervised learning framework. However, only a few works have explored unsupervised disentangled sequential representation learning due to challenges of generating sequential data. In this paper, we propose recurrent Wasserstein Autoencoder (R-WAE), a new framework for generative modeling of sequential data. R-WAE disentangles the representation of an input sequence into static and dynamic factors (i.e., time-invariant and time-varying parts). Our theoretical analysis shows that, R-WAE minimizes an upper bound of a penalized form of the Wasserstein distance between model distribution and sequential data distribution, and simultaneously maximizes the mutual information between input data and different disentangled latent factors, respectively. This is superior to (recurrent) VAE which does not explicitly enforce mutual information maximization between input data and disentangled latent representations. When the number of actions in sequential data is available as weak supervision information, R-WAE is extended to learn a categorical latent representation of actions to improve its disentanglement. Experiments on a variety of datasets show that our models outperform other baselines with the same settings in terms of disentanglement and unconditional video generation both quantitatively and qualitatively.","Jun Han,Jun Han,Jun Han,Martin Renqiang Min,Ligong Han,Li Erran Li,Xuan Zhang",arXiv: Learning,2021,,,
1134,Double Deep Q-Learning for Optimal Execution,"Optimal trade execution is an important problem faced by essentially all traders. Much research into optimal execution uses stringent model assumptions and applies continuous time stochastic control to solve them. Here, we instead take a model free approach and develop a variation of Deep Q-Learning to estimate the optimal actions of a trader. The model is a fully connected Neural Network trained using Experience Replay and Double DQN with input features given by the current state of the limit order book, other trading signals, and available execution actions, while the output is the Q-value function estimating the future rewards under an arbitrary action. We apply our model to nine different stocks and find that it outperforms the standard benchmark approach on most stocks using the measures of (i) mean and median out-performance, (ii) probability of out-performance, and (iii) gain-loss ratios.","Brian Ning,Franco Ho Ting Lin,Sebastian Jaimungal",,2018,,,
1135,An empirical study into finding optima in stochastic optimization of neural networks,"Abstract   Mini-batch sub-sampling (MBSS) in neural network training is unavoidable due to growing data demands, memory-limited computational resources such as graphical processing units, and the dynamics of on-line learning. This study distinguishes between static MBSS and dynamic MBSS. In static MBSS, mini-batches are intermittently fixed during training, resulting in smooth but biased loss functions. During dynamic MBSS, mini-batches are resampled at every loss evaluation, resulting in sampling induced discontinuities by trading sampling bias for sampling variance. This renders classical minimization strategies ineffective in dynamic MBSS losses, as these may locate spurious sampling induced minima, while critical points may not exist. This paper re-evaluates the information used to define optima in stochastic loss functions of neural networks by defining the solution to a stochastic optimization problem as the stochastic non-negative associated gradient projection point (SNN-GPP). We demonstrate that SNN-GPPs offer a more robust description of full-batch optima than minimizers and critical points. An empirical investigation compares local minima to SNN-GPPs for the potential training of a simple neural network training problem with different activation functions. Since SNN-GPPs better approximate the location of true optima, we conclude that line searches locating SNN-GPPs can contribute significantly to automating neural network training.","Dominic Kafka,Daniel N. Wilke",Information Sciences,2021,,,
1136,An Application of Deep Reinforcement Learning to Algorithmic Trading,"This scientific research paper presents an innovative approach based on deep reinforcement learning (DRL) to solve the algorithmic trading problem of determining the optimal trading position at any point in time during a trading activity in stock markets. It proposes a novel DRL trading strategy so as to maximise the resulting Sharpe ratio performance indicator on a broad range of stock markets. Denominated the Trading Deep Q-Network algorithm (TDQN), this new trading strategy is inspired from the popular DQN algorithm and significantly adapted to the specific algorithmic trading problem at hand. The training of the resulting reinforcement learning (RL) agent is entirely based on the generation of artificial trajectories from a limited set of stock market historical data. In order to objectively assess the performance of trading strategies, the research paper also proposes a novel, more rigorous performance assessment methodology. Following this new performance assessment approach, promising results are reported for the TDQN strategy.","Thibaut Th 'eate,Damien Ernst,Damien Ernst,Damien Ernst",,2020,,,
1137,An improved moving average technical trading rule,"Abstract   This paper proposes a modified version of the widely used price and moving average cross-over trading strategies. The suggested approach (presented in its ‘long only’ version) is a combination of cross-over ‘buy’ signals and a dynamic threshold value which acts as a dynamic trailing stop. The trading behaviour and performance from this modified strategy are different from the standard approach with results showing that, on average, the proposed modification increases the cumulative return and the Sharpe ratio of the investor while exhibiting smaller maximum drawdown and smaller drawdown duration than the standard strategy.","Fotis Papailias,Dimitrios D. Thomakos",Physica A-statistical Mechanics and Its Applications,2015,,,
1138,Deep Learning Based Recommender System: A Survey and New Perspectives,"With the growing volume of online information, recommender systems have been an effective strategy to overcome information overload. The utility of recommender systems cannot be overstated, given their widespread adoption in many web applications, along with their potential impact to ameliorate many problems related to over-choice. In recent years, deep learning has garnered considerable interest in many research fields such as computer vision and natural language processing, owing not only to stellar performance but also to the attractive property of learning feature representations from scratch. The influence of deep learning is also pervasive, recently demonstrating its effectiveness when applied to information retrieval and recommender systems research. The field of deep learning in recommender system is flourishing. This article aims to provide a comprehensive review of recent research efforts on deep learning-based recommender systems. More concretely, we provide and devise a taxonomy of deep learning-based recommendation models, along with a comprehensive summary of the state of the art. Finally, we expand on current trends and provide new perspectives pertaining to this new and exciting development of the field.","Shuai Zhang,Shuai Zhang,Lina Yao,Aixin Sun,Yi Tay",ACM Computing Surveys,2019,,,
1139,On the impossibility of informationally efficient markets,,"Sanford J. Grossman,Joseph E. Stiglitz",The American Economic Review,1980,,,
1140,Modeling Stock Price Dynamics With Fuzzy Opinion Networks,"We propose a mathematical model for the word-of-mouth communications among stock investors through social networks and explore how the changes of the investors’ social networks influence the stock price dynamics and vice versa. An investor is modeled as a Gaussian fuzzy set (a fuzzy opinion) with the center and standard deviation as inputs and the fuzzy set itself as output. Investors are connected in the following fashion: the center input of an investor is taken as the average of the neighbors’ outputs, where two investors are neighbors if their fuzzy opinions are close enough to each other, and the standard deviation (uncertainty) input is taken with local, global, or external reference schemes to model different scenarios of how investors define uncertainties. The centers and standard deviations of the fuzzy opinions are the expected prices and their uncertainties, respectively, that are used as inputs to the price dynamic equation. We prove that with the local reference scheme the investors converge to different groups in finite time, while with the global or external reference schemes all investors converge to a consensus within finite time and the consensus may change with time in the external reference case. We show how to model trend followers, contrarians, and manipulators within this mathematical framework and prove that the biggest enemy of a manipulator is the other manipulators. We perform Monte Carlo simulations to show how the model parameters influence the price dynamics, and we apply a modified version of the model to the daily closing prices of 15 top banking and real estate stocks in Hong Kong for the recent two years from December 5, 2013 to December 4, 2015 and discover that a sharp increase of the combined uncertainty is a reliable signal to predict the reversal of the current price trend.","Li-Xin Wang,Li-Xin Wang,Li-Xin Wang",IEEE Transactions on Fuzzy Systems,2017,,,
1141,Autoencoder asset pricing models,"Abstract   We propose a new latent factor conditional asset pricing model. Like Kelly, Pruitt, and Su (KPS, 2019), our model allows for latent factors and factor exposures that depend on covariates such as asset characteristics. But, unlike the linearity assumption of KPS, we model factor exposures as a flexible nonlinear function of covariates. Our model retrofits the workhorse unsupervised dimension reduction device from the machine learning literature – autoencoder neural networks – to incorporate information from covariates along with returns themselves. This delivers estimates of nonlinear conditional exposures and the associated latent factors. Furthermore, our machine learning framework imposes the economic restriction of no-arbitrage. Our autoencoder asset pricing model delivers out-of-sample pricing errors that are far smaller (and generally insignificant) compared to other leading factor models.","Shihao Gu,Bryan T. Kelly,Dacheng Xiu",Journal of Econometrics,2020,,,
1142,Deep Reinforcement Learning for Asset Allocation in US Equities,"Reinforcement learning is a machine learning approach concerned with solving dynamic optimization problems in an almost model-free way by maximizing a reward function in state and action spaces. This property makes it an exciting area of research for financial problems. Asset allocation, where the goal is to obtain the weights of the assets that maximize the rewards in a given state of the market considering risk and transaction costs, is a problem easily framed using a reinforcement learning framework. So it is first a prediction problem for the vector of expected returns and covariance matrix and then an optimization problem for returns, risk, and market impact, usually a quadratic programming one. Investors and financial researchers have been working with approaches like mean-variance optimization, minimum variance, risk parity, and equally weighted and several methods to make expected returns and covariance matrices' predictions more robust and after use mean-variance like the Black Litterman model. 

This paper demonstrates the application of reinforcement learning to create a financial model-free solution to the asset allocation problem, learning to solve the problem using time series and deep neural networks. We demonstrate this on daily data for the top 24 stocks in the US equities universe with daily rebalancing. We use a deep reinforcement model on US stocks using different deep learning architectures. We use Long Short Term Memory networks, Convolutional Neural Networks, and Recurrent Neural Networks and compare them with more traditional portfolio management approaches like mean-variance, minimum variance, risk parity, and equally weighted. The Deep Reinforcement Learning approach shows better results than traditional approaches using a simple reward function and only being given the time series of stocks. In Finance, no training to test error generalization results come guaranteed. We can say that the modeling framework can deal with time series prediction and asset allocation, including transaction costs.","Miquel Noguer i Alonso,Sonam Srivastava,Sonam Srivastava",,2020,,,
1143,BDLOB: Bayesian Deep Convolutional Neural Networks for Limit Order Books,"We showcase how dropout variational inference can be applied to a large-scale deep learning model that predicts price movements from limit order books (LOBs), the canonical data source representing trading and pricing movements. We demonstrate that uncertainty information derived from posterior predictive distributions can be utilised for position sizing, avoiding unnecessary trades and improving profits. Further, we test our models by using millions of observations across several instruments and markets from the London Stock Exchange. Our results suggest that those Bayesian techniques not only deliver uncertainty information that can be used for trading but also improve predictive performance as stochastic regularisers. To the best of our knowledge, we are the first to apply Bayesian networks to LOBs.","Zihao Zhang,Zihao Zhang,Zihao Zhang,Stefan Zohren,Stephen J. Roberts",,2018,,,
1144,Deep Learning Modeling of the Limit Order Book: A Comparative Perspective,"The present work addresses theoretical and practical questions in the domain of Deep Learning for High Frequency Trading. State-of-the-art models such as Random models, Logistic Regressions, LSTMs, LSTMs equipped with an Attention mask, CNN-LSTMs and MLPs are reviewed and compared on the same tasks, feature space, and dataset and clustered according to pairwise similarity and performance metrics. The underlying dimensions of the modeling techniques are hence investigated to understand whether these are intrinsic to the Limit Order Book’s dynamics. We observe that the Multilayer Perceptron performs comparably to or better than state-of-the-art CNN-LSTM architectures indicating that dynamic spatial and temporal dimensions are a good approximation of the LOB’s dynamics, but not necessarily the true underlying dimensions.","Antonio Briola,Jeremy D. Turiel,Tomaso Aste",,2020,,,
1145,On the determinants of bitcoin returns: a LASSO approach,"We examine the significance of twenty-one potential drivers of bitcoin returns for the period 2010 to 2017 (2,533 daily observations). Within a LASSO framework, we examine the effects of factors such as stock market returns, exchange rates, gold and oil returns, FED’s and ECB’s rates and internet trends on bitcoin returns for alternate time periods. Search intensity and gold returns emerge as the most important variables for bitcoin returns.","Theodore Panagiotidis,Thanasis Stengos,Orestis Vravosinos",,2018,,,
1146,Temporal Relational Ranking for Stock Prediction,"Stock prediction aims to predict the future trends of a stock in order to help investors make good investment decisions. Traditional solutions for stock prediction are based on time-series models. With the recent success of deep neural networks in modeling sequential data, deep learning has become a promising choice for stock prediction.

However, most existing deep learning solutions are not optimized toward the target of investment, i.e., selecting the best stock with the highest expected revenue. Specifically, they typically formulate stock prediction as a classification (to predict stock trends) or a regression problem (to predict stock prices). More importantly, they largely treat the stocks as independent of each other. The valuable signal in the rich relations between stocks (or companies), such as two stocks are in the same sector and two companies have a supplier-customer relation, is not considered.

In this work, we contribute a new deep learning solution, named Relational Stock Ranking (RSR), for stock prediction. Our RSR method advances existing solutions in two major aspects: (1) tailoring the deep learning models for stock ranking, and (2) capturing the stock relations in a time-sensitive manner. The key novelty of our work is the proposal of a new component in neural network modeling, named Temporal Graph Convolution, which jointly models the temporal evolution and relation network of stocks. To validate our method, we perform back-testing on the historical data of two stock markets, NYSE and NASDAQ. Extensive experiments demonstrate the superiority of our RSR method. It outperforms state-of-the-art stock prediction solutions achieving an average return ratio of 98% and 71% on NYSE and NASDAQ, respectively.","Fuli Feng,Xiangnan He,Xiang Wang,Xiang Wang,Cheng Luo,Yiqun Liu,Yiqun Liu,Tat-Seng Chua",ACM Transactions on Information Systems,2019,,,
1147,Q-Learning-based financial trading systems with applications,The design of financial trading systems (FTSs) is a subject of high interest both for the academic environment and for the professional one due to the promises by machine learning methodologies. In this paper we consider the Reinforcement Learning-based policy evaluation approach known as Q-Learning algorithm (QLa). QLa is an algorithm which real-time optimizes its behavior in relation to the responses it gets from the environment in which it operates. In particular: first we introduce the essential aspects of QLa which are of interest for our purposes, second we present some original FTSs based on differently configured QLas, then we apply such FTSs to an artificial time series of daily stock prices and to six real ones from the Italian stock market belonging to the FTSE MIB basket. The results we achieve are generally satisfactory.,"Marco Corazza,Francesco Bertoluzzo",,2014,
1148,Deep Hedging of Derivatives Using Reinforcement Learning,"This paper shows how reinforcement learning can be used to derive optimal hedging strategies for derivatives when there are transaction costs. The paper illustrates the approach by showing the diﬀerence between using delta hedging and optimal hedging for a short position in a call option when the objective is to minimize a function equal to the mean hedging cost plus a constant times the standard deviation of the hedging cost. Two situations are considered. In the ﬁrst, the asset price follows geometric Brownian motion. In the second, the asset price follows a stochastic volatility process. The paper extends the basic reinforcement learning approach in a number of ways. First, it uses two diﬀerent Q-functions so that both the expected value of the cost and the expected value of the square of the cost are tracked for diﬀerent state/action combinations. This approach increases the range of objective functions that can be used. Second, it uses a learning algorithm that allows for continuous state and action space. Third, it compares the accounting P&L approach (where the hedged position is valued at each step) and the cash ﬂow approach (where cash inﬂows and outﬂows are used). We ﬁnd that a hybrid approach involving the use of an accounting P&L approach that incorporates a relatively simple valuation model works well. The valuation model does not have to correspond to the process assumed for the underlying asset price.","Jay Cao,Jacky Chen,John C. Hull,Zissis Poulos",,2019,,,
1149,The Economics of Bitcoins - Market Characteristics and Price Jumps,"This paper deals with the economics of Bitcoins in two ways. First, it broadens the discussion on how to capture Bitcoins using economic terms. Center stage in this analysis take the discussion of some unique characteristics of this market as well as the comparison of Bitcoins and gold. Second, the paper empirically analyses Bitcoin prices using an autoregressive jump-intensity GARCH model; a model tested and proven by the empirical finance community. Results suggest that Bitcoin price are particularly marked by extreme price movements; a behaviour generally observed in immature markets.",Marc Gronwald,,2014,,,
1150,Strategic asset allocation and market timing: a reinforcement learning approach,"We apply the recurrent reinforcement learning method of Moody, Wu, Liao, and Saffell (1998) in the context of the strategic asset allocation computed for sample data from US, UK, Germany, and Japan. It is found that the optimal asset allocation deviates substantially from the fixed-mix rule. The investor actively times the market and he is able to outperform it consistently over the almost two decades we analyze.","Thorsten Hens,Peter Wöhrmann,Peter Wöhrmann",Computing in Economics and Finance,2007,,,
1151,Principal Components as a Measure of Systemic Risk,"The U. S. government’s failure to provide adequate oversight and prudent regulation of the financial markets, together with excessive risk taking by some financial institutions, pushed the world financial system to the brink of systemic failure in 2008. As a consequence of this near catastrophe, both regulators and investors have become keenly interested in developing tools for monitoring systemic risk. But this is easier said than done. Securitization, private transacting, complexity, and “flexible accounting” prevent us from directly observing the many explicit linkages of financial institutions. As an alternative, the authors introduce a measure of implied systemic risk, the absorption ratio, which equals the fraction of the total variance of a set of asset returns explained or “absorbed” by a fixed number of eigenvectors. The absorption ratio captures the extent to which markets are unified or tightly coupled. When markets are tightly coupled, they are more fragile in the sense that negative shocks propagate more quickly and broadly than when markets are loosely linked.","Mark Kritzman,Yuanzhen Li,Sébastien Page,Sébastien Page,Roberto Rigobon",The Journal of Portfolio Management,2011,,,
1152,Textual sentiment in finance: a survey of methods and models,"Abstract   We survey the textual sentiment literature, comparing and contrasting the various information sources, content analysis methods, and empirical models that have been used to date. We summarize the important and influential findings about how textual sentiment impacts on individual, firm-level and market-level behavior and performance, and vice versa. We point to what is agreed and what remains controversial. Promising directions for future research are emerging from the availability of more accurate and efficient sentiment measures resulting from increasingly sophisticated textual content analysis coupled with more extensive field-specific dictionaries. This is enabling more wide-ranging studies that use increasingly sophisticated models to help us better understand behavioral finance patterns across individuals, institutions and markets.","Colm Kearney,Sha Liu",International Review of Financial Analysis,2014,,,
1153,Reinforcement Learning for Portfolio Management,"In this thesis, we develop a comprehensive account of the expressive power, modelling efficiency, and performance advantages of so-called trading agents (i.e., Deep Soft Recurrent Q-Network (DSRQN) and Mixture of Score Machines (MSM)), based on both traditional system identification (model-based approach) as well as on context-independent agents (model-free approach). The analysis provides conclusive support for the ability of model-free reinforcement learning methods to act as universal trading agents, which are not only capable of reducing the computational and memory complexity (owing to their linear scaling with the size of the universe), but also serve as generalizing strategies across assets and markets, regardless of the trading universe on which they have been trained. The relatively low volume of daily returns in financial market data is addressed via data augmentation (a generative approach) and a choice of pre-training strategies, both of which are validated against current state-of-the-art models. For rigour, a risk-sensitive framework which includes transaction costs is considered, and its performance advantages are demonstrated in a variety of scenarios, from synthetic time-series (sinusoidal, sawtooth and chirp waves), simulated market series (surrogate data based), through to real market data (S\&P 500 and EURO STOXX 50). The analysis and simulations confirm the superiority of universal model-free reinforcement learning agents over current portfolio management model in asset allocation strategies, with the achieved performance advantage of as much as 9.2\% in annualized cumulative returns and 13.4\% in annualized Sharpe Ratio.",Angelos Filos,,2019,,,
1154,Proof that Properly Anticipated Prices Fluctuate Randomly,"By positing a rather general stochastic model of price change, I shall deduce a fairly sweeping theorem in which next-period's price differences are shown to be uncorrelated with (if not completely independent of) previous period's price differences. This martingale property of zero expected capital gain will then be replaced by the slightly more general case of a constant mean percentage gain per unit time.",Paul A. Samuelson,,2015,,,
1155,Multi-Agent Reinforcement Learning in a Realistic Limit Order Book Market Simulation,"Optimal order execution is widely studied by industry practitioners and academic researchers because it determines the profitability of investment decisions and high-level trading strategies, particularly those involving large volumes of orders. However, complex and unknown market dynamics pose enormous challenges for the development and validation of optimal execution strategies. We propose a model-free approach by training Reinforcement Learning (RL) agents in a realistic market simulation environment with multiple agents. First, we have configured a multi-agent historical order book simulation environment for execution tasks based on an Agent-Based Interactive Discrete Event Simulation (ABIDES) [arXiv:1904.12066]. Second, we formulated the problem of optimal execution in an RL setting in which an intelligent agent can make order execution and placement decisions based on market microstructure trading signals in HFT. Third, we developed and trained an RL execution agent using the Double Deep Q-Learning (DDQL) algorithm in the ABIDES environment. In some scenarios, our RL agent converges towards a Time-Weighted Average Price (TWAP) strategy. Finally, we evaluated the simulation with our RL agent by comparing the simulation on the actual market Limit Order Book (LOB) characteristics.","Michaël Karpe,Jin Fang,Zhongyao Ma,Chen Wang",,2020,,,
1156,Universal Features of Price Formation in Financial Markets: Perspectives From Deep Learning,"Using a large-scale Deep Learning approach applied to a high-frequency database containing billions of electronic market quotes and transactions for US equities, we uncover nonparametric evidence for the existence of a universal and stationary price formation mechanism relating the dynamics of supply and demand for a stock, as revealed through the order book, to subsequent variations in its market price. We assess the model by testing its out-of-sample predictions for the direction of price moves given the history of price and order flow, across a wide range of stocks and time periods. The universal price formation model is shown to exhibit a remarkably stable out-of-sample prediction accuracy across time, for a wide range of stocks from different sectors. Interestingly, these results also hold for stocks which are not part of the training sample, showing that the relations captured by the model are universal and not asset-specific. 
The universal model --- trained on data from all stocks --- outperforms, in terms of out-of-sample prediction accuracy, asset-specific linear and nonlinear models trained on time series of any given stock, showing that the universal nature of price formation weighs in favour of pooling together financial data from various stocks, rather than designing asset or sector-specific models as commonly done. Standard data normalizations based on volatility, price level or average spread, or partitioning the training data into sectors or categories such as large/small tick stocks, do not improve training results. On the other hand, inclusion of price and order flow history over many past observations is shown to improve forecasting performance, showing evidence of path-dependence in price dynamics.","Justin A. Sirignano,Rama Cont",,2018,,,
1157,Deep Learning modeling of Limit Order Book: a comparative perspective,"The present work addresses theoretical and practical questions in the domain of Deep Learning for High Frequency Trading, with a thorough review and analysis of the literature and state-of-the-art models. Random models, Logistic Regressions, LSTMs, LSTMs equipped with an Attention mask, CNN-LSTMs and MLPs are compared on the same tasks, feature space, and dataset and clustered according to pairwise similarity and performance metrics. The underlying dimensions of the modeling techniques are hence investigated to understand whether these are intrinsic to the Limit Order Book's dynamics. It is possible to observe that the Multilayer Perceptron performs comparably to or better than state-of-the-art CNN-LSTM architectures indicating that dynamic spatial and temporal dimensions are a good approximation of the LOB's dynamics, but not necessarily the true underlying dimensions.","Antonio Briola,Jeremy D. Turiel,Tomaso Aste",,2020,,,
1158,Deep Reinforcement Learning in Cryptocurrency Market Making,"This paper sets forth a framework for deep reinforcement learning as applied to market making (DRLMM) for cryptocurrencies. Two advanced policy gradient-based algorithms were selected as agents to interact with an environment that represents the observation space through limit order book data, and order flow arrival statistics. Within the experiment, a forward-feed neural network is used as the function approximator and two reward functions are compared. The performance of each combination of agent and reward function is evaluated by daily and average trade returns. Using this DRLMM framework, this paper demonstrates the effectiveness of deep reinforcement learning in solving stochastic inventory control challenges market makers face.",Jonathan Sadighian,,2019,,,
1159,Double auction dynamics: structural effects of non-binding price controls,"In competitive equilibrium, non-binding price controls (that is, price floors below and ceilings above the equilibrium) should not affect market outcomes, but in laboratory experiments they do. We build a simple dynamic model of double auction markets with “zero-intelligence” (ZI) computer traders that accounts for many, though not all, of the discrepancies between the data and the Walrasian tatonnement predictions. The success of the model in organizing the data, and in isolating various consequences of price controls, shows that the simple ZI model is a powerful tool to gain insights into the dynamics of market institutions.","Dhananjay (Dan) K. Gode,Shyam Sunder",Journal of Economic Dynamics and Control,2004,,,
1160,An Analysis of the Implications for Stock and Futures Price Volatility of Program Trading and Dynamic Hedging Strategies,"The notion that a real security is redundant when it can be synthesized by a dynamic trading strategy ignores the informati onal role of real securities markets. Portfolio insurance uses a dyna mic strategy to synthesize a European put. The absence of trading in an appropriate real put option prevents the transmittal of informatio n to market participants about the future price volatility associated with current dynamic hedging strategies. Less information is transmi tted to potential liquidity providers. It will, therefore, be more di fficult for the market to absorb the trades implied by the dynamic he dging strategies, and the stocks' future price volatility will rise. Copyright 1988 by the University of Chicago.",Sanford J. Grossman,The Journal of Business,1988,,,
1161,What do insiders know? Evidence from insider trading around share repurchases and SEOs,"Abstract   We examine the nature of information contained in insider trades prior to corporate events. Insiders' net buying increases before open market share repurchase announcements and decreases before seasoned equity offers. Higher insider net buying is associated with better post-event operating performance, a reduction in undervaluation, and, for repurchases, lower post-event cost of capital. Insider trading also predicts announcement returns and long-term abnormal returns following events. Overall, our results suggest that insider trades before corporate events contain information about changes both in fundamentals and in investor sentiment.","Peter Cziraki,Evgeny Lyandres,Roni Michaely",Journal of Corporate Finance,2019,,,
1162,Continuous-Time Mean-Variance Portfolio Selection: A Reinforcement Learning Framework,"We approach the continuous-time mean-variance (MV) portfolio selection with reinforcement learning (RL). The problem is to achieve the best tradeoff between exploration and exploitation, and is formulated as an entropy-regularized, relaxed stochastic control problem. We prove that the optimal feedback policy for this problem must be Gaussian, with time-decaying variance. We then establish connections between the entropy-regularized MV and the classical MV, including the solvability equivalence and the convergence as exploration weighting parameter decays to zero. Finally, we prove a policy improvement theorem, based on which we devise an implementable RL algorithm. We find that our algorithm outperforms both an adaptive control based method and a deep neural networks based algorithm by a large margin in our simulations.","Haoran Wang,Xun Yu Zhou",,2019,,,
1163,Deep Hedging: Hedging Derivatives Under Generic Market Frictions Using Reinforcement Learning,"This article discusses a new application of reinforcement learning: to the problem of hedging a portfolio of “over-the-counter” derivatives under under market frictions such as trading costs and liquidity constraints. It is an extended version of our recent work https://www.ssrn.com/abstract=3120710, here using notation more common in the machine learning literature. The objective is to maximize a non-linear risk-adjusted return function by trading in liquid hedging instruments such as equities or listed options. The approach presented here is the first efficient and model-independent algorithm which can be used for such problems at scale.","Hans Buehler,Lukas Gonon,Josef Teichmann,Ben Wood,Baranidharan Mohan,Jonathan Kochems",,2019,,,
1164,Dark trading and price discovery,"Abstract   Regulators globally are concerned that dark trading harms price discovery. We show that dark trades are less informed than lit trades. High levels of dark trading increase adverse selection risk on the lit exchange by increasing the concentration of informed traders. Using both high- and low-frequency measures of informational efficiency we find that low levels of non-block dark trading are benign or even beneficial for informational efficiency, but high levels are harmful. In contrast, we find no evidence that block trades in the dark impede price discovery.","Carole Comerton-Forde,Talis J. Putnins,Tālis J. Putniņš",Journal of Financial Economics,2015,,,
1165,Qlib: An AI-oriented Quantitative Investment Platform,"Quantitative investment aims to maximize the return and minimize the risk in a sequential trading period over a set of financial instruments. Recently, inspired by rapid development and great potential of AI technologies in generating remarkable innovation in quantitative investment, there has been increasing adoption of AI-driven workflow for quantitative research and practical investment. In the meantime of enriching the quantitative investment methodology, AI technologies have raised new challenges to the quantitative investment system. Particularly, the new learning paradigms for quantitative investment call for an infrastructure upgrade to accommodate the renovated workflow; moreover, the data-driven nature of AI technologies indeed indicates a requirement of the infrastructure with more powerful performance; additionally, there exist some unique challenges for applying AI technologies to solve different tasks in the financial scenarios. To address these challenges and bridge the gap between AI technologies and quantitative investment, we design and develop Qlib that aims to realize the potential, empower the research, and create the value of AI technologies in quantitative investment.","Xiao Yang,Weiqing Liu,Weiqing Liu,Liu Weiqing,Dong Zhou,Jiang Bian,Jiang Bian,Tie-Yan Liu",,2020,,,
1166,How Noise Trading Affects Markets: An Experimental Analysis,"We use a laboratory market to investigate the behavior of traders who lack informational advantages and have no exogenous reason to trade. We find that these uninformed traders behave largely as irrational contrarian ""noise traders,"" trading against recent price movements to their own detriment. The uninformed traders provide some benefits to the market: increasing market volume and depth, while reducing bid-ask spreads and the temporary price impact of trades. However, their noise trading also diminishes the ability of market prices to adjust to new information. A securities transaction tax reduces uninformed trader activity, but it reduces informed trader activity by approximately the same amount; as a result, the tax does not alter the impact of noise trading on the informational efficiency of the market. The Author 2009. Published by Oxford University Press on behalf of The Society for Financial Studies. All rights reserved. For Permissions, please e-mail: journals.permissions@oxfordjournals.org., Oxford University Press.","Robert J. Bloomfield,Maureen O'Hara,Gideon Saar",Review of Financial Studies,2009,,,
1167,Logistic Regression in Rare Events Data,"We study rare events data, binary dependent variables with dozens to thousands of times fewer ones (events, such as wars, vetoes, cases of political activism, or epidemiological infections) than zeros (“nonevents”). In many literatures, these variables have proven difficult to explain and predict, a problem that seems to have at least two sources. First, popular statistical procedures, such as logistic regression, can sharply underestimate the probability of rare events. We recommend corrections that outperform existing methods and change the estimates of absolute and relative risks by as much as some estimated effects reported in the literature. Second, commonly used data collection strategies are grossly inefficient for rare events data. The fear of collecting data with too few events has led to data collections with huge numbers of observations but relatively few, and poorly measured, explanatory variables, such as in international conflict data with more than a quarter-million dyads, only a few of which are at war. As it turns out, more efficient sampling designs exist for making valid inferences, such as sampling all available events (e.g., wars) and a tiny fraction of nonevents (peace). This enables scholars to save as much as 99% of their (nonfixed) data collection costs or to collect much more meaningful explanatory variables. We provide methods that link these two results, enabling both types of corrections to work simultaneously, and software that implements the methods developed.","Gary King,Langche Zeng",Political Analysis,2001,,,
1168,DoubleEnsemble: A New Ensemble Method Based on Sample Reweighting and Feature Selection for Financial Data Analysis,"Modern machine learning models (such as deep neural networks and boosting decision tree models) have become increasingly popular in financial market prediction, due to their superior capacity to extract complex non-linear patterns. However, since financial datasets have very low signal-to-noise ratio and are non-stationary, complex models are often very prone to overfitting and suffer from instability issues. Moreover, as various machine learning and data mining tools become more widely used in quantitative trading, many trading firms have been producing an increasing number of features (aka factors). Therefore, how to automatically select effective features becomes an imminent problem. To address these issues, we propose DoubleEnsemble, an ensemble framework leveraging learning trajectory based sample reweighting and shuffling based feature selection. Specifically, we identify the key samples based on the training dynamics on each sample and elicit key features based on the ablation impact of each feature via shuffling. Our model is applicable to a wide range of base models, capable of extracting complex patterns, while mitigating the overfitting and instability issues for financial market prediction. We conduct extensive experiments, including price prediction for cryptocurrencies and stock trading, using both DNN and gradient boosting decision tree as base models. Our experiment results demonstrate that DoubleEnsemble achieves a superior performance compared with several baseline methods.","Chuheng Zhang,Yuanqi Li,Xi Chen,Yifei Jin,Pingzhong Tang,Jian Li,Jian Li,Jian Li",,2020,,,
1169,Bridging the Gap Between Markowitz Planning and Deep Reinforcement Learning,"While researchers in the asset management industry have mostly focused on techniques based on financial and risk planning techniques like Markowitz efficient frontier, minimum variance, maximum diversification or equal risk parity, in parallel, another community in machine learning has started working on reinforcement learning and more particularly deep reinforcement learning to solve other decision making problems for challenging task like autonomous driving, robot learning, and on a more conceptual side games solving like Go. 

This paper aims to bridge the gap between these two approaches by showing Deep Reinforcement Learning (DRL) techniques can shed new lights on portfolio allocation thanks to a more general optimization setting that casts portfolio allocation as an optimal control problem that is not just a one-step optimization, but rather a continuous control optimization with a delayed reward. The advantages are numerous: (i) DRL maps directly market conditions to actions by design and hence should adapt to changing environment, (ii) DRL does not rely on any traditional financial risk assumptions like that risk is represented by variance, (iii) DRL can incorporate additional data and be a multi inputs method as opposed to more traditional optimization methods. We present on an experiment some encouraging results using convolution networks.","Eric Benhamou,Eric Benhamou,Eric Benhamou,David Saltiel,Sandrine Ungari,Abhishek Mukhopadhyay",,2020,,,
1170,Deep Reinforcement Learning for Autonomous Driving: A Survey,"With the development of deep representation learning, the domain of reinforcement learning (RL) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical RL algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in RL are discussed.","B Ravi Kiran,Bangalore Ravi Kiran,Ibrahim Sobh,Victor Talpaert,Patrick Mannion,Ahmad A. Al Sallab,Senthil Yogamani,Patrick Pérez",IEEE Transactions on Intelligent Transportation Systems,2021,,,
1171,A Modularized and Scalable Multi-Agent Reinforcement Learning-based System for Financial Portfolio Management.,"Financial Portfolio Management is one of the most applicable problems in Reinforcement Learning (RL) by its sequential decision-making nature. Existing RL-based approaches, while inspiring, often lack scalability, reusability, or profundity of intake information to accommodate the ever-changing capital markets. In this paper, we design and develop MSPM, a novel Multi-agent Reinforcement learning-based system with a modularized and scalable architecture for portfolio management. MSPM involves two asynchronously updated units: Evolving Agent Module (EAM) and Strategic Agent Module (SAM). A self-sustained EAM produces signal-comprised information for a specific asset using heterogeneous data inputs, and each EAM possesses its reusability to have connections to multiple SAMs. A SAM is responsible for the assets reallocation of a portfolio using profound information from the EAMs connected. With the elaborate architecture and the multi-step condensation of the volatile market information, MSPM aims to provide a customizable, stable, and dedicated solution to portfolio management that existing approaches do not. We also tackle data-shortage issue of newly-listed stocks by transfer learning, and validate the necessity of EAM. Experiments on 8-year U.S. stock markets data prove the effectiveness of MSPM in profits accumulation by its outperformance over existing benchmarks.","Zhenhan Huang,Fumihide Tanaka,Fumihide Tanaka",arXiv: Portfolio Management,2021,,,
1172,EnAET: A Self-Trained framework for Semi-Supervised and Supervised Learning with Ensemble Transformations,"Deep neural networks have been successfully applied to many real-world applications. However, such successes rely heavily on large amounts of labeled data that is expensive to obtain. Recently, many methods for semi-supervised learning have been proposed and achieved excellent performance. In this study, we propose a new EnAET framework to further improve existing semi-supervised methods with self-supervised information. To our best knowledge, all current semi-supervised methods improve performance with prediction consistency and confidence ideas. We are the first to explore the role of {\bf self-supervised} representations in {\bf semi-supervised} learning under a rich family of transformations. Consequently, our framework can integrate the self-supervised information as a regularization term to further improve {\it all} current semi-supervised methods. In the experiments, we use MixMatch, which is the current state-of-the-art method on semi-supervised learning, as a baseline to test the proposed EnAET framework. Across different datasets, we adopt the same hyper-parameters, which greatly improves the generalization ability of the EnAET framework. Experiment results on different datasets demonstrate that the proposed EnAET framework greatly improves the performance of current semi-supervised algorithms. Moreover, this framework can also improve {\bf supervised learning} by a large margin, including the extremely challenging scenarios with only 10 images per class. The code and experiment records are available in \url{https://github.com/maple-research-lab/EnAET}.","Xiao Wang,Xiao Wang,Xiao Wang,Xiao Wang,Daisuke Kihara,Jiebo Luo,Guo-Jun Qi",arXiv: Computer Vision and Pattern Recognition,2019,,,
1173,Deep Stock Trading: A Hierarchical Reinforcement Learning Framework for Portfolio Optimization and Order Execution,"Portfolio management via reinforcement learning is at the forefront of fintech research, which explores how to optimally reallocate a fund into different financial assets over the long term by trial-and-error. Existing methods are impractical since they usually assume each reallocation can be finished immediately and thus ignoring the price slippage as part of the trading cost. To address these issues, we propose a hierarchical reinforced stock trading system for portfolio management (HRPM). Concretely, we decompose the trading process into a hierarchy of portfolio management over trade execution and train the corresponding policies. The high-level policy gives portfolio weights at a lower frequency to maximize the long term profit and invokes the low-level policy to sell or buy the corresponding shares within a short time window at a higher frequency to minimize the trading cost. We train two levels of policies via pre-training scheme and iterative training scheme for data efficiency. Extensive experimental results in the U.S. market and the China market demonstrate that HRPM achieves significant improvement against many state-of-the-art approaches.","Rundong Wang,Hongxin Wei,Bo An,Zhouyan Feng,Jun Yao",arXiv: Artificial Intelligence,2020,,,
1174,Deep ensemble learning-based approach to real-time power system state estimation,"Abstract   Power system state estimation (PSSE) is commonly formulated as weighted least-square (WLS) algorithm and solved using iterative methods such as Gauss-Newton methods. However, iterative methods have become more sensitive to system operating conditions than ever before due to the deployment of intermittent renewable energy sources, zero emission technologies (e.g., electric vehicles), and demand response programs. Appropriate PSSE approaches are required to avoid pitfalls of the WLS-based PSSE computations for accurate prediction of operating conditions. This paper proposes a data-driven real-time PSSE using a deep ensemble learning algorithm. In the proposed approach, the ensemble learning setup is formulated with dense residual neural networks as base-learners and multivariate-linear regressor as meta-learner. Historical measurements and states are utilised to train and test the model. The trained model can be used in real-time to estimate power system states (voltage magnitudes and phase angles) using real-time measurements. Most of current data-driven PSSE methods assume the availability of a complete set of measurements, which may not be the case in real power system data-acquisition. This paper adopts multivariate linear regression to forecast system states for instants of missing measurements to assist the proposed PSSE technique. Case studies are performed on various IEEE standard benchmark systems to validate the proposed approach. The results show that the proposed approach outperforms existing data-driven PSSE techniques. The developed source code of the proposed solution is publicly available at  https://github.com/nbhusal/Power-System-State-Estimation .","Narayan Bhusal,Raj Mani Shukla,Mukesh Gautam,Mohammed Benidris,Shamik Sengupta",International Journal of Electrical Power & Energy Systems,2021,,,
1175,Learning to trade in financial time series using high-frequency through wavelet transformation and deep reinforcement learning,"Deep learning-based financial approaches have received attention from both investors and researchers. This study demonstrates how to optimize portfolios, asset allocation, and trading systems based on deep reinforcement learning using three frameworks. In the proposed deep learning structure, the input data are first decomposed through wavelet transformation (WT) to remove noise from stock price time-series data. Then, only the mother wavelet (high-frequency) data are used as input. Second, reinforcement learning is performed using the high-frequency data. The reinforcement learning network employs long short-term memory (LSTM). Actions are determined by the LSTM network or randomly. Third, it learns the optimal investment trading system using the actions of a given transaction and appropriate rewards. The structure of the optimal investment trading system obtained by the proposed deep reinforcement learning structure improves trading performance without requiring the construction of a predictive model. To investigate the performance of the proposed structure, we applied the S&P500, DJI, and KOSPI200 indices to the proposed structure (HW_LSTM_RL) and other reinforcement learning structures for comparison. We evaluated the difference in Sharpe ratio for various test periods (one to three years) and for different rewards. Using the decomposed high-frequency data as input, a portfolio of investment transactions was improved for highly volatile markets. In deep reinforcement learning, we found that network composition and appropriate rewards have significant influence on learning transactions in financial time-series data. Thus, the proposed HW_LSTM_RL structure demonstrates the importance of input data composition, learning network settings, and rewards.","Jimin Lee,Hayeong Koh,Hi Jun Choe",Applied Intelligence,2021,,,
1176,Mean-variance optimization for asset allocation,"The mean–variance model is widely acknowledged as the foundation of portfolio allocation because it provides a framework for analyzing the trade-off between risk and return for gaining diversification benefits. Despite the well-known shortcomings of the model, it is often the starting point for making asset allocation decisions. In this article, the authors briefly review mean–variance optimization and approaches for resolving its limitations by demonstrating backtest results on asset allocation. Feedback from asset managers is also included to explain how optimization methods are applied in practice.  TOPICS:Quantitative methods, statistical methods, portfolio construction, performance measurement  Key Findings  ▪ Mean–variance optimization provides a framework for analyzing risk–return trade-offs when making asset allocation decisions.  ▪ Sensitivity of the mean–variance model can be addressed with robust models, and the model generates candidate asset allocations that are informative in practice.  ▪ Feedback from asset managers suggests that mean–variance optimization along with simulation and various robust methods are being applied in practice for asset allocation.","Jang Ho Kim,Yongjae Lee,Woo Chang Kim,Frank J. Fabozzi",The Journal of Portfolio Management,2021,,,
1177,NVIDIA A100 Tensor Core GPU: Performance and Innovation,"NVIDIA A100 Tensor Core GPU is NVIDIA's latest flagship GPU. It has been designed with many new innovative features to provide performance and capabilities for HPC, AI, and data analytics workloads. Feature enhancements include a Third-Generation Tensor Core, new asynchronous data movement and programming model, enhanced L2 cache, HBM2 DRAM, and third-generation NVIDIA NVLink I/O.","Jack Hilaire Choquette,Gandhi Wishwesh Anil,Wishwesh Gandhi,Olivier Giroux,Nick Stam,Ronny Krashinsky,Krashinsky Ronny Meir",IEEE Micro,2021,,,
1178,Deep Learning for Portfolio Optimisation,"We adopt deep learning models to directly optimize the portfolio Sharpe ratio. The framework we present circumvents the requirements for forecasting expected returns and allows us to directly optimize portfolio weights by updating model parameters. Instead of selecting individual assets, we trade Exchange-Traded Funds (ETFs) of market indices to form a portfolio. Indices of different asset classes show robust correlations and trading them substantially reduces the spectrum of available assets to choose from. We compare our method with a wide range of algorithms with results showing that our model obtains the best performance over the testing period, from 2011 to the end of April 2020, including the financial instabilities of the first quarter of 2020. A sensitivity analysis is included to understand the relevance of input features and we further study the performance of our approach under different cost rates and different risk levels via volatility scaling.","Zihao Zhang,Zihao Zhang,Zihao Zhang,Stefan Zohren,Stephen J. Roberts",,2020,,,
1179,Sustainable MLOps: Trends and Challenges,"Even simply through a GoogleTrends search it becomes clear that Machine-Learning Operations-or MLOps, for short-are climbing in interest from both a scientific and practical perspective. On the one hand, software components and middleware are proliferating to support all manners of MLOps, from AutoML (i.e., software which enables developers with limited machine-learning expertise to train high-quality models specific to their domain or data) to feature-specific ML engineering, e.g., Explainability and Interpretability. On the other hand, the more these platforms penetrate the day-to-day activities of software operations, the more the risk for AI Software becoming unsustainable from a social, technical, or organisational perspective. This paper offers a concise definition of MLOps and AI Software Sustainability and outlines key challenges in its pursuit.",Damian A. Tamburri,,2020,,,
1180,Special issue on Artificial Intelligence in Engineering Education,,Priyan Malarvizhi Kumar,Computer Applications in Engineering Education,2021,,,
1181,The applicability of self-play algorithms to trading and forecasting financial markets : a feasibility study,"The central research question to answer in this feasibility study is whether the Artificial Intelligence (AI) methodology of Self-Play can be applied to financial markets. In typical use-cases of Self-Play, two AI agents play against each other in a particular game, e.g. chess or Go. By repeatedly playing the game, they learn its rules as well as possible winning strategies. When considering financial markets, however, we usually have one player – the trader – that does not face one individual adversary but competes against a vast universe of other market participants. Furthermore, the optimal behaviour in financial markets is not described via a winning strategy, but via the objective of maximising profits while managing risks appropriately. Lastly, data issues cause additional challenges, since, in finance, they are quite often incomplete, noisy and difficult to obtain.

We will show that academic research using Self-Play has mostly not focused on finance, and if it has, it was usually restricted to stock markets, not considering the large FX, commodities and bond markets. Despite those challenges, we see enormous potential of applying self-play concepts and algorithms to financial markets.","Jan-Alexander Posth,Jan-Alexander Posth,Branka Hadji Misheva,Branka Hadji Misheva,Piotr Kamil Kotlarz,Jörg Osterrieder,Jörg Osterrieder,Peter Schwendner",,2020,,,
1182,Automatic COVID-19 detection from X-ray images using ensemble learning with convolutional neural network,"COVID-19 continues to have catastrophic effects on the lives of human beings throughout the world. To combat this disease it is necessary to screen the affected patients in a fast and inexpensive way. One of the most viable steps towards achieving this goal is through radiological examination, Chest X-Ray being the most easily available and least expensive option. In this paper, we have proposed a Deep Convolutional Neural Network-based solution which can detect the COVID-19 +ve patients using chest X-Ray images. Multiple state-of-the-art CNN models—DenseNet201, Resnet50V2 and Inceptionv3, have been adopted in the proposed work. They have been trained individually to make independent predictions. Then the models are combined, using a new method of weighted average ensembling technique, to predict a class value. To test the efficacy of the solution we have used publicly available chest X-ray images of COVID +ve and –ve cases. 538 images of COVID +ve patients and 468 images of COVID –ve patients have been divided into training, test and validation sets. The proposed approach gave a classification accuracy of 91.62% which is higher than the state-of-the-art CNN models as well the compared benchmark algorithm. We have developed a GUI-based application for public use. This application can be used on any computer by any medical personnel to detect COVID +ve patients using Chest X-Ray images within a few seconds.","Amit Kumar Das,Sayantani Ghosh,Samiruddin Thunder,Rohit Dutta,Sachin Agarwal,Amlan Chakrabarti",Pattern Analysis and Applications,2021,,,
1183,Using Computer-Aided Design Software in Teaching Environmental Art Design,"Computer-Aided Design and Applications is an international journal on the applications of CAD and CAM. It publishes papers in the general domain of CAD plus in emerging fields like bio-CAD, nano-CAD, soft-CAD, garment-CAD, PLM, PDM, CAD data mining, CAD and the internet, CAD education, genetic algorithms and CAD engines. The journal is aimed at all developers and users of CAD technology to ptovide CAD solutions for various stages of design and manufacturing. The journal publishes all about Computer-Aided Design and Computer-Aided technologies.","Hua Jin,Jie Yang,Jie Yang",Computer-aided Design and Applications,2021,,,
1184,DeL-IoT: A deep ensemble learning approach to uncover anomalies in IoT,"Abstract   Internet of Things (IoT) devices are inherently vulnerable due to insecure design, implementation, and configuration. Aggressive behavior changes, due to increased attacker’s sophistication, and the heterogeneity of the data in IoT have proven that securing IoT devices trigger multiple challenges. It includes complex and dynamic attack detection, data imbalance, data heterogeneity, real-time response, and prediction capability. Most researchers are not focusing on the class imbalance, dynamic attack detection, and data heterogeneity problems together in Software-Defined Networking (SDN) enabled IoT anomaly detection. Thus, to address these challenging tasks, we propose DeL-IoT, a deep ensemble learning framework for IoT anomaly detection and prediction using SDN, having three primary modules including anomaly detection, intelligent flow management, and device status forecasting. The DeL-IoT employs deep and stacked autoencoders to extract handy features for stacking into an ensemble learning model. This framework yields efficient detection of anomalies, manages flows dynamically, and forecasts both short and long-term device status for early action. We validate the proposed DeL-IoT framework with testbed and benchmark datasets. We demonstrate that in even a 1% imbalanced dataset, the performance of our proposed method, deep feature extraction with a deep ensemble learning model, is around 3% better than the single model. The extensive experimental results show that our models have a better and more reliable performance than the competing models showcased in the relevant related work.","Enkhtur Tsogbaatar,Enkhtur Tsogbaatar,Monowar H. Bhuyan,Yuzo Taenaka,Yuzo Taenaka,Doudou Fall,Khishigjargal Gonchigsumlaa,Erik Elmroth,Youki Kadobayashi",,2021,,,
1185,Market sentiment-aware deep reinforcement learning approach for stock portfolio allocation,"Abstract   The stock market currently remains one of the most difficult systems to model in finance. Hence, it is a challenge to solve stock portfolio allocation wherein an optimal investment strategy must be found for a curated collection of stocks that effectively maximizes return while minimizing the risk involved. Deep reinforcement learning approaches have shown promising results when used to automate portfolio allocation, by training an intelligent agent on historical stock prices. However, modern investors are actively engaging with digital platforms such as social media and online news websites to understand and better analyze portfolios. The overall attitude thus formed by investors toward a particular stock or financial market is known as market sentiment. Existing approaches do not incorporate market sentiment which has been empirically shown to influence investor decisions. In our paper, we propose a novel deep reinforcement learning approach to effectively train an intelligent automated trader, that not only uses the historical stock price data but also perceives market sentiment for a stock portfolio consisting of the Dow Jones companies. We demonstrate that our approach is more robust in comparison to existing baselines across standardized metrics such as the Sharpe ratio and annualized investment return.","Prahlad Koratamaddi,Karan Wadhwani,Mridul Gupta,Sriram G. Sanjeevi","Engineering Science and Technology, an International Journal",2021,,,
1186,Providing music service in Ambient Intelligence: experiments with gym users,"Abstract   Ambient Intelligence (AmI) is an interdisciplinary research area of ICT which has evolved since the 90s, taking great advantage from the advent of the Internet of Things (IoT). AmI creates, by using Artificial Intelligence (AI), an intelligent ecosystem in which computers, sensors, lighting, music, personal devices, and distributed services, work together to improve the user experience through the support of natural and intuitive user interfaces. Nowadays, AmI is used in various contexts, e.g., for building smart homes and smart cities, providing healthcare, and creating an adequate atmosphere in retail and public environments.  In this paper, we propose a novel AmI system for gym environments, named Gym Intelligence, able to provide adequate music atmosphere, according to the users’ physical effort during the training. The music is taken from Spotify and is classified according to some music features, as provided by Spotify itself. The system is based on a multi-agent computational intelligence model built on two main components:     (  i  )     machine learning methods that forecast appropriate values for the Spotify music features, and     (  ii  )     a multi-objective dynamic genetic algorithm that selects a specific Spotify music track, according to such values. Gym Intelligence is built by sensing the ambient with a minimal, low-cost, and non-intrusive set of sensors, and it has been designed considering the outcome of a preliminary analysis in real gyms, involving real users. We have considered well-known regression methods and we have validated them using a collected data     (  i  )     about the users’ physical effort, through the sensors, and     (  ii  )     about the users’ music preferences, through an Android app that the users have used during the training. Among the regression methods considered, the one that provided the best results is the Random Forest, which predicted Spotify music features with a mean absolute error of 0.02 and a root mean squared error of 0.05. We have implemented Gym Intelligence and deployed it in five real gyms. We have evaluated it conducting several experiments. The experiments show how, with the help of Gym Intelligence, the users’ satisfaction about the provided background music, rose from 3.05 to 4.91 (on a scale from 1 to 5, where 5 is the maximum score).","Roberto De Prisco,Alfonso Guarino,Nicola Lettieri,Delfina Malandrino,Rocco Zaccagnino",Expert Systems With Applications,2021,,,
1187,Portfolio management system in equity market neutral using reinforcement learning,"Portfolio management involves position sizing and resource allocation. Traditional and generic portfolio strategies require forecasting of future stock prices as model inputs, which is not a trivial task since those values are difficult to obtain in the real-world applications. To overcome the above limitations and provide a better solution for portfolio management, we developed a Portfolio Management System (PMS) using reinforcement learning with two neural networks (CNN and RNN). A novel reward function involving Sharpe ratios is also proposed to evaluate the performance of the developed systems. Experimental results indicate that the PMS with the Sharpe ratio reward function exhibits outstanding performance, increasing return by 39.0% and decreasing drawdown by 13.7% on average compared to the reward function of trading return. In addition, the proposed PMS_CNN model is more suitable for the construction of a reinforcement learning portfolio, but has 1.98 times more drawdown risk than the PMS_RNN. Among the conducted datasets, the PMS outperforms the benchmark strategies in TW50 and traditional stocks, but is inferior to a benchmark strategy in the financial dataset. The PMS is profitable, effective, and offers lower investment risk among almost all datasets. The novel reward function involving the Sharpe ratio enhances performance, and well supports resource-allocation for empirical stock trading.","Mu-En Wu,Jia-Hao Syu,Jerry Chun-Wei Lin,Jan-Ming Ho",Applied Intelligence,2021,,,
1188,Frequent Pattern Mining,"This comprehensive reference consists of 18 chapters from prominent researchers in the field. Each chapter is self-contained, and synthesizes one aspect of frequent pattern mining. An emphasis is placed on simplifying the content, so that students and practitioners can benefit from the book. Each chapter contains a survey describing key research on the topic, a case study and future directions. Key topics include: Pattern Growth Methods, Frequent Pattern Mining in Data Streams, Mining Graph Patterns, Big Data Frequent Pattern Mining, Algorithms for Data Clustering and more. Advanced-level students in computer science, researchers and practitioners from industry will find this book an invaluable reference.","Charu C. Aggarwal,Jiawei Han",,2014,,,
1189,Podracer architectures for scalable Reinforcement Learning,"Supporting state-of-the-art AI research requires balancing rapid prototyping, ease of use, and quick iteration, with the ability to deploy experiments at a scale traditionally associated with production systems.Deep learning frameworks such as TensorFlow, PyTorch and JAX allow users to transparently make use of accelerators, such as TPUs and GPUs, to offload the more computationally intensive parts of training and inference in modern deep learning systems. Popular training pipelines that use these frameworks for deep learning typically focus on (un-)supervised learning. How to best train reinforcement learning (RL) agents at scale is still an active research area. In this report we argue that TPUs are particularly well suited for training RL agents in a scalable, efficient and reproducible way. Specifically we describe two architectures designed to make the best use of the resources available on a TPU Pod (a special configuration in a Google data center that features multiple TPU devices connected to each other by extremely low latency communication channels).","Matteo Hessel,Manuel Kroiss,Aidan Clark,Iurii Kemaev,Iurii Kemaev,Iurii Kemaev,John Quan,Thomas Keck,Fabio Viola,Hado van Hasselt,Hado van Hasselt,Hado van Hasselt",arXiv: Learning,2021,,,
1190,From Technology to Society: An Overview of Blockchain-Based DAO,"Decentralized Autonomous Organization (DAO) is believed to play a significant role in our future society governed in a decentralized way. In this article, we first explain the definitions and preliminaries of DAO. Then, we conduct a literature review of the existing studies of DAO published in the recent few years. Through the literature review, we find out that a comprehensive survey towards the state-of-the-art studies of DAO is still missing. To fill this gap, we perform such an overview by identifying and classifying the most valuable proposals and perspectives closely related to the combination of DAO and blockchain technologies. We anticipate that this survey can help researchers, engineers, and educators acknowledge the cutting-edge development of blockchain-related DAO technologies.","Lu Liu,Lu Liu,Lu Liu,Sicong Zhou,Huawei Huang,Zibin Zheng,Zibin Zheng",,2021,,,
1191,Factor Selection with Deep Reinforcement Learning for Financial Forecasting,"The selection of a relevant factor set is essential for obtaining a financial forecasting model with high information coefficient (IC). We introduce a factor selection algorithm that uses a value network to evaluate the ICs of forecasting models and a genetic algorithm to propose new factor set candidates. The value network is trained from backtesting with historical data, with the genetic algorithm proposing candidates based on the value network evaluation. Such reinforcement learning approach iteratively selects factor sets that result in higher ICs. We demonstrate a substantial performance improvement on a typical robust linear regression model. By providing an optimal input factor set, our approach can likely improve any forecasting model, including those with built-in factor selection operations. Using the validation data, we demonstrate that our algorithm is not merely fitting historical patterns. It also shows high efficacy in achieving better factor set selection for forward prediction.","Ziwei Wang,Nelson Leung",Social Science Research Network,2018,,,
1192,Exploring the Scale-Free Nature of Stock Markets: Hyperbolic Graph Learning for Algorithmic Trading,"Quantitative trading and investment decision making are intricate financial tasks in the ever-increasing sixty trillion dollars global stock market. Despite advances in stock forecasting, a limitation of most existing neural methods is that they treat stocks independent of each other, ignoring the valuable rich signals between related stocks’ movements. Motivated by financial literature that shows stock markets and inter-stock correlations show scale-free network characteristics, we leverage domain knowledge on the Web to model inter-stock relations as a graph in four major global stock markets and formulate stock selection as a scale-free graph-based learning to rank problem. To capture the scale-free spatial and temporal dependencies in stock prices, we propose HyperStockGAT: Hyperbolic Stock Graph Attention Network, the first model on the Riemannian Manifolds for stock selection. Our work’s key novelty is the proposal of modeling the complex, scale-free nature of inter-stock relations through temporal hyperbolic graph learning on Riemannian manifolds that can represent the spatial correlations between stocks more accurately. Through extensive experiments on long-term real-world data spanning over six years on four of the world’s biggest markets: NASDAQ, NYSE, TSE, and China exchanges, we show that HyperStockGAT significantly outperforms state-of-the-art stock forecasting methods in terms of profitability by over 12%, and risk-adjusted Sharpe Ratio by over 4%. We analyze HyperStockGAT’s components’ contributions through a series of exploratory and ablative experiments to demonstrate its practical applicability to real-world trading. Furthermore, we propose a novel hyperbolic architecture that can be applied across various spatiotemporal problems on the Web’s commonly occurring scale-free networks.","Ramit Sawhney,Ramit Sawhney,Ramit Sawhney,Shivam Agarwal,Arnav Wadhwa,Rajiv Ratn Shah",,2021,,,
1193,Random Vector Functional Link Neural Network based Ensemble Deep Learning,"Abstract   In this paper, we propose deep learning frameworks based on the randomized neural network. Inspired by the principles of Random Vector Functional Link (RVFL) network, we present a deep RVFL network (dRVFL) with stacked layers. The parameters of the hidden layers of the dRVFL are randomly generated within a suitable range and kept fixed while the output weights are computed using the closed-form solution as in a standard RVFL network. We also propose an ensemble deep network (edRVFL) that can be regarded as a marriage of ensemble learning with deep learning. Unlike traditional ensembling approaches that require training several models independently from scratch, edRVFL is obtained by training a single dRVFL network once. Both dRVFL and edRVFL frameworks are generic and can be used with any RVFL variant. To illustrate this, we integrate the deep learning RVFL networks with a recently proposed sparse pre-trained RVFL (SP-RVFL). Experiments on 46 tabular UCI classification datasets and 12 sparse datasets demonstrate that the proposed deep RVFL networks outperform state-of-the-art deep feed-forward neural networks (FNNs).","Rakesh Katuwal,Qiushi Shi,Ponnuthurai Nagaratnam Suganthan,M. Tanveer",Pattern Recognition,2019,,,
1194,Collaborative city digital twin for the COVID-19 pandemic: A federated learning solution,"The novel coronavirus, COVID-19, has caused a crisis that affects all segments of the population  As the knowledge and understanding of COVID-19 evolve, an appropriate response plan for this pandemic is considered one of the most effective methods for controlling the spread of the virus  Recent studies indicate that a city Digital Twin (DT) is beneficial for tackling this health crisis, because it can construct a virtual replica to simulate factors, such as climate conditions, response policies, and people's trajectories, to help plan efficient and inclusive decisions  However, a city DTsystem relies on long-term and high-quality data collection to make appropriate decisions, limiting its advantages when facing urgent crises, such as the COVID-19 pandemic  Federated Learning (FL), in which all clients can learn a shared model while retaining all training data locally, emerges as a promising solution for accumulating the insights from multiple data sources efficiently Furthermore, the enhanced privacy protection settings removing the privacy barriers lie in this collaboration  In this work, we propose a framework that fused city DT with FL to achieve a novel collaborative paradigm that allows multiple city DTs to share the local strategy and status quickly  In particular, an FL central server manages the local updates of multiple collaborators (city DTs), providing a global model that is trained in multiple iterations at different city DT systems until the model gains the correlations between various response plans and infection trends  This approach means a collaborative city DT paradigm fused with FL techniques can obtain knowledge and patterns from multiple DTs and eventually establish a 'global view' of city crisis management  Meanwhile, it also helps improve each city's DT by consolidating other DT's data without violating privacy rules  In this paper, we use the COVID-19 pandemic as the use case of the proposed framework  The experimental results on a real dataset with various response plans validate our proposed solution and demonstrate its superior performance  ©2021 Tsinghua University Press  © 2021 Tsinghua University Press  All rights reserved","Junjie Pang,Yan Huang,Yan Huang,Zhenzhen Xie,Jianbo Li,Jianbo Li,Jianbo Li,Zhipeng Cai,Zhipeng Cai,Zhipeng Cai",Tsinghua Science & Technology,2021,,,
1195,FinGAT: Financial Graph Attention Networks for Recommending Top-K Profitable Stocks,"Financial technology (FinTech) has drawn much attention among investors and companies. While conventional stock analysis in FinTech targets at predicting stock prices, less effort is made for profitable stock recommendation. Besides, in existing approaches on modeling time series of stock prices, the relationships among stocks and sectors (i.e., categories of stocks) are either neglected or pre-defined. Ignoring stock relationships will miss the information shared between stocks while using pre-defined relationships cannot depict the latent interactions or influence of stock prices between stocks. In this work, we aim at recommending the top-K profitable stocks in terms of return ratio using time series of stock prices and sector information. We propose a novel deep learning-based model, Financial Graph Attention Networks (FinGAT), to tackle the task under the setting that no pre-defined relationships between stocks are given. The idea of FinGAT is three-fold. First, we devise a hierarchical learning component to learn short-term and long-term sequential patterns from stock time series. Second, a fully-connected graph between stocks and a fully-connected graph between sectors are constructed, along with graph attention networks, to learn the latent interactions among stocks and sectors. Third, a multi-task objective is devised to jointly recommend the profitable stocks and predict the stock movement. Experiments conducted on Taiwan Stock, S&P 500, and NASDAQ datasets exhibit remarkable recommendation performance of our FinGAT, comparing to state-of-the-art methods.","Yi-Ling Hsu,Yu-Che Tsai,Yu-Che Tsai,Yu-Che Tsai,Cheng-Te Li,Cheng-Te Li,Cheng-Te Li",IEEE Transactions on Knowledge and Data Engineering,2021,,,
1196,Ontology-Based Sentiment Analysis on News Title,"News are one source of information that is easily accessible for the public. News can build an image of or shape the public opinion on the reported object. As such, news is an important medium in General Election (GE) for both the public and the candidates, to know what instruments are being put in place and how they fare. In this research, we performed an ontology-based sentiment analysis to find out whether the 2019 GE in Indonesia has received a positive or a negative response according to the different instruments used. We performed two kinds of experiment, one was based on lexicon and another was using Support Vector Machine (SVM) algorithm. The results of our experiments show that SVM has a better classification performance. For lexicon-based method, the overall recall score was 28.8% and the overall precision score was 46.3%. Whereas for SVM-based method, the scores were 82.4% and 83.3% respectively. These differences may be accounted for the fact that SVM considered our training input instead of just a fixed list of words.","Eunike Andriani Kardinata,Nur Aini Rakhmawati,Nurrida Aini Zuhroh",,2021,,,
1197,LSTM with bio inspired algorithm for action recognition in sports videos,"Abstract   Nowadays, Sport-related movement recognition plays an essential part in the wellbeing of people's lives. The mention of human movements and gestures is often studied in sports to help analyze, guide, and evaluate activity. The automatic detection of sports-related signals helps find the injuries or indirect physical issues in the human body. Action recognition patterns with complicated motion status and periodicity in sports games can help to more accurately estimate the duration of successful action states. Actions are recognized by identifying the activity in a clip. Quality evaluation of action assigns a quantitative score based on the performance of the action. Based on the score, the action states are analyzed. The main issue of sports game identification correctly tracks the behavior of sportspeople. In this paper, Long Short Term Memory networks (LSTM) with a Bio-inspired Algorithm (BIA) framework have been proposed to recognize the action of a sportsperson and motivate a person to improve sports skills. Action recognition and classification can also be used to produce matching or practice output statistics automatically. The proposed LSTM-BIA utilizes predefined actions by modeling the monitoring effects with discriminative temporal signals. It uses the Spatial pyramid pooling SPP-net to obtain the robust characteristic of each frame's tracked area. The new SPP-net network structure will produce an adjusted description irrespective of the scale and resolution of the object. It could be used for identification and entity recognition and enables variable-length image input into CNN. The experimental results show that the proposed method can evaluate the actual action of sportspersons with high accuracy when compared to other methods.","Jun Chen,R. Dinesh Jackson Samuel,R. Dinesh Jackson Samuel,Parthasarathy Poovendran",Image and Vision Computing,2021,,,
1198,The digital twin revolution,A uniform mathematical framework based on probabilistic graphical models drives the digital twin technologies towards dynamical control with real-time data.,Omer San,,2021,,,
1199,Classification of Alzheimer's disease using ensemble of deep neural networks trained through transfer learning.,"Alzheimer's disease (AD) is one of the deadliest neurodegenerative diseases ailing the elderly population all over the world. An ensemble of Deep learning (DL) models can learn highly complicated patterns from MRI scans for the detection of AD by utilizing diverse solutions. In this work, we propose a computationally efficient, DL-architecture agnostic, ensemble of deep neural networks, named 'Deep Transfer Ensemble (DTE)' trained using transfer learning for the classification of AD. DTE leverages the complementary feature views and diversity introduced by many different locally optimum solutions reached by individual networks through the randomization of hyper-parameters. DTE achieves an accuracy of 99.05% and 85.27% on two independent splits of the large dataset for cognitively normal (NC) vs AD classification task. For the task of mild cognitive impairment (MCI) vs AD classification, DTE achieves 98.71% and 83.11% respectively on the two independent splits. It also performs reasonable on a small dataset consisting of only 50 samples per class. It achieved a maximum accuracy of 85% for NC vs AD on the small dataset. It also outperformed snapshot ensembles along with several other existing deep models from similar kind of previous works by other researchers.","M. Tanveer,Mohammad Tanveer,Ashraf Haroon Rashid,M. A. Ganaie,M.A. Ganaie,M. Reza,M. Reza,Motahar Reza,Muhammad Imran Razzak,Imran Razzak,Imran Razzak,Kai-Lung Hua,Kai-Lung Hua,Kai-Lung Hua",IEEE Journal of Biomedical and Health Informatics,2021,,,
1200,A Probabilistic Graphical Model Foundation for Enabling Predictive Digital Twins at Scale,"A unifying mathematical formulation is needed to move from one-off digital twins built through custom implementations to robust digital twin implementations at scale. This work proposes a probabilistic graphical model as a formal mathematical representation of a digital twin and its associated physical asset. We create an abstraction of the asset–twin system as a set of coupled dynamical systems, evolving over time through their respective state spaces and interacting via observed data and control inputs. The formal definition of this coupled system as a probabilistic graphical model enables us to draw upon well-established theory and methods from Bayesian statistics, dynamical systems and control theory. The declarative and general nature of the proposed digital twin model make it rigorous yet flexible, enabling its application at scale in a diverse range of application areas. We demonstrate how the model is instantiated to enable a structural digital twin of an unmanned aerial vehicle (UAV). The digital twin is calibrated using experimental data from a physical UAV asset. Its use in dynamic decision-making is then illustrated in a synthetic example where the UAV undergoes an in-flight damage event and the digital twin is dynamically updated using sensor data. The graphical model foundation ensures that the digital twin calibration and updating process is principled, unified and able to scale to an entire fleet of digital twins. This work proposes a probabilistic graphical model as a formal mathematical foundation for digital twins, and demonstrates how this model supports principled data assimilation, optimal control and end-to-end uncertainty quantification.","Michael G. Kapteyn,Jacob V. R. Pretorius,Karen Willcox",,2021,,,
1201,Digital twin for human-machine interaction with convolutional neural network,"Digital twin (DT) technology aims to create avirtual model of aphysical entity and efficiently analyze the intelligent manufacturing system. Based on the DT, human-machineinteraction (HMI) is a...","Tian Wang,Jiakun Li,Yingjun Deng,Chuang Wang,Hichem Snoussi,Fei Tao",International Journal of Computer Integrated Manufacturing,2021,,,
1202,The Proportion for Splitting Data into Training and Test Set for the Bootstrap in Classification Problems,"Background: The bootstrap can be alternative to cross-validation as a training/test set splitting method since it minimizes the computing time in classification problems in comparison to the tenfold cross-validation. Objectives: Тhis research investigates what proportion should be used to split the dataset into the training and the testing set so that the bootstrap might be competitive in terms of accuracy to other resampling methods. Methods/Approach: Different train/test split proportions are used with the following resampling methods: the bootstrap, the leave-one-out cross-validation, the tenfold cross-validation, and the random repeated train/test split to test their performance on several classification methods. The classification methods used include the logistic regression, the decision tree, and the k-nearest neighbours. Results: The findings suggest that using a different structure of the test set (e.g. 30/70, 20/80) can further optimize the performance of the bootstrap when applied to the logistic regression and the decision tree. For the k-nearest neighbour, the tenfold cross-validation with a 70/30 train/test splitting ratio is recommended. Conclusions: Depending on the characteristics and the preliminary transformations of the variables, the bootstrap can improve the accuracy of the classification problem.",Borislava Vrigazova,,2021,,,
1203,"Mapping the NFT revolution: market trends, trade networks and visual features","Non Fungible Tokens (NFTs) are digital assets that represent objects like art, videos, in-game items and music. They are traded online, often with cryptocurrency, and they are generally encoded as smart contracts on a blockchain. Media and public attention towards NFTs has exploded in 2021, when the NFT art market has experienced record sales while celebrated new star artists. However, little is known about the overall structure and evolution of the NFT market. Here, we analyse data concerning 6.1 million trades of 4.7 million NFTs generating a total trading volume of 935 millions US dollars. Our data are obtained primarily from the Ethereum and WAX blockchains and cover the period between June 23, 2017 and April 27, 2021. First, we characterize the statistical properties of the market. Second, we build the network of interactions and show that traders have bursts of activity followed by inactive periods, and typically specialize on NFTs associated to similar objects. Third, we cluster objects associated to NFTs according to their visual features and show that NFTs within the same category tend to be visually homogeneous. Finally, we investigate the predictability of NFT sales. We use simple machine learning algorithms and find that prices can be best predicted by the sale history of the NFT collection, but also by some features describing the properties of the associated object (e.g., visual features of digital images). We anticipate that our analysis will be of interest to both researchers and practitioners and will spark further research on the NFT production, adoption and trading in different contexts.","Matthieu Nadini,Laura Alessandretti,F. Giacinto,Flavio Di Giacinto,Mauro Martino,Mauro Martino,Luca Maria Aiello,Luca Maria Aiello,Luca Maria Aiello,Andrea Baronchelli,A. Baronchelli,Andrea Baronchelli",arXiv: Statistical Finance,2021,,,
1204,Temporally Correlated Task Scheduling for Sequence Learning,"Sequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. For example, (i) in simultaneous machine translation, one can conduct translation under different latency (i.e., how many input words to read/wait before translation); (ii) in stock trend forecasting, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). While it is clear that those temporally correlated tasks can help each other, there is a very limited exploration on how to better leverage multiple auxiliary tasks to boost the performance of the main task. In this work, we introduce a learnable scheduler to sequence learning, which can adaptively select auxiliary tasks for training depending on the model status and the current training data. The scheduler and the model for the main task are jointly trained through bi-level optimization. Experiments show that our method significantly improves the performance of simultaneous machine translation and stock trend forecasting.","Xueqing Wu,Lewen Wang,Yingce Xia,Weiqing Liu,Weiqing Liu,Lijun Wu,Shufang Xie,Tao Qin,Tao Qin,Tao Qin,Tie-Yan Liu",,2021,,,
1205,Dealing with transaction costs in portfolio optimization: online gradient descent with momentum,"Outperforming the markets through active investment strategies is one of the main challenges in finance. The random movements of assets and the unpredictability of catalysts make it hard to perform better than the average market, therefore, in such a competitive environment, methods designed to keep low transaction costs have a significant impact on the obtained wealth. This paper focuses on investing techniques to beat market returns through online portfolio optimization while controlling transaction costs. Such a framework differs from classical approaches as it assumes that the market has an adversarial behavior, which requires frequent portfolio rebalancing. This paper analyses critically the known online learning literature dealing with transaction costs and proposes a novel algorithm, namely Online Gradient Descent with Momentum (OGDM), to control (theoretically and empirically) the costs. The existing algorithms designed for this setting are either (i) not providing theoretical guarantees, (ii) providing a bound to the total regret, conditionally on unrealistic assumptions or (iii) computationally not efficient. In this paper, we prove that OGDM has nice theoretical, empirical, and computational performances. We show that it has regret, considering costs, of the order [EQUATION], T being the investment horizon, and has Θ(M) per-step computational complexity, M being the number of assets. Furthermore, we show that this algorithm provides competitive gains when compared empirically with state-of-the-art online learning algorithms on a real-world dataset.","Edoardo Vittori,E Vittori,Edoardo Vittori,Martino Bernasconi de Luca,Francesco Trovò,Francesco Trovo,Marcello Restelli",,2020,,,
1206,AlphaEvolve: A Learning Framework to Discover Novel Alphas in Quantitative Investment,"Alphas are stock prediction models capturing trading signals in a stock market. A set of effective alphas can generate weakly correlated high returns to diversify the risk. Existing alphas can be categorized into two classes: Formulaic alphas are simple algebraic expressions of scalar features, and thus can generalize well and be mined into a weakly correlated set. Machine learning alphas are data-driven models over vector and matrix features. They are more predictive than formulaic alphas, but are too complex to mine into a weakly correlated set. In this paper, we introduce a new class of alphas to model scalar, vector, and matrix features which possess the strengths of these two existing classes. The new alphas predict returns with high accuracy and can be mined into a weakly correlated set. In addition, we propose a novel alpha mining framework based on AutoML, called AlphaEvolve, to generate the new alphas. To this end, we first propose operators for generating the new alphas and selectively injecting relational domain knowledge to model the relations between stocks. We then accelerate the alpha mining by proposing a pruning technique for redundant alphas. Experiments show that AlphaEvolve can evolve initial alphas into the new alphas with high returns and weak correlations.","Can Cui,Wei Wang,Meihui Zhang,Gang Chen,Zhaojing Luo,Beng Chin Ooi",,2021,,,
1207,Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport,"Successful quantitative investment usually relies on precise predictions of the future movement of the stock price. Recently, machine learning based solutions have shown their capacity to give more accurate stock prediction and become indispensable components in modern quantitative investment systems. However, the i.i.d. assumption behind existing methods is inconsistent with the existence of diverse trading patterns in the stock market, which inevitably limits their ability to achieve better stock prediction performance. In this paper, we propose a novel architecture, Temporal Routing Adaptor (TRA), to empower existing stock prediction models with the ability to model multiple stock trading patterns. Essentially, TRA is a lightweight module that consists of a set of independent predictors for learning multiple patterns as well as a router to dispatch samples to different predictors. Nevertheless, the lack of explicit pattern identifiers makes it quite challenging to train an effective TRA-based model. To tackle this challenge, we further design a learning algorithm based on Optimal Transport (OT) to obtain the optimal sample to predictor assignment and effectively optimize the router with such assignment through an auxiliary loss term. Experiments on the real-world stock ranking task show that compared to the state-of-the-art baselines, e.g., Attention LSTM and Transformer, the proposed method can improve information coefficient (IC) from 0.053 to 0.059 and 0.051 to 0.056 respectively. Our dataset and code used in this work are publicly available: https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TRA.","Hengxu Lin,Dong Zhou,Weiqing Liu,Jiang Bian",,2021,,,
1208,Quantitative Day Trading from Natural Language using Reinforcement Learning.,"It is challenging to design profitable and practical trading strategies, as stock price movements are highly stochastic, and the market is heavily influenced by chaotic data across sources like news and social media. Existing NLP approaches largely treat stock prediction as a classification or regression problem and are not optimized to make profitable investment decisions. Further, they do not model the temporal dynamics of large volumes of diversely influential text to which the market responds quickly. Building on these shortcomings, we propose a deep reinforcement learning approach that makes time-aware decisions to trade stocks while optimizing profit using textual data. Our method outperforms state-of-the-art in terms of risk-adjusted returns in trading simulations on two benchmarks: Tweets (English) and financial news (Chinese) pertaining to two major indexes and four global stock markets. Through extensive experiments and studies, we build the case for our method as a tool for quantitative trading.","Ramit Sawhney,Arnav Wadhwa,Shivam Agarwal,Rajiv Ratn Shah",,2021,,,
1209,TabNet: Attentive Interpretable Tabular Learning.,"We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.","Sercan O. Arik,Tomas Pfister",,2019,,,
1210,Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport,"Successful quantitative investment usually relies on precise predictions of the future movement of the stock price. Recently, machine learning based solutions have shown their capacity to give more accurate stock prediction and become indispensable components in modern quantitative investment systems. However, the i.i.d. assumption behind existing methods is inconsistent with the existence of diverse trading patterns in the stock market, which inevitably limits their ability to achieve better stock prediction performance. In this paper, we propose a novel architecture, Temporal Routing Adaptor (TRA), to empower existing stock prediction models with the ability to model multiple stock trading patterns. Essentially, TRA is a lightweight module that consists of a set of independent predictors for learning multiple patterns as well as a router to dispatch samples to different predictors. Nevertheless, the lack of explicit pattern identifiers makes it quite challenging to train an effective TRA-based model. To tackle this challenge, we further design a learning algorithm based on Optimal Transport (OT) to obtain the optimal sample to predictor assignment and effectively optimize the router with such assignment through an auxiliary loss term. Experiments on the real-world stock ranking task show that compared to the state-of-the-art baselines, e.g., Attention LSTM and Transformer, the proposed method can improve information coefficient (IC) from 0.053 to 0.059 and 0.051 to 0.056 respectively. Our dataset and code used in this work are publicly available: this https URL.","Hengxu Lin,Dong Zhou,Weiqing Liu,Jiang Bian",arXiv: Learning,2021,,,
1211,Universal Trading for Order Execution with Oracle Policy Distillation.,"As a fundamental problem in algorithmic trading, order execution aims at fulfilling a specific trading order, either liquidation or acquirement, for a given instrument. Towards effective execution strategy, recent years have witnessed the shift from the analytical view with model-based market assumptions to model-free perspective, i.e., reinforcement learning, due to its nature of sequential decision optimization. However, the noisy and yet imperfect market information that can be leveraged by the policy has made it quite challenging to build up sample efficient reinforcement learning methods to achieve effective order execution. In this paper, we propose a novel universal trading policy optimization framework to bridge the gap between the noisy yet imperfect market states and the optimal action sequences for order execution. Particularly, this framework leverages a policy distillation method that can better guide the learning of the common policy towards practically optimal execution by an oracle teacher with perfect information to approximate the optimal trading strategy. The extensive experiments have shown significant improvements of our method over various strong baselines, with reasonable trading actions.","Yuchen Fang,Kan Ren,Weiqing Liu,Dong Zhou,Weinan Zhang,Jiang Bian,Yong Yu,Tie-Yan Liu",,2021,,,
1212,Modern Art Education and Teaching Based on Artificial Intelligence,"The rapid advancement of artificial intelligence has been intensely employed in art teaching and learning. Including the advancement of smart technologies, there are various difficulties in improvi...","Zhang Wen,Achyut Shankar,A. Antonidoss",Journal of Interconnection Networks,2021,,,
1213,Exploiting Cross-Session Information for Session-based Recommendation with Graph Neural Networks,"Different from the traditional recommender system, the session-based recommender system introduces the concept of the session, i.e., a sequence of interactions between a user and multiple items within a period, to preserve the user's recent interest. The existing work on the session-based recommender system mainly relies on mining sequential patterns within individual sessions, which are not expressive enough to capture more complicated dependency relationships among items. In addition, it does not consider the cross-session information due to the anonymity of the session data, where the linkage between different sessions is prevented. In this paper, we solve these problems with the graph neural networks technique. First, each session is represented as a graph rather than a linear sequence structure, based on which a novel Full Graph Neural Network (FGNN) is proposed to learn complicated item dependency. To exploit and incorporate cross-session information in the individual session's representation learning, we further construct a Broadly Connected Session (BCS) graph to link different sessions and a novel Mask-Readout function to improve session embedding based on the BCS graph. Extensive experiments have been conducted on two e-commerce benchmark datasets, i.e., Yoochoose and Diginetica, and the experimental results demonstrate the superiority of our proposal through comparisons with state-of-the-art session-based recommender models.","Qiu Ruihong,Ruihong Qiu,Zi Huang,Huang Zi,Jingjing Li,Li Jingjing,Yin Hongzhi,Hongzhi Yin",arXiv: Information Retrieval,2021,,,
1214,A Deep Reinforcement Learning Framework for Continuous Intraday Market Bidding,"The large integration of variable energy resources is expected to shift a large part of the energy exchanges closer to real-time, where more accurate forecasts are available. In this context, the short-term electricity markets and in particular the intraday market are considered a suitable trading floor for these exchanges to occur. A key component for the successful renewable energy sources integration is the usage of energy storage. In this paper, we propose a novel modelling framework for the strategic participation of energy storage in the European continuous intraday market where exchanges occur through a centralized order book. The goal of the storage device operator is the maximization of the profits received over the entire trading horizon, while taking into account the operational constraints of the unit. The sequential decision-making problem of trading in the intraday market is modelled as a Markov Decision Process. An asynchronous distributed version of the fitted Q iteration algorithm is chosen for solving this problem due to its sample efficiency. The large and variable number of the existing orders in the order book motivates the use of high-level actions and an alternative state representation. Historical data are used for the generation of a large number of artificial trajectories in order to address exploration issues during the learning process. The resulting policy is back-tested and compared against a benchmark strategy that is the current industrial standard. Results indicate that the agent converges to a policy that achieves in average higher total revenues than the benchmark strategy.","Ioannis Boukas,Damien Ernst,Thibaut Théate,Adrien Bolland,Alexandre Huynen,Martin Buchwald,Christelle Wynants,Bertrand Cornélusse",Machine Learning,2021,,,
1215,"A machine learning-based approach to identify unlawful practices in online terms of service: analysis, implementation and evaluation","Terms of Service (ToS) are fundamental factors in the creation of physical as well as online legally relevant relationships. They not only define mutual rights and obligations but also inform users about contract key issues that, in online settings, span from liability limitations to data management and processing conditions. Despite their crucial role, however, ToS are often neglected by users that frequently accept without even reading what they agree upon, representing a critical issue when there exist potentially unfair clauses. To enhance users’ awareness and uphold legal safeguards, we first propose a definition of ToS unfairness based on a novel unfairness measure computed counting the unfair clauses contained in a ToS, and therefore, weighted according to their direct impact on the customers concrete interests. Secondly, we introduce a novel machine learning-based approach to classify ToS clauses, represented by using sentence embedding, in different categories classes and fairness levels. Results of a test involving well-known machine learning models show that Support Vector Machine is able to classify clauses into categories with a F1-score of 86% outperforming state-of-the-art methods, while Random Forest is able to classify clauses into fairness levels with a F1-score of 81%. With the final goal of making terms of service more readable and understandable, we embedded this approach into ToSware, a prototype of a Google Chrome extension. An evaluation study was performed to measure ToSware effectiveness, efficiency, and the overall users’ satisfaction when interacting with it.","Alfonso Guarino,Nicola Lettieri,Delfina Malandrino,Rocco Zaccagnino",Neural Computing and Applications,2021,,,
1216,Blockchain and AI Meet in the Metaverse,"With new technologies related to the development of computers, graphics, and hardware, the virtual world has become a reality. As COVID-19 spreads around the world, the demand for virtual reality increases, and the industry represented by the Metaverse is developing. In the Metaverse, a virtual world that transcends reality, artificial intelligence and blockchain technology are being combined. This chapter explains how artificial intelligence and blockchain can affect the Metaverse.","Hyun-joo Jeon,Ho-chang Youn,Sang-mi Ko,Tae-heon Kim",,2021,,,
1217,Beyond NFTs: A Possible Future for Digital Art,"Abstract The Computer Arts Society, founded in 1969 as a BCS Special Interest Group and still going strong in its 51st year, has seen the ebb and flow of interest in various digital artforms, writes Dr Nick Lambert, Chair of the BCS Computer Arts Society SG.",Nick Lambert,Itnow,2021,,,
1218,Accelerating Transactions Relay in Blockchain Networks via Reputation,"For a blockchain system, the network layer is of great importance for scalability and security. The critical task of blockchain networks is to provide a fast delivery of data. A rapid spread accelerates the transactions to be included into blocks and then confirmed. Existing blockchain systems, especially the cryptocurrencies like Bitcoin, take a simple strategy that requires relay nodes to verify all received transactions and then forward valid ones to all outbound neighbors. Unfortunately, this design is inefficient and slows down the transmission of transactions. In this paper, we introduce the concept of reputation and propose a novel relay protocol, RepuLay, to accelerate the transmission of transactions across the network. First of all, we design a reputation mechanism to help each node identify the unreliable and inactive neighbors. In this mechanism, two values are used to define one’s reputation. Each node keeps a local list of reputations of all its neighbors. Based on the reputation mechanism, RepuLay adopts probabilistic strategies to process transactions. More specifically, after receiving a transaction, the relay node verifies it with a certain probability, which is deduced from the first value of sender’s reputation. Next, the valid and unverified transactions are forwarded to some neighbors. Each neighbor has some probability to be chosen as a receiver and the probability is determined by its second value of reputation. Theoretically, we prove that our design can guarantee the quality of relayed transactions. Further simulation results confirm that RepuLay effectively accelerates the spread of transactions and optimize the usage of nodes’ bandwidths.","Mengqian Zhang,Mengqian Zhang,Yukun Cheng,Yukun Cheng,Xiaotie Deng,Bo Wang,Jan Xie,Yuanyuan Yang,Jiarui Zhang,Jiarui Zhang",,2021,,,
1219,Some studies in machine learning using the game of checkers,Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will lear...,SamuelA. L.,Ibm Journal of Research and Development,1959,,,
1220,Hybrid CNN-LSTM deep learning model and ensemble technique for automatic detection of myocardial infarction using big ECG data,"Automatic and accurate prognosis of myocardial infarction (MI) using electrocardiogram (ECG) signals is a challenging task for the diagnosis and treatment of heart diseases. MI is also referred as a “Heart Attack”, which is the most fatal cardiovascular disease. In many cases, MI does not show any symptoms, hence it is also called a “silent heart attack”. In such cases, patients do not get time to prepare themselves. Hence this disease is more dangerous and fatal with a high mortality rate. Hence, we have proposed an automated detection system of MI using electrocardiogram (ECG) signals by a convolutional neural network (CNN), hybrid CNN- long short-term memory network (LSTM), and ensemble technique to choose the optimum performing model. In this work, we have used 123,998 ECG beats obtained from the “PTB diagnostic database (PTBDB)” and “MIT-BIH arrhythmia database (MITDB) to develop the model. The experiment is performed in two stages: (i) using original and unbalanced datasets and (ii) using a balanced dataset, obtained from synthetic minority oversampling technique (SMOTE) data sampling technique. We have obtained the highest classification accuracy of 99.82%, 99.88%, and 99.89% using CNN, hybrid CNN-LSTM, and ensemble techniques, respectively. Hence the proposed novel data balancing technique (SMOTE-Tomek Link) not only solves the imbalanced data problem but also increases the minority class accuracy significantly. Now our developed model is ready for the clinical application that can be installed in hospitals for the detection of MI.","Hari Mohan Rai,Kalyan Chatterjee",Applied Intelligence,2021,,,
1221,Learning to Build High-Fidelity and Robust Environment Models,"This paper is concerned with robust learning to simulate (RL2S), a new problem of reinforcement learning (RL) that focuses on learning a high-fidelity environment model (i.e., simulator) for serving diverse downstream tasks. Different from the environment learning in model-based RL, where the learned dynamics model is only appropriate to provide simulated data for the specific policy, the goal of RL2S is to build a simulator that is of high fidelity when interacting with various policies. Thus the robustness (i.e., the ability to provide accurate simulations to various policies) of the simulator over diverse corner cases (policies) is the key challenge to address. Via formulating the policy-environment as a dual Markov decision process, we transform RL2S as a novel robust imitation learning problem and propose efficient algorithms to solve it. Experiments on continuous control scenarios demonstrate that the RL2S enabled methods outperform the others on learning high-fidelity simulators for evaluating, ranking and training various policies.","Weinan Zhang,Weinan Zhang,Zhengyu Yang,Jian Shen,Jian Shen,Minghuan Liu,Minghuan Liu,Yimin Huang,Xing Zhang,Zhang Xing,Ruiming Tang,Ruiming Tang,Zhenguo Li,Zhenguo Li",,2021,,,
1222,Quantifying ESG alpha using scholar big data: an automated machine learning approach,"ESG (Environmental, social and governance) alpha strategy that makes sustainable investment has gained popularity among investors. The ESG fields of study in scholar big data is a valuable alternative data that reflects a company's long-term ESG commitment. However, it is considered a difficulty to quantitatively measure a company's ESG premium and its impact to the company's stock price using scholar big data. In this paper, we utilize ESG scholar data as alternative data to develop an automatic trading strategy and propose a practical machine learning approach to quantify the ESG premium of a company and capture the ESG alpha. First, we construct our ESG investment universe and apply feature engineering on the companies' ESG scholar data from the Microsoft Academic Graph database. Then, we train six complementary machine learning models using a combination of financial indicators and ESG scholar data features and employ an ensemble method to predict stock prices and automatically set up portfolio allocation. Finally, we manage our portfolio, trade and rebalance the portfolio allocation monthly using predicted stock prices. We backtest our ESG alpha strategy and compare its performance with benchmarks. The proposed ESG alpha strategy achieves a cumulative return of 2,154.4% during the backtesting period of ten years, which significantly outperforms the NASDAQ-100 index's 397.4% and S&P 500's 226.9%. The traditional financial indicators results in only 1,443.7%, thus our scholar data-based ESG alpha strategy is better at capturing ESG premium than traditional financial indicators.","Qian Chen,Xiao-Yang Liu",,2020,,,
1223,A tabular sarsa-based stock market agent,"Automated stock trading is now the de-facto way that investors have chosen to obtain high profits in the stock market while keeping risk under control. One of the approaches is to create agents employing Reinforcement Learning (RL) algorithms to learn and decide whether or not to operate in the market in order to achieve maximum profit. Automated financial trading systems can learn how to trade optimally while interacting with the market pretty much like a human investor learns how to trade. In this research, a simple RL agent was implemented using the SARSA algorithm. Next, it was tested against 10 stocks from Brazilian stock market B3 (Bolsa, Brasil, Balcao). Results from experiments showed that the agent was able to provide high profits with less risk when compared to a supervised learning agent that used a LSTM neural network.","Renato Arantes de Oliveira,Heitor S. Ramos,Heitor S. Ramos,Daniel Hasan Dalip,Adriano C. M. Pereira",,2020,,,
1224,Risk-sensitive reinforcement learning: a martingale approach to reward uncertainty,"We introduce a novel framework to account for sensitivity to rewards uncertainty in sequential decision-making problems. While risk-sensitive formulations for Markov decision processes studied so far focus on the distribution of the cumulative reward as a whole, we aim at learning policies sensitive to the uncertain/stochastic nature of the rewards, which has the advantage of being conceptually more meaningful in some cases. To this end, we present a new decomposition of the randomness contained in the cumulative reward based on the Doob decomposition of a stochastic process, and introduce a new conceptual tool - the chaotic variation - which can rigorously be interpreted as the risk measure of the martingale component associated to the cumulative reward process. We innovate on the reinforcement learning side by incorporating this new risk-sensitive approach into model-free algorithms, both policy gradient and value function based, and illustrate its relevance on grid world and portfolio optimization problems.","Nelson Vadori,Sumitra Ganesh,Prashant Reddy,Prashant P. Reddy,Manuela Veloso",,2020,,,
1225,Get Real: Realism Metrics for Robust Limit Order Book Market Simulations,"Machine learning (especially reinforcement learning) methods for trading are increasingly reliant on simulation for agent training and testing. Furthermore, simulation is important for validation of hand-coded trading strategies and for testing hypotheses about market structure. A challenge, however, concerns the robustness of policies validated in simulation because the simulations lack fidelity. In fact, researchers have shown that many market simulation approaches fail to reproduce statistics and stylized facts seen in real markets. As a step towards addressing this we surveyed the literature to collect a set of reference metrics and applied them to real market data and simulation output. Our paper provides a comprehensive catalog of these metrics including mathematical formulations where appropriate. Our results show that there are still significant discrepancies between simulated markets and real ones. However, this work serves as a benchmark against which we can measure future improvement.","Svitlana Vyetrenko,David Byrd,Nick Petosa,Mahmoud Mahfouz,Danial Dervovic,Manuela Veloso,Tucker Balch,Tucker Hybinette Balch",Research Papers in Economics,2019,,,
1226,"Generating synthetic data in finance: opportunities, challenges and pitfalls","Financial services generate a huge volume of data that is extremely complex and varied. These datasets are often stored in silos within organisations for various reasons, including but not limited to regulatory requirements and business needs. As a result, data sharing within different lines of business as well as outside of the organisation (e.g. to the research community) is severely limited. It is therefore critical to investigate methods for synthesising financial datasets that follow the same properties of the real data while respecting the need for privacy of the parties involved. This introductory paper aims to highlight the growing need for effective synthetic data generation in the financial domain. We highlight three main areas of focus that are of particular importance while generating synthetic financial datasets: 1) Generating realistic synthetic datasets. 2) Measuring the similarities between real and generated datasets. 3) Ensuring the generative process satisfies any privacy constraints. Although these challenges are also present in other domains, the additional regulatory and privacy requirements within financial services present unique questions that are not asked elsewhere. Due to the size and influence of the financial services industry, answering these questions has the potential for a great and lasting impact. Finally, we aim to develop a shared vocabulary and context for generating synthetic financial data using two types of financial datasets as examples.","Samuel Assefa,Samuel Assefa,Danial Dervovic,Mahmoud Mahfouz,Mahmoud Mahfouz,Robert E. Tillman,Prashant Reddy,Prashant P. Reddy,Manuela Veloso,Manuela Veloso",,2020,,,
1227,"Mapping the NFT revolution: market trends, trade networks, and visual features.","Non Fungible Tokens (NFTs) are digital assets that represent objects like art, videos, in-game items and music. They are traded online, often with cryptocurrency, and they are generally encoded as smart contracts on a blockchain. Media and public attention towards NFTs has exploded in 2021, when the NFT art market has experienced record sales while celebrated new star artists. However, little is known about the overall structure and evolution of the NFT market. Here, we analyse data concerning 6.1 million trades of 4.7 million NFTs generating a total trading volume of 935 millions US dollars. Our data are obtained primarily from the Ethereum and WAX blockchains and cover the period between June 23, 2017 and April 27, 2021. First, we characterize the statistical properties of the market. Second, we build the network of interactions and show that traders have bursts of activity followed by inactive periods, and typically specialize on NFTs associated to similar objects. Third, we cluster objects associated to NFTs according to their visual features and show that NFTs within the same category tend to be visually homogeneous. Finally, we investigate the predictability of NFT sales. We use simple machine learning algorithms and find that prices can be best predicted by the sale history of the NFT collection, but also by some features describing the properties of the associated object (e.g., visual features of digital images). We anticipate that our analysis will be of interest to both researchers and practitioners and will spark further research on the NFT production, adoption and trading in different contexts.","Matthieu Nadini,Laura Alessandretti,F. Giacinto,Flavio Di Giacinto,Mauro Martino,Luca Maria Aiello,Luca Maria Aiello,Andrea Baronchelli",Scientific Reports,2021,,,
1228,Epic Games v. Apple: Fortnite battle that can change the industry,"When Apple Store was launched, there were 500 applications available for iPhone users. Since then, the number of applications in the App Store skyrocketed and in 2017 reached around 2.2 million. In recent years, the number of apps in the App Store is steadily declining, due to Apple’s decision to remove old apps that do not function or the apps that do not follow current app guidelines. The distribution of the apps is only available through the App Store, where the only available payment processor is controlled by Apple. That places Apple in a unique position.The case Epic Games v. Apple raises a broader discussion, whether Apple as the “gatekeeper” of Apps can restrict distribution and access to the apps in the iOS operational system, and whether that kind of activity can be deemed as a monopolist and restrictive competition in App distribution market. This paper will analyze and critically evaluate the recent lawsuit that was brought up against Apple by Epic Games. The main aspect of this analysis is whether Apple can legally restrict the developer’s ability to distribute the applications through the App Store and if it does not restrict the competition. This article is composed of several chapters. Chapter one will examine the relevant facts of the Epic and Apple lawsuit and will summarize the key arguments of this case. The second chapter will explore the relevant legislation and the relevant market related to previously mention proceedings and will explain how the doctrine of the essential facility might affect the case. Chapter three will delve into similar cases brought up earlier and will cover the distribution of digital goods. Chapter four will provide conclusions and the paths moving forward.The object of the paper is to perform a detailed analysis of the case. The purpose of the paper is an assessment of the relevant facts and legal framework regarding Epic’s claim, as well as analyze the topics of foreclosure and dominance in the market. To write this paper several academic writing methods such as descriptive to provide readers with relevant legislation and inform them about relevant facts of the case, also analytical to form the readers’ opinions regarding the recent events and activities of both sides of the suit, also a comparative to compare different legal frameworks in the United States of America and European Union regarding the regulation of monopoly were used. There is no doubt this topic has enormous relevance because of its’ possible after-effects. Epic’s claim already has an impact not only on Apple but also on the whole app development and distribution industry of digital goods and might create a precedent to the similar cases. Currently, this claim is only discussed in the media, and there is no precedent. This article will not give a clear answer to how this lawsuit will be resolved, because it mainly depends on court interpretation of the relevant market. We would rather give a few alternative solutions to this case.","Paulina Ambrasaitė,Agnė Smagurauskaitė",,2021,,,
1229,Lightweight batch authentication and privacy-preserving scheme for online education system,"Abstract   Nowadays, the online education system (OES) attracts learners to improve their knowledge and skills through learning flexibility and efficient assessment procedures. The learner can learn the courses and obtain a degree in a comfortable place, no need to go to school or college. Physical verification in the traditional education system is replaced by a secure authentication process. Many authentication schemes are existing to provide authentication, but they are suffering from authentication burden, due to high computational cost. In this work, two batch authentication schemes are introduced to validate the multiple learners and batch of messages/materials simultaneously instead of authenticating one after another. It reduces the authentication burden during the learner registration and message/material distribution process. The security and performance analysis sections ensures that the proposed scheme provides the essential security features with less computational costs compared to that of existing competitive schemes.","Jegadeesan Subramani,Tu N. Nguyen,Maria Azees,Azees Maria,Arun Sekar Rajasekaran,Korhan Cengiz",Computers & Electrical Engineering,2021,,,
1230,Decision making of autonomous vehicles in lane change scenarios: Deep reinforcement learning approaches with risk awareness,"Abstract   Driving safety is the most important element that needs to be considered for autonomous vehicles (AVs). To ensure driving safety, we proposed a lane change decision-making framework based on deep reinforcement learning to find a risk-aware driving decision strategy with the minimum expected risk for autonomous driving. Firstly, a probabilistic-model based risk assessment method was proposed to assess the driving risk using position uncertainty and distance-based safety metrics. Then, a risk aware decision making algorithm was proposed to find a strategy with the minimum expected risk using deep reinforcement learning. Finally, our proposed methods were evaluated in CARLA in two scenarios (one with static obstacles and one with dynamically moving vehicles). The results show that our proposed methods can generate robust safe driving strategies and achieve better driving performances than previous methods.","Guofa Li,Yifan Yang,Shen Li,Xingda Qu,Nengchao Lyu,Shengbo Eben Li",Transportation Research Part C-emerging Technologies,2021,,,
1231,FinRL-Podracer: High Performance and Scalable Deep Reinforcement Learning for Quantitative Finance,"Machine learning techniques are playing more and more important roles in finance market investment. However, finance quantitative modeling with conventional supervised learning approaches has a number of limitations. The development of deep reinforcement learning techniques is partially addressing these issues. Unfortunately, the steep learning curve and the difficulty in quick modeling and agile development are impeding finance researchers from using deep reinforcement learning in quantitative trading. In this paper, we propose an RLOps in finance paradigm and present a FinRL-Podracer framework to accelerate the development pipeline of deep reinforcement learning (DRL)-driven trading strategy and to improve both trading performance and training efficiency. FinRL-Podracer is a cloud solution that features high performance and high scalability and promises continuous training, continuous integration, and continuous delivery of DRL-driven trading strategies, facilitating a rapid transformation from algorithmic innovations into a profitable trading strategy. First, we propose a generational evolution mechanism with an ensemble strategy to improve the trading performance of a DRL agent, and schedule the training of a DRL algorithm onto a GPU cloud via multi-level mapping. Then, we carry out the training of DRL components with high-performance optimizations on GPUs. Finally, we evaluate the FinRL-Podracer framework for a stock trend prediction task on an NVIDIA DGX SuperPOD cloud. FinRL-Podracer outperforms three popular DRL libraries Ray RLlib, Stable Baseline 3 and FinRL, i.e., 12% \sim 35% improvements in annual return, 0.1 \sim 0.6 improvements in Sharpe ratio and 3 times \sim 7 times speed-up in training time. We show the high scalability by training a trading agent in 10 minutes with $80$ A100 GPUs, on NASDAQ-100 constituent stocks with minute-level data over 10 years.","Zechu Li,Xiao-Yang Liu,Jiahao Zheng,Zhaoran Wang,Anwar Walid,Jian Guo",Research Papers in Economics,2021,,,
1232,Isaac Gym: High Performance GPU Based Physics Simulation For Robot Learning,Isaac Gym offers a high performance learning platform to train policies for wide variety of robotics tasks directly on GPU. Both physics simulation and the neural network policy training reside on GPU and communicate by directly passing data from physics buffers to PyTorch tensors without ever going through any CPU bottlenecks. This leads to blazing fast training times for complex robotics tasks on a single GPU with 2-3 orders of magnitude improvements compared to conventional RL training that uses a CPU based simulator and GPU for neural networks. We host the results and videos at \url{this https URL} and isaac gym can be downloaded at \url{this https URL}.,"Viktor Makoviychuk,Lukasz Wawrzyniak,Yunrong Guo,Michelle Lu,Kier Storey,Miles Macklin,David Hoeller,Nikita Rudin,Arthur Allshire,Ankur Handa,Gavriel State",,2021,,,
1233,Explainable Deep Reinforcement Learning for Portfolio Management: An Empirical Approach,"Deep reinforcement learning (DRL) has been widely studied in the portfolio management task. However, it is challenging to understand a DRL-based trading strategy because of the black-box nature of deep neural networks. In this paper, we propose an empirical approach to explain the strategies of DRL agents for the portfolio management task. First, we use a linear model in hindsight as the reference model, which finds the best portfolio weights by assuming knowing actual stock returns in foresight. In particular, we use the coefficients of a linear model in hindsight as the reference feature weights. Secondly, for DRL agents, we use integrated gradients to define the feature weights, which are the coefficients between reward and features under a linear regression model. Thirdly, we study the prediction power in two cases, single-step prediction and multi-step prediction. In particular, we quantify the prediction power by calculating the linear correlations between the feature weights of a DRL agent and the reference feature weights, and similarly for machine learning methods. Finally, we evaluate a portfolio management task on Dow Jones 30 constituent stocks during 01/01/2009 to 09/01/2021. Our approach empirically reveals that a DRL agent exhibits a stronger multi-step prediction power than machine learning methods.","Mao Guan,Xiao-Yang Liu",Research Papers in Economics,2021,,,
1234,Neural Networks,,Christopher M. Bishop,,1996,,,
1235,Deep Learning: Methods and Applications,,Li Deng,Foundations and Trends in Signal Processing,2014,,,
1236,Vessel segmentation for X-ray coronary angiography using ensemble methods with deep learning and filter-based features,"Automated segmentation of coronary arteries is a crucial step for computer-aided coronary artery disease (CAD) diagnosis and treatment planning. Correct delineation of the coronary artery is challenging in X-ray coronary angiography (XCA) due to the low signal-to-noise ratio and confounding background structures.A novel ensemble framework for coronary artery segmentation in XCA images is proposed, which utilizes deep learning and filter-based features to construct models using the gradient boosting decision tree (GBDT) and deep forest classifiers. The proposed method was trained and tested on 130 XCA images. For each pixel of interest in the XCA images, a 37-dimensional feature vector was constructed based on (1) the statistics of multi-scale filtering responses in the morphological, spatial, and frequency domains; and (2) the feature maps obtained from trained deep neural networks. The performance of these models was compared with those of common deep neural networks on metrics including precision, sensitivity, specificity, F1 score, AUROC (the area under the receiver operating characteristic curve), and IoU (intersection over union).With hybrid under-sampling methods, the best performing GBDT model achieved a mean F1 score of 0.874, AUROC of 0.947, sensitivity of 0.902, and specificity of 0.992; while the best performing deep forest model obtained a mean F1 score of 0.867, AUROC of 0.95, sensitivity of 0.867, and specificity of 0.993. Compared with the evaluated deep neural networks, both models had better or comparable performance for all evaluated metrics with lower standard deviations over the test images.The proposed feature-based ensemble method outperformed common deep convolutional neural networks in most performance metrics while yielding more consistent results. Such a method can be used to facilitate the assessment of stenosis and improve the quality of care in patients with CAD.","Zijun Gao,Linhong Wang,Reza Soroushmehr,Alexander Wood,Jonathan Gryak,Brahmajee K. Nallamothu,Kayvan Najarian",BMC Medical Imaging,2022,,,
1237,Cryptocurrency trading: a comprehensive survey,"Abstract In recent years, the tendency of the number of financial institutions to include cryptocurrencies in their portfolios has accelerated. Cryptocurrencies are the first pure digital assets to be included by asset managers. Although they have some commonalities with more traditional assets, they have their own separate nature and their behaviour as an asset is still in the process of being understood. It is therefore important to summarise existing research papers and results on cryptocurrency trading, including available trading platforms, trading signals, trading strategy research and risk management. This paper provides a comprehensive survey of cryptocurrency trading research, by covering 146 research papers on various aspects of cryptocurrency trading ( e . g ., cryptocurrency trading systems, bubble and extreme condition, prediction of volatility and return, crypto-assets portfolio construction and crypto-assets, technical trading and others). This paper also analyses datasets, research trends and distribution among research objects (contents/properties) and technologies, concluding with some promising opportunities that remain open in cryptocurrency trading.","Fan Fang,Carmine Ventre,Michail Basios,Leslie Kanthan,David Martinez-Rego,Fan Wu,Lingbo Li",Financial Innovation,2022,,,
1238,Federated Learning,,"Qiang Yang,Yang Liu,Yong Cheng,Yan Kang,Tianqi Chen,Han Yu",Synthesis Lectures on Artificial Intelligence and Machine Learning,2019,,,
1239,Stochastic Controls,,"Jiongmin Yong,Xun Yu Zhou",,1999,,,
1240,Bagging predictors,"Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.",Leo Breiman,Machine Learning,1996,,,
1241,Realizing the Metaverse with Edge Intelligence: A Match Made in Heaven,"Dubbed ""the successor to the mobile Internet"", the concept of the Metaverse has recently exploded in popularity. While there exists lite versions of the Metaverse today, we are still far from realizing the vision of a seamless, shardless, and interoperable Metaverse given the stringent sensing, communication, and computation requirements. Moreover, the birth of the Metaverse comes amid growing privacy concerns among users. In this article, we begin by providing a preliminary definition of the Metaverse. We discuss the architecture of the Metaverse and mainly focus on motivating the convergence of edge intelligence and the infrastructure layer of the Metaverse. We present major edge-based technological developments and their integration to support the Metaverse engine. Then, we present our research attempts through a case study of virtual city development in the Metaverse. Finally, we discuss the open research issues.","Lim, Wei Yang Bryan,Xiong, Zehui,Niyato, Dusit,Cao, Xianbin,Miao, Chunyan,Sun, Sumei,Yang, Qiang",,2022,,,
1242,Representation Learning Using Deep Random Vector Functional Link Networks for Clustering,"• Unsupervised RVFL (usRVFL) framework based on manifold regularization proposed to perform unsupervised representation learning. • Embedded features allow usRVFL to perform unsupervised tasks such as clustering. • Inspired by consensus clustering, unsupervised deep RVFL proposed. • Evaluations on benchmark datasets have shown that usdRVFL achieves equivalent or superior clustering performance to state-of-the-art spectral clustering methods. Random Vector Functional Link (RVFL) Networks have received a lot of attention due to the fast training speed as the non-iterative solution characteristic. Currently, the main research direction of RVFLs has supervised learning, including semi-supervised and multi-label. There are hardly any unsupervised research results for RVFLs. In this paper, we propose the unsupervised RVFL (usRVFL), and the unsupervised framework is generic that can be used with other RVFL variants, thus we extend it to an ensemble deep variant, unsupervised deep RVFL (usdRVFL). The unsupervised method is based on the manifold regularization while the deep variant is related to the consensus clustering method, which can increase the capability and diversity of RVFLs. Our unsupervised approaches also benefit from fast training speed, even the deep variant offers a very competitive computation efficiency. Empirical experiments on several benchmark datasets demonstrated the effectiveness of the proposed algorithms.","Minghui Hu,Ponnuthurai Nagaratnam Suganthan",Pattern Recognition,2022,,,
1243,Ensemble deep random vector functional link network using privileged information for Alzheimer's disease diagnosis.,"In this paper, deep RVFL and its ensembles are enabled to incorporate privileged information, however, the standard RVFL model and its deep models are unable to use privileged information. Privileged information-based approach commonly seen in human learning. To fill this gap, we incorporate learning using privileged information (LUPI) in deep RVFL model and propose deep RVFL with LUPI framework (dRVFL+). Privileged information is available while training the models. To make the model more robust, we propose ensemble deep RVFL+ with LUPI framework (edRVFL+). Unlike traditional ensemble approach wherein multiple base learners are trained, the proposed edRVFL+ optimises a single network and generates an ensemble via optimization at different levels of random projections of the data. Both dRVFL+ and edRVFL+ efficiently utilise the privileged information which results in better generalization performance. In LUPI framework, half of the available features are used as normal features and rest as the privileged features. However, we propose a novel approach for generating the privileged information. To the best of our knowledge, this is first time that a separate privileged information is generated. The proposed models are employed for the diagnosis of Alzheimer's disease. Experimental results show the promising performance of both the proposed models.","M A Ganaie,M Tanveer",IEEE/ACM Transactions on Computational Biology and Bioinformatics,2022,,,
1244,Identification of osteoporosis using ensemble deep learning model with panoramic radiographs and clinical covariates.,"Osteoporosis is becoming a global health issue due to increased life expectancy. However, it is difficult to detect in its early stages owing to a lack of discernible symptoms. Hence, screening for osteoporosis with widely used dental panoramic radiographs would be very cost-effective and useful. In this study, we investigate the use of deep learning to classify osteoporosis from dental panoramic radiographs. In addition, the effect of adding clinical covariate data to the radiographic images on the identification performance was assessed. For objective labeling, a dataset containing 778 images was collected from patients who underwent both skeletal-bone-mineral density measurement and dental panoramic radiography at a single general hospital between 2014 and 2020. Osteoporosis was assessed from the dental panoramic radiographs using convolutional neural network (CNN) models, including EfficientNet-b0, -b3, and -b7 and ResNet-18, -50, and -152. An ensemble model was also constructed with clinical covariates added to each CNN. The ensemble model exhibited improved performance on all metrics for all CNNs, especially accuracy and AUC. The results show that deep learning using CNN can accurately classify osteoporosis from dental panoramic radiographs. Furthermore, it was shown that the accuracy can be improved using an ensemble model with patient covariates.","Shintaro Sukegawa,Ai Fujimura,Akira Taguchi,Norio Yamamoto,Akira Kitamura,Ryosuke Goto,Keisuke Nakano,Kiyofumi Takabatake,Hotaka Kawai,Hitoshi Nagatsuka,Yoshihiko Furuki",Scientific Reports,2022,,,
1245,Classification of Alzheimer's Disease Using Ensemble of Deep Neural Networks Trained Through Transfer Learning.,"Alzheimer's disease (AD) is one of the deadliest neurodegenerative diseases ailing the elderly population all over the world. An ensemble of Deep learning (DL) models can learn highly complicated patterns from MRI scans for the detection of AD by utilizing diverse solutions. In this work, we propose a computationally efficient, DL-architecture agnostic, ensemble of deep neural networks, named 'Deep Transfer Ensemble (DTE)' trained using transfer learning for the classification of AD. DTE leverages the complementary feature views and diversity introduced by many different locally optimum solutions reached by individual networks through the randomization of hyper-parameters. DTE achieves an accuracy of 99.05% and 85.27% on two independent splits of the large dataset for cognitively normal (NC) vs AD classification task. For the task of mild cognitive impairment (MCI) vs AD classification, DTE achieves 98.71% and 83.11% respectively on the two independent splits. It also performs reasonable on a small dataset consisting of only 50 samples per class. It achieved a maximum accuracy of 85% for NC vs AD on the small dataset. It also outperformed snapshot ensembles along with several other existing deep models from similar kind of previous works by other researchers.","M Tanveer,A H Rashid,M A Ganaie,M Reza,Imran Razzak,Kai-Lung Hua",IEEE Journal of Biomedical and Health Informatics,2022,,,
1246,Ensemble deep random vector functional link network using privileged information for Alzheimer's disease diagnosis.,"In this paper, deep RVFL and its ensembles are enabled to incorporate privileged information, however, the standard RVFL model and its deep models are unable to use privileged information. Privileged information-based approach commonly seen in human learning. To fill this gap, we incorporate learning using privileged information (LUPI) in deep RVFL model and propose deep RVFL with LUPI framework (dRVFL+). Privileged information is available while training the models. To make the model more robust, we propose ensemble deep RVFL+ with LUPI framework (edRVFL+). Unlike traditional ensemble approach wherein multiple base learners are trained, the proposed edRVFL+ optimises a single network and generates an ensemble via optimization at different levels of random projections of the data. Both dRVFL+ and edRVFL+ efficiently utilise the privileged information which results in better generalization performance. In LUPI framework, half of the available features are used as normal features and rest as the privileged features. However, we propose a novel approach for generating the privileged information. To the best of our knowledge, this is first time that a separate privileged information is generated. The proposed models are employed for the diagnosis of Alzheimer's disease. Experimental results show the promising performance of both the proposed models.","M A Ganaie,M Tanveer",IEEE/ACM Transactions on Computational Biology and Bioinformatics,2022,,,
1247,Die Insel,,,Schweizerische Ärztezeitung =,,,,
1248,The Nature of Statistical Learning Theory,,Vladimir N. Vapnik,,,,,
1249,Efficient Capital Markets: A Review of Theory and Empirical Work,,Eugene F. Fama,Journal of Finance,,,,
1250,Adaptive Control Processes,"The aim of this work is to present a unified approach to the modern field of control theory and to provide a technique for making problems involving deterministic, stochastic, and adaptive processes of both linear and nonlinear type amenable to machine solution. Mr. Bellman has used the theory of dynamic programming to formulate, analyze, and prepare these processes for numerical treatment by digital computers. The unique concept of the book is that of a single problem stretching from recognition and formulation to analytic treatment and computational solution. Due to the emphasis upon ideas and concepts, this book is equally suited for the pure and applied mathematician, and for control engineers in all fields. Originally published in 1961. The Princeton Legacy Library uses the latest print-on-demand technology to again make available previously out-of-print books from the distinguished backlist of Princeton University Press. These editions preserve the original texts of these important books while presenting them in durable paperback and hardcover editions. The goal of the Princeton Legacy Library is to vastly increase access to the rich scholarly heritage found in the thousands of books published by Princeton University Press since its founding in 1905.",Richard E. Bellman,,,,,
1251,Tagebuch,,,,,,,
1252,Stacked regressions,,Leo Breiman,Machine Learning,,,,
1253,10.3726/978-3-653-05477-4/3,,,,,,,
1254,Analyzing bagging,,"Peter Bühlmann,Bin Yu",Annals of Statistics,,,,
1255,TOSCA Solves Big Problems in the Cloud and Beyond!,,"Paul Lipton,Derek Palma,Matt Rutkowski,Damian Andrew Tamburri",IEEE Cloud Computing,,,,
1256,,Performance functions and reinforcement learning for trading systems and portfolios,,"John Moody,Lizhong Wu,Yuansong Liao,Matthew Saffell",Journal of Forecasting,,,
1257,Neural Network Learning,"First published in 1999, this book describes theoretical advances in the study of artificial neural networks. It explores probabilistic models of supervised learning problems, and addresses the key statistical and computational questions. Research on pattern classification with binary-output networks is surveyed, including a discussion of the relevance of the Vapnik-Chervonenkis dimension, and calculating estimates of the dimension for several neural network models. A model of classification by real-output networks is developed, and the usefulness of classification with a 'large margin' is demonstrated. The authors explain the role of scale-sensitive versions of the Vapnik-Chervonenkis dimension in large margin classification, and in real prediction. They also discuss the computational complexity of neural network learning, describing a variety of hardness results, and outlining two efficient constructive learning algorithms. The book is self-contained and is intended to be accessible to researchers and graduate students in computer science, engineering, and mathematics.","Martin Anthony,Peter L. Bartlett",,,,,
1258,An experiment in linguistic synthesis with a fuzzy logic controller,,"E.H. Mamdani,S. Assilian",International journal of man-machine studies,,,,
1259,COLLADA,"COLLADA is a COLLAborative Design Activity for establishing an open standard Digital Asset schema for interactive 3D applications. This book explains in detail how to use the COLLADA technology in a project utilizing 3D assets, and ultimately how to create an effective content creation pipeline for the most complex development. Errata are posted at","Remi Arnaud,Mark C. Barnes",,,,,
1260,Metaverse for Social Good,"In recent years, the metaverse has attracted enormous attention from around the world with the development of related technologies. The expected metaverse should be a realistic society with more direct and physical interactions, while the concepts of race, gender, and even physical disability would be weakened, which would be highly beneficial for society. However, the development of metaverse is still in its infancy, with great potential for improvement. Regarding metaverse's huge potential, industry has already come forward with advance preparation, accompanied by feverish investment, but there are few discussions about metaverse in academia to scientifically guide its development. In this paper, we highlight the representative applications for social good. Then we propose a three-layer metaverse architecture from a macro perspective, containing infrastructure, interaction, and ecosystem. Moreover, we journey toward both a historical and novel metaverse with a detailed timeline and table of specific attributes. Lastly, we illustrate our implemented blockchain-driven metaverse prototype of a university campus and discuss the prototype design and insights.","Haihan Duan,Jiaye Li,Sizheng Fan,Zhonghao Lin,Xiao Wu,Wei Cai",,2021,,,
1261,Risk-sensitive reinforcement learning,"We introduce a novel framework to account for sensitivity to rewards uncertainty in sequential decision-making problems. While risk-sensitive formulations for Markov decision processes studied so far focus on the distribution of the cumulative reward as a whole, we aim at learning policies sensitive to the uncertain/stochastic nature of the rewards, which has the advantage of being conceptually more meaningful in some cases. To this end, we present a new decomposition of the randomness contained in the cumulative reward based on the Doob decomposition of a stochastic process, and introduce a new conceptual tool - the \textit{chaotic variation} - which can rigorously be interpreted as the risk measure of the martingale component associated to the cumulative reward process. We innovate on the reinforcement learning side by incorporating this new risk-sensitive approach into model-free algorithms, both policy gradient and value function based, and illustrate its relevance on grid world and portfolio optimization problems.","Vadori, Nelson,Ganesh, Sumitra,Reddy, Prashant,Veloso, Manuela",,2020,,,
1262,TensorLayer,"Deep learning has enabled major advances in the fields of computer vision, natural language processing, and multimedia among many others. Developing a deep learning system is arduous and complex, as it involves constructing neural network architectures, managing training/trained models, tuning optimization process, preprocessing and organizing data, etc. TensorLayer is a versatile Python library that aims at helping researchers and engineers efficiently develop deep learning systems. It offers rich abstractions for neural networks, model and data management, and parallel workflow mechanism. While boosting efficiency, TensorLayer maintains both performance and scalability. TensorLayer was released in September 2016 on GitHub, and has helped people from academia and industry develop real-world applications of deep learning.","Dong, Hao,Supratak, Akara,Mai, Luo,Liu, Fangde,Oehmichen, Axel,Yu, Simiao,Guo, Yike",,2017,,,
